[2025-08-07 02:31:44,631] [INFO] Namespace(path=['mitigation_t2/shaped-len-gaussian-1e-05-0.5-2500-10.0-250.csv'], pktcount=1000, kfold=1, model=['bigru'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/mitigation_evaluation/mitigation_shaped-len-gaussian-1e-05-0.5-2500-10.0-250_1754533902.debug', leaderboard_path='output/meta-free-apps/mitigation_evaluation/mitigation_shaped-len-gaussian-1e-05-0.5-2500-10.0-250_1754533902.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'))
[2025-08-07 02:33:52,560] [INFO] Processed data from mitigation_t2/shaped-len-gaussian-1e-05-0.5-2500-10.0-250.csv:
[2025-08-07 02:33:52,560] [INFO] (84492, 1000)
[2025-08-07 02:33:52,560] [INFO] [['52' '52' '52' ... <NA> <NA> <NA>]
 ['52' '52' '1432' ... <NA> <NA> <NA>]
 ['52' '609' '770' ... <NA> <NA> <NA>]
 ...
 ['1432' '1432' '1432' ... '675' '1432' '1432']
 ['1432' '1432' '1432' ... '1274' '608' '311']
 ['455' '1432' '1432' ... '1432' '1432' '1432']]
[2025-08-07 02:35:26,630] [INFO] Feature 0 normalized using token
[2025-08-07 02:35:26,630] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2025-08-07 02:35:26,709] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1382, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-08-07 02:35:26,709] [INFO] Training...
[2025-08-07 02:37:26,014] [INFO] Epoch 1/50, ValAcc: 0.91%, TrainLoss: 4.5131, ValLoss: 4.5120, LR: 0.001
[2025-08-07 02:39:23,265] [INFO] Epoch 2/50, ValAcc: 0.91%, TrainLoss: 4.5111, ValLoss: 4.5121, LR: 0.001
[2025-08-07 02:39:23,266] [INFO] ValAcc too low 0.91%. Stopping early.
[2025-08-07 02:39:29,023] [WARNING] ValAcc too low (0.91%), getting stuck in a bad local minimum...
[2025-08-07 02:39:29,024] [INFO] Retraining with new initialization: attempt 1...
[2025-08-07 02:41:26,400] [INFO] Epoch 1/50, ValAcc: 1.07%, TrainLoss: 4.5132, ValLoss: 4.5114, LR: 0.001
[2025-08-07 02:43:23,765] [INFO] Epoch 2/50, ValAcc: 0.91%, TrainLoss: 4.5111, ValLoss: 4.5119, LR: 0.001
[2025-08-07 02:43:23,766] [INFO] ValAcc too low 0.91%. Stopping early.
[2025-08-07 02:43:29,539] [WARNING] ValAcc too low (0.91%), getting stuck in a bad local minimum...
[2025-08-07 02:43:29,540] [INFO] Retraining with new initialization: attempt 2...
[2025-08-07 02:45:27,098] [INFO] Epoch 1/50, ValAcc: 1.24%, TrainLoss: 4.5097, ValLoss: 4.4933, LR: 0.001
[2025-08-07 02:47:25,247] [INFO] Epoch 2/50, ValAcc: 1.25%, TrainLoss: 4.4987, ValLoss: 4.4858, LR: 0.001
[2025-08-07 02:49:23,324] [INFO] Epoch 3/50, ValAcc: 1.22%, TrainLoss: 4.4937, ValLoss: 4.4888, LR: 0.001
[2025-08-07 02:51:20,361] [INFO] Epoch 4/50, ValAcc: 1.18%, TrainLoss: 4.4891, ValLoss: 4.4926, LR: 0.001
[2025-08-07 02:53:17,360] [INFO] Epoch 5/50, ValAcc: 1.22%, TrainLoss: 4.4833, ValLoss: 4.4821, LR: 0.001
[2025-08-07 02:55:14,374] [INFO] Epoch 6/50, ValAcc: 1.25%, TrainLoss: 4.4787, ValLoss: 4.4817, LR: 0.001
[2025-08-07 02:57:11,446] [INFO] Epoch 7/50, ValAcc: 1.23%, TrainLoss: 4.4725, ValLoss: 4.4766, LR: 0.001
[2025-08-07 02:59:08,545] [INFO] Epoch 8/50, ValAcc: 1.92%, TrainLoss: 4.4606, ValLoss: 4.4227, LR: 0.001
[2025-08-07 03:01:05,674] [INFO] Epoch 9/50, ValAcc: 3.04%, TrainLoss: 4.3682, ValLoss: 4.2299, LR: 0.001
[2025-08-07 03:03:02,861] [INFO] Epoch 10/50, ValAcc: 5.64%, TrainLoss: 4.1864, ValLoss: 4.0156, LR: 0.001
[2025-08-07 03:05:00,012] [INFO] Epoch 11/50, ValAcc: 12.00%, TrainLoss: 3.8583, ValLoss: 3.5410, LR: 0.001
[2025-08-07 03:06:57,165] [INFO] Epoch 12/50, ValAcc: 14.44%, TrainLoss: 3.5242, ValLoss: 3.3940, LR: 0.001
[2025-08-07 03:08:54,289] [INFO] Epoch 13/50, ValAcc: 20.26%, TrainLoss: 3.2892, ValLoss: 3.1125, LR: 0.001
[2025-08-07 03:10:51,395] [INFO] Epoch 14/50, ValAcc: 21.85%, TrainLoss: 3.0807, ValLoss: 3.0453, LR: 0.001
[2025-08-07 03:12:48,564] [INFO] Epoch 15/50, ValAcc: 20.99%, TrainLoss: 2.8880, ValLoss: 3.0614, LR: 0.001
[2025-08-07 03:14:45,687] [INFO] Epoch 16/50, ValAcc: 25.38%, TrainLoss: 2.7372, ValLoss: 2.8485, LR: 0.001
[2025-08-07 03:16:42,764] [INFO] Epoch 17/50, ValAcc: 27.69%, TrainLoss: 2.5982, ValLoss: 2.7762, LR: 0.001
[2025-08-07 03:18:40,239] [INFO] Epoch 18/50, ValAcc: 30.53%, TrainLoss: 2.4872, ValLoss: 2.7186, LR: 0.001
[2025-08-07 03:20:38,704] [INFO] Epoch 19/50, ValAcc: 29.50%, TrainLoss: 2.3771, ValLoss: 2.7071, LR: 0.001
[2025-08-07 03:22:36,650] [INFO] Epoch 20/50, ValAcc: 27.62%, TrainLoss: 2.2985, ValLoss: 2.9469, LR: 0.001
[2025-08-07 03:24:33,941] [INFO] Epoch 21/50, ValAcc: 36.91%, TrainLoss: 2.2257, ValLoss: 2.2763, LR: 0.001
[2025-08-07 03:26:31,172] [INFO] Epoch 22/50, ValAcc: 34.78%, TrainLoss: 2.1477, ValLoss: 2.5408, LR: 0.001
[2025-08-07 03:28:28,410] [INFO] Epoch 23/50, ValAcc: 31.33%, TrainLoss: 2.0840, ValLoss: 2.6664, LR: 0.001
[2025-08-07 03:30:25,522] [INFO] Epoch 24/50, ValAcc: 32.49%, TrainLoss: 2.0393, ValLoss: 2.4979, LR: 0.001
[2025-08-07 03:32:22,564] [INFO] Epoch 25/50, ValAcc: 32.83%, TrainLoss: 1.8516, ValLoss: 2.7036, LR: 0.0005
[2025-08-07 03:34:19,704] [INFO] Epoch 26/50, ValAcc: 36.28%, TrainLoss: 1.7750, ValLoss: 2.5118, LR: 0.0005
[2025-08-07 03:36:16,728] [INFO] Epoch 27/50, ValAcc: 37.66%, TrainLoss: 1.7219, ValLoss: 2.4991, LR: 0.0005
[2025-08-07 03:38:13,779] [INFO] Epoch 28/50, ValAcc: 36.32%, TrainLoss: 1.6134, ValLoss: 2.5971, LR: 0.00025
[2025-08-07 03:40:10,811] [INFO] Epoch 29/50, ValAcc: 36.88%, TrainLoss: 1.5565, ValLoss: 2.6433, LR: 0.00025
[2025-08-07 03:42:07,848] [INFO] Epoch 30/50, ValAcc: 37.69%, TrainLoss: 1.5201, ValLoss: 2.5226, LR: 0.00025
[2025-08-07 03:44:05,440] [INFO] Epoch 31/50, ValAcc: 36.14%, TrainLoss: 1.4502, ValLoss: 2.7410, LR: 0.000125
[2025-08-07 03:46:02,537] [INFO] Epoch 32/50, ValAcc: 36.12%, TrainLoss: 1.4161, ValLoss: 2.8332, LR: 0.000125
[2025-08-07 03:47:59,561] [INFO] Epoch 33/50, ValAcc: 36.01%, TrainLoss: 1.3943, ValLoss: 2.8437, LR: 0.000125
[2025-08-07 03:49:56,520] [INFO] Epoch 34/50, ValAcc: 36.92%, TrainLoss: 1.3576, ValLoss: 2.7741, LR: 6.25e-05
[2025-08-07 03:49:56,520] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-08-07 03:50:13,771] [INFO] Namespace(path=['mitigation_t2/shaped-len-gaussian-1e-05-0.5-2500-10.0-250.csv'], pktcount=1000, kfold=1, model=['bigru'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/mitigation_evaluation/mitigation_shaped-len-gaussian-1e-05-0.5-2500-10.0-250_1754533902.debug', leaderboard_path='output/meta-free-apps/mitigation_evaluation/mitigation_shaped-len-gaussian-1e-05-0.5-2500-10.0-250_1754533902.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='250_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.3659,0.3584,0.3745,0.3659
[2025-08-07 03:50:14,918] [INFO] [(0.3658796378483934, 0.35835856511117137, 0.3745471835867618, 0.36588515903596175)]
