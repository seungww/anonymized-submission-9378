[2025-07-29 08:26:54,591] [INFO] Namespace(path=['meta-free-apps/meta-ip_outlen/meta-ip_outlen.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777612.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777612.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'))
[2025-07-29 08:30:05,907] [INFO] Processed data from meta-free-apps/meta-ip_outlen/meta-ip_outlen.csv:
[2025-07-29 08:30:05,908] [INFO] (84492, 1000)
[2025-07-29 08:30:05,908] [INFO] [['656' '52' '83' ... <NA> <NA> <NA>]
 ['423' '52' '52' ... '1432' '417' '76']
 ['423' '52' '52' ... <NA> <NA> <NA>]
 ...
 ['242' '52' '76' ... '52' '83' '52']
 ['60' '52' '569' ... '1432' '1432' '1432']
 ['64' '52' '52' ... '52' '52' '355']]
[2025-07-29 08:30:06,247] [INFO] Training Fold 1/5
[2025-07-29 08:31:29,018] [INFO] Feature 0 normalized using token
[2025-07-29 08:31:29,018] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2025-07-29 08:31:29,091] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 08:31:29,091] [INFO] Training...
[2025-07-29 08:35:04,058] [INFO] Epoch 1/50, ValAcc: 60.86%, TrainLoss: 3.3322, ValLoss: 1.3322, LR: 0.001
[2025-07-29 08:38:37,708] [INFO] Epoch 2/50, ValAcc: 70.75%, TrainLoss: 1.2898, ValLoss: 1.0577, LR: 0.001
[2025-07-29 08:42:10,820] [INFO] Epoch 3/50, ValAcc: 73.62%, TrainLoss: 1.0385, ValLoss: 0.9067, LR: 0.001
[2025-07-29 08:45:44,220] [INFO] Epoch 4/50, ValAcc: 75.05%, TrainLoss: 0.9160, ValLoss: 0.8562, LR: 0.001
[2025-07-29 08:49:17,525] [INFO] Epoch 5/50, ValAcc: 77.37%, TrainLoss: 0.8438, ValLoss: 0.7806, LR: 0.001
[2025-07-29 08:52:50,547] [INFO] Epoch 6/50, ValAcc: 77.93%, TrainLoss: 0.8065, ValLoss: 0.7586, LR: 0.001
[2025-07-29 08:56:23,768] [INFO] Epoch 7/50, ValAcc: 78.66%, TrainLoss: 0.7664, ValLoss: 0.7398, LR: 0.001
[2025-07-29 08:59:56,742] [INFO] Epoch 8/50, ValAcc: 78.70%, TrainLoss: 0.7346, ValLoss: 0.7312, LR: 0.001
[2025-07-29 09:03:30,056] [INFO] Epoch 9/50, ValAcc: 79.86%, TrainLoss: 0.7180, ValLoss: 0.7002, LR: 0.001
[2025-07-29 09:07:02,441] [INFO] Epoch 10/50, ValAcc: 79.72%, TrainLoss: 0.6923, ValLoss: 0.7106, LR: 0.001
[2025-07-29 09:10:34,194] [INFO] Epoch 11/50, ValAcc: 79.85%, TrainLoss: 0.6835, ValLoss: 0.7356, LR: 0.001
[2025-07-29 09:12:51,575] [INFO] Epoch 12/50, ValAcc: 81.56%, TrainLoss: 0.6701, ValLoss: 0.6508, LR: 0.001
[2025-07-29 09:16:24,796] [INFO] Epoch 13/50, ValAcc: 81.30%, TrainLoss: 0.6515, ValLoss: 0.6555, LR: 0.001
[2025-07-29 09:19:57,971] [INFO] Epoch 14/50, ValAcc: 80.54%, TrainLoss: 0.6392, ValLoss: 0.7278, LR: 0.001
[2025-07-29 09:23:31,231] [INFO] Epoch 15/50, ValAcc: 80.44%, TrainLoss: 0.6313, ValLoss: 0.6735, LR: 0.001
[2025-07-29 09:27:03,700] [INFO] Epoch 16/50, ValAcc: 82.72%, TrainLoss: 0.5641, ValLoss: 0.6253, LR: 0.0005
[2025-07-29 09:30:36,997] [INFO] Epoch 17/50, ValAcc: 83.25%, TrainLoss: 0.5341, ValLoss: 0.6227, LR: 0.0005
[2025-07-29 09:34:10,123] [INFO] Epoch 18/50, ValAcc: 81.95%, TrainLoss: 0.5281, ValLoss: 0.6688, LR: 0.0005
[2025-07-29 09:37:43,312] [INFO] Epoch 19/50, ValAcc: 83.11%, TrainLoss: 0.5126, ValLoss: 0.6598, LR: 0.0005
[2025-07-29 09:41:16,407] [INFO] Epoch 20/50, ValAcc: 82.97%, TrainLoss: 0.5049, ValLoss: 0.6415, LR: 0.0005
[2025-07-29 09:44:49,723] [INFO] Epoch 21/50, ValAcc: 84.00%, TrainLoss: 0.4728, ValLoss: 0.6479, LR: 0.00025
[2025-07-29 09:47:03,986] [INFO] Epoch 22/50, ValAcc: 83.99%, TrainLoss: 0.4522, ValLoss: 0.6569, LR: 0.00025
[2025-07-29 09:50:37,143] [INFO] Epoch 23/50, ValAcc: 83.68%, TrainLoss: 0.4434, ValLoss: 0.6695, LR: 0.00025
[2025-07-29 09:54:10,210] [INFO] Epoch 24/50, ValAcc: 84.11%, TrainLoss: 0.4175, ValLoss: 0.6876, LR: 0.000125
[2025-07-29 09:57:43,384] [INFO] Epoch 25/50, ValAcc: 84.39%, TrainLoss: 0.4094, ValLoss: 0.6831, LR: 0.000125
[2025-07-29 10:01:16,419] [INFO] Epoch 26/50, ValAcc: 84.07%, TrainLoss: 0.4007, ValLoss: 0.6884, LR: 0.000125
[2025-07-29 10:04:49,649] [INFO] Epoch 27/50, ValAcc: 84.33%, TrainLoss: 0.3890, ValLoss: 0.6993, LR: 6.25e-05
[2025-07-29 10:04:49,649] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 10:05:11,561] [INFO] Namespace(path=['meta-free-apps/meta-ip_outlen/meta-ip_outlen.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777612.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777612.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipoutlen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8369,0.8391,0.8458,0.8373
[2025-07-29 10:05:11,563] [INFO] Training Fold 2/5
[2025-07-29 10:06:38,300] [INFO] Feature 0 normalized using token
[2025-07-29 10:06:38,300] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2025-07-29 10:06:38,352] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 10:06:38,352] [INFO] Training...
[2025-07-29 10:10:11,137] [INFO] Epoch 1/50, ValAcc: 59.93%, TrainLoss: 3.2088, ValLoss: 1.3821, LR: 0.001
[2025-07-29 10:13:43,646] [INFO] Epoch 2/50, ValAcc: 69.09%, TrainLoss: 1.2790, ValLoss: 1.1360, LR: 0.001
[2025-07-29 10:17:16,490] [INFO] Epoch 3/50, ValAcc: 72.15%, TrainLoss: 1.0681, ValLoss: 0.9950, LR: 0.001
[2025-07-29 10:20:48,243] [INFO] Epoch 4/50, ValAcc: 73.23%, TrainLoss: 0.9652, ValLoss: 0.9536, LR: 0.001
[2025-07-29 10:23:04,931] [INFO] Epoch 5/50, ValAcc: 75.03%, TrainLoss: 0.9092, ValLoss: 0.8836, LR: 0.001
[2025-07-29 10:26:37,702] [INFO] Epoch 6/50, ValAcc: 75.60%, TrainLoss: 0.8787, ValLoss: 0.8663, LR: 0.001
[2025-07-29 10:30:10,389] [INFO] Epoch 7/50, ValAcc: 76.49%, TrainLoss: 0.8371, ValLoss: 0.8433, LR: 0.001
[2025-07-29 10:33:43,099] [INFO] Epoch 8/50, ValAcc: 76.22%, TrainLoss: 0.8086, ValLoss: 0.8286, LR: 0.001
[2025-07-29 10:37:15,789] [INFO] Epoch 9/50, ValAcc: 77.76%, TrainLoss: 0.7801, ValLoss: 0.7896, LR: 0.001
[2025-07-29 10:40:48,216] [INFO] Epoch 10/50, ValAcc: 78.15%, TrainLoss: 0.7673, ValLoss: 0.7690, LR: 0.001
[2025-07-29 10:44:20,644] [INFO] Epoch 11/50, ValAcc: 78.40%, TrainLoss: 0.7464, ValLoss: 0.7623, LR: 0.001
[2025-07-29 10:47:53,534] [INFO] Epoch 12/50, ValAcc: 78.36%, TrainLoss: 0.7225, ValLoss: 0.7732, LR: 0.001
[2025-07-29 10:51:26,207] [INFO] Epoch 13/50, ValAcc: 79.75%, TrainLoss: 0.7020, ValLoss: 0.7109, LR: 0.001
[2025-07-29 10:54:59,072] [INFO] Epoch 14/50, ValAcc: 77.67%, TrainLoss: 0.6980, ValLoss: 0.7588, LR: 0.001
[2025-07-29 10:58:31,721] [INFO] Epoch 15/50, ValAcc: 79.94%, TrainLoss: 0.6877, ValLoss: 0.7186, LR: 0.001
[2025-07-29 11:00:48,878] [INFO] Epoch 16/50, ValAcc: 78.19%, TrainLoss: 0.6641, ValLoss: 0.7723, LR: 0.001
[2025-07-29 11:04:14,946] [INFO] Epoch 17/50, ValAcc: 81.09%, TrainLoss: 0.6093, ValLoss: 0.6722, LR: 0.0005
[2025-07-29 11:07:47,574] [INFO] Epoch 18/50, ValAcc: 81.24%, TrainLoss: 0.5837, ValLoss: 0.6904, LR: 0.0005
[2025-07-29 11:11:20,254] [INFO] Epoch 19/50, ValAcc: 81.01%, TrainLoss: 0.5754, ValLoss: 0.7015, LR: 0.0005
[2025-07-29 11:14:52,970] [INFO] Epoch 20/50, ValAcc: 80.64%, TrainLoss: 0.5544, ValLoss: 0.7250, LR: 0.0005
[2025-07-29 11:18:25,798] [INFO] Epoch 21/50, ValAcc: 81.91%, TrainLoss: 0.5198, ValLoss: 0.6886, LR: 0.00025
[2025-07-29 11:21:58,418] [INFO] Epoch 22/50, ValAcc: 81.73%, TrainLoss: 0.5022, ValLoss: 0.6867, LR: 0.00025
[2025-07-29 11:25:31,098] [INFO] Epoch 23/50, ValAcc: 80.97%, TrainLoss: 0.4898, ValLoss: 0.7150, LR: 0.00025
[2025-07-29 11:29:03,820] [INFO] Epoch 24/50, ValAcc: 81.33%, TrainLoss: 0.4673, ValLoss: 0.7077, LR: 0.000125
[2025-07-29 11:31:52,161] [INFO] Epoch 25/50, ValAcc: 82.09%, TrainLoss: 0.4572, ValLoss: 0.7027, LR: 0.000125
[2025-07-29 11:33:47,127] [INFO] Epoch 26/50, ValAcc: 82.00%, TrainLoss: 0.4455, ValLoss: 0.7152, LR: 0.000125
[2025-07-29 11:35:41,978] [INFO] Epoch 27/50, ValAcc: 82.17%, TrainLoss: 0.4323, ValLoss: 0.7325, LR: 6.25e-05
[2025-07-29 11:35:41,978] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 11:35:53,585] [INFO] Namespace(path=['meta-free-apps/meta-ip_outlen/meta-ip_outlen.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777612.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777612.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipoutlen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8284,0.8328,0.8423,0.8298
[2025-07-29 11:35:53,586] [INFO] Training Fold 3/5
[2025-07-29 11:37:11,108] [INFO] Feature 0 normalized using token
[2025-07-29 11:37:11,108] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2025-07-29 11:37:11,165] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 11:37:11,165] [INFO] Training...
[2025-07-29 11:39:06,031] [INFO] Epoch 1/50, ValAcc: 34.83%, TrainLoss: 3.9581, ValLoss: 2.4357, LR: 0.001
[2025-07-29 11:41:01,688] [INFO] Epoch 2/50, ValAcc: 71.85%, TrainLoss: 1.5448, ValLoss: 1.0089, LR: 0.001
[2025-07-29 11:42:58,276] [INFO] Epoch 3/50, ValAcc: 74.12%, TrainLoss: 1.0368, ValLoss: 0.9187, LR: 0.001
[2025-07-29 11:44:55,008] [INFO] Epoch 4/50, ValAcc: 74.92%, TrainLoss: 0.9380, ValLoss: 0.8844, LR: 0.001
[2025-07-29 11:46:52,102] [INFO] Epoch 5/50, ValAcc: 76.93%, TrainLoss: 0.8798, ValLoss: 0.8403, LR: 0.001
[2025-07-29 11:48:48,992] [INFO] Epoch 6/50, ValAcc: 77.02%, TrainLoss: 0.8403, ValLoss: 0.8167, LR: 0.001
[2025-07-29 11:50:46,002] [INFO] Epoch 7/50, ValAcc: 76.84%, TrainLoss: 0.8063, ValLoss: 0.8202, LR: 0.001
[2025-07-29 11:52:42,913] [INFO] Epoch 8/50, ValAcc: 77.51%, TrainLoss: 0.7713, ValLoss: 0.7732, LR: 0.001
[2025-07-29 11:54:39,732] [INFO] Epoch 9/50, ValAcc: 78.52%, TrainLoss: 0.7497, ValLoss: 0.7645, LR: 0.001
[2025-07-29 11:56:36,756] [INFO] Epoch 10/50, ValAcc: 78.20%, TrainLoss: 0.7310, ValLoss: 0.7773, LR: 0.001
[2025-07-29 11:58:33,697] [INFO] Epoch 11/50, ValAcc: 78.49%, TrainLoss: 0.7169, ValLoss: 0.7358, LR: 0.001
[2025-07-29 12:00:30,658] [INFO] Epoch 12/50, ValAcc: 79.31%, TrainLoss: 0.6958, ValLoss: 0.7138, LR: 0.001
[2025-07-29 12:02:27,430] [INFO] Epoch 13/50, ValAcc: 80.05%, TrainLoss: 0.6927, ValLoss: 0.6860, LR: 0.001
[2025-07-29 12:04:24,420] [INFO] Epoch 14/50, ValAcc: 78.89%, TrainLoss: 0.6865, ValLoss: 0.7157, LR: 0.001
[2025-07-29 12:06:21,508] [INFO] Epoch 15/50, ValAcc: 80.71%, TrainLoss: 0.6658, ValLoss: 0.6909, LR: 0.001
[2025-07-29 12:08:18,541] [INFO] Epoch 16/50, ValAcc: 80.44%, TrainLoss: 0.6606, ValLoss: 0.6905, LR: 0.001
[2025-07-29 12:10:15,636] [INFO] Epoch 17/50, ValAcc: 82.07%, TrainLoss: 0.5960, ValLoss: 0.6554, LR: 0.0005
[2025-07-29 12:12:12,889] [INFO] Epoch 18/50, ValAcc: 81.36%, TrainLoss: 0.5726, ValLoss: 0.6580, LR: 0.0005
[2025-07-29 12:14:10,058] [INFO] Epoch 19/50, ValAcc: 81.96%, TrainLoss: 0.5583, ValLoss: 0.6600, LR: 0.0005
[2025-07-29 12:16:07,104] [INFO] Epoch 20/50, ValAcc: 81.43%, TrainLoss: 0.5546, ValLoss: 0.6849, LR: 0.0005
[2025-07-29 12:18:04,331] [INFO] Epoch 21/50, ValAcc: 82.71%, TrainLoss: 0.5181, ValLoss: 0.6512, LR: 0.00025
[2025-07-29 12:20:01,436] [INFO] Epoch 22/50, ValAcc: 82.93%, TrainLoss: 0.4994, ValLoss: 0.6830, LR: 0.00025
[2025-07-29 12:21:58,493] [INFO] Epoch 23/50, ValAcc: 82.54%, TrainLoss: 0.4887, ValLoss: 0.7003, LR: 0.00025
[2025-07-29 12:23:55,656] [INFO] Epoch 24/50, ValAcc: 83.27%, TrainLoss: 0.4807, ValLoss: 0.6920, LR: 0.00025
[2025-07-29 12:25:52,874] [INFO] Epoch 25/50, ValAcc: 82.97%, TrainLoss: 0.4625, ValLoss: 0.6869, LR: 0.000125
[2025-07-29 12:27:50,027] [INFO] Epoch 26/50, ValAcc: 83.23%, TrainLoss: 0.4510, ValLoss: 0.7008, LR: 0.000125
[2025-07-29 12:29:47,001] [INFO] Epoch 27/50, ValAcc: 83.16%, TrainLoss: 0.4411, ValLoss: 0.7042, LR: 0.000125
[2025-07-29 12:31:44,016] [INFO] Epoch 28/50, ValAcc: 82.99%, TrainLoss: 0.4277, ValLoss: 0.7111, LR: 6.25e-05
[2025-07-29 12:31:44,016] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 12:31:55,831] [INFO] Namespace(path=['meta-free-apps/meta-ip_outlen/meta-ip_outlen.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777612.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777612.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipoutlen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8352,0.8390,0.8507,0.8347
[2025-07-29 12:31:55,832] [INFO] Training Fold 4/5
[2025-07-29 12:33:15,777] [INFO] Feature 0 normalized using token
[2025-07-29 12:33:15,777] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2025-07-29 12:33:15,834] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 12:33:15,834] [INFO] Training...
[2025-07-29 12:35:13,142] [INFO] Epoch 1/50, ValAcc: 72.19%, TrainLoss: 2.3190, ValLoss: 0.9706, LR: 0.001
[2025-07-29 12:37:10,210] [INFO] Epoch 2/50, ValAcc: 76.64%, TrainLoss: 0.9007, ValLoss: 0.7846, LR: 0.001
[2025-07-29 12:39:06,981] [INFO] Epoch 3/50, ValAcc: 78.15%, TrainLoss: 0.7608, ValLoss: 0.7399, LR: 0.001
[2025-07-29 12:41:03,661] [INFO] Epoch 4/50, ValAcc: 78.69%, TrainLoss: 0.7070, ValLoss: 0.7400, LR: 0.001
[2025-07-29 12:43:00,240] [INFO] Epoch 5/50, ValAcc: 80.08%, TrainLoss: 0.6665, ValLoss: 0.6694, LR: 0.001
[2025-07-29 12:44:56,969] [INFO] Epoch 6/50, ValAcc: 78.51%, TrainLoss: 0.6376, ValLoss: 0.7145, LR: 0.001
[2025-07-29 12:46:53,715] [INFO] Epoch 7/50, ValAcc: 79.56%, TrainLoss: 0.6123, ValLoss: 0.6808, LR: 0.001
[2025-07-29 12:48:50,705] [INFO] Epoch 8/50, ValAcc: 81.41%, TrainLoss: 0.5954, ValLoss: 0.6526, LR: 0.001
[2025-07-29 12:50:47,797] [INFO] Epoch 9/50, ValAcc: 81.07%, TrainLoss: 0.5916, ValLoss: 0.6356, LR: 0.001
[2025-07-29 12:52:45,109] [INFO] Epoch 10/50, ValAcc: 82.24%, TrainLoss: 0.5654, ValLoss: 0.6675, LR: 0.001
[2025-07-29 12:54:42,447] [INFO] Epoch 11/50, ValAcc: 82.33%, TrainLoss: 0.5566, ValLoss: 0.6437, LR: 0.001
[2025-07-29 12:56:39,602] [INFO] Epoch 12/50, ValAcc: 81.75%, TrainLoss: 0.5566, ValLoss: 0.6468, LR: 0.001
[2025-07-29 12:58:36,916] [INFO] Epoch 13/50, ValAcc: 83.60%, TrainLoss: 0.4769, ValLoss: 0.5907, LR: 0.0005
[2025-07-29 13:00:34,182] [INFO] Epoch 14/50, ValAcc: 83.16%, TrainLoss: 0.4479, ValLoss: 0.6054, LR: 0.0005
[2025-07-29 13:02:31,358] [INFO] Epoch 15/50, ValAcc: 83.85%, TrainLoss: 0.4306, ValLoss: 0.6200, LR: 0.0005
[2025-07-29 13:04:28,504] [INFO] Epoch 16/50, ValAcc: 83.61%, TrainLoss: 0.4230, ValLoss: 0.6372, LR: 0.0005
[2025-07-29 13:06:25,791] [INFO] Epoch 17/50, ValAcc: 84.26%, TrainLoss: 0.3832, ValLoss: 0.6130, LR: 0.00025
[2025-07-29 13:08:22,867] [INFO] Epoch 18/50, ValAcc: 83.98%, TrainLoss: 0.3600, ValLoss: 0.6333, LR: 0.00025
[2025-07-29 13:10:20,024] [INFO] Epoch 19/50, ValAcc: 84.08%, TrainLoss: 0.3506, ValLoss: 0.6464, LR: 0.00025
[2025-07-29 13:12:17,220] [INFO] Epoch 20/50, ValAcc: 84.46%, TrainLoss: 0.3271, ValLoss: 0.6551, LR: 0.000125
[2025-07-29 13:14:14,287] [INFO] Epoch 21/50, ValAcc: 84.46%, TrainLoss: 0.3135, ValLoss: 0.6991, LR: 0.000125
[2025-07-29 13:16:11,013] [INFO] Epoch 22/50, ValAcc: 84.60%, TrainLoss: 0.3060, ValLoss: 0.7129, LR: 0.000125
[2025-07-29 13:18:07,688] [INFO] Epoch 23/50, ValAcc: 84.96%, TrainLoss: 0.2893, ValLoss: 0.7209, LR: 6.25e-05
[2025-07-29 13:18:07,688] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 13:18:19,407] [INFO] Namespace(path=['meta-free-apps/meta-ip_outlen/meta-ip_outlen.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777612.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777612.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipoutlen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8460,0.8475,0.8506,0.8468
[2025-07-29 13:18:19,408] [INFO] Training Fold 5/5
[2025-07-29 13:19:36,230] [INFO] Feature 0 normalized using token
[2025-07-29 13:19:36,230] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2025-07-29 13:19:36,312] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 13:19:36,312] [INFO] Training...
[2025-07-29 13:21:33,051] [INFO] Epoch 1/50, ValAcc: 62.09%, TrainLoss: 3.0369, ValLoss: 1.3309, LR: 0.001
[2025-07-29 13:23:29,592] [INFO] Epoch 2/50, ValAcc: 71.05%, TrainLoss: 1.2455, ValLoss: 1.0626, LR: 0.001
[2025-07-29 13:25:25,983] [INFO] Epoch 3/50, ValAcc: 72.89%, TrainLoss: 1.0278, ValLoss: 0.9408, LR: 0.001
[2025-07-29 13:27:22,528] [INFO] Epoch 4/50, ValAcc: 73.28%, TrainLoss: 0.9486, ValLoss: 0.9211, LR: 0.001
[2025-07-29 13:29:19,051] [INFO] Epoch 5/50, ValAcc: 73.33%, TrainLoss: 0.8960, ValLoss: 0.8751, LR: 0.001
[2025-07-29 13:31:15,473] [INFO] Epoch 6/50, ValAcc: 74.57%, TrainLoss: 0.8609, ValLoss: 0.8715, LR: 0.001
[2025-07-29 13:33:11,947] [INFO] Epoch 7/50, ValAcc: 74.22%, TrainLoss: 0.8392, ValLoss: 0.8702, LR: 0.001
[2025-07-29 13:35:08,475] [INFO] Epoch 8/50, ValAcc: 75.17%, TrainLoss: 0.8146, ValLoss: 0.8531, LR: 0.001
[2025-07-29 13:37:04,938] [INFO] Epoch 9/50, ValAcc: 76.89%, TrainLoss: 0.7849, ValLoss: 0.7881, LR: 0.001
[2025-07-29 13:39:01,280] [INFO] Epoch 10/50, ValAcc: 77.42%, TrainLoss: 0.7682, ValLoss: 0.7952, LR: 0.001
[2025-07-29 13:40:57,746] [INFO] Epoch 11/50, ValAcc: 76.98%, TrainLoss: 0.7476, ValLoss: 0.7893, LR: 0.001
[2025-07-29 13:42:54,214] [INFO] Epoch 12/50, ValAcc: 77.35%, TrainLoss: 0.7252, ValLoss: 0.7681, LR: 0.001
[2025-07-29 13:44:50,601] [INFO] Epoch 13/50, ValAcc: 75.75%, TrainLoss: 0.7103, ValLoss: 0.8356, LR: 0.001
[2025-07-29 13:46:47,094] [INFO] Epoch 14/50, ValAcc: 76.72%, TrainLoss: 0.6994, ValLoss: 0.7896, LR: 0.001
[2025-07-29 13:48:43,572] [INFO] Epoch 15/50, ValAcc: 78.13%, TrainLoss: 0.6905, ValLoss: 0.7296, LR: 0.001
[2025-07-29 13:50:40,073] [INFO] Epoch 16/50, ValAcc: 76.21%, TrainLoss: 0.6740, ValLoss: 0.8508, LR: 0.001
[2025-07-29 13:52:35,742] [INFO] Epoch 17/50, ValAcc: 78.66%, TrainLoss: 0.6707, ValLoss: 0.7419, LR: 0.001
[2025-07-29 13:54:30,830] [INFO] Epoch 18/50, ValAcc: 79.93%, TrainLoss: 0.6694, ValLoss: 0.7079, LR: 0.001
[2025-07-29 13:56:26,294] [INFO] Epoch 19/50, ValAcc: 80.67%, TrainLoss: 0.6595, ValLoss: 0.7061, LR: 0.001
[2025-07-29 13:58:21,800] [INFO] Epoch 20/50, ValAcc: 80.52%, TrainLoss: 0.6470, ValLoss: 0.7341, LR: 0.001
[2025-07-29 14:00:17,415] [INFO] Epoch 21/50, ValAcc: 79.40%, TrainLoss: 0.6384, ValLoss: 0.7344, LR: 0.001
[2025-07-29 14:02:12,996] [INFO] Epoch 22/50, ValAcc: 78.27%, TrainLoss: 0.6338, ValLoss: 0.8134, LR: 0.001
[2025-07-29 14:04:08,463] [INFO] Epoch 23/50, ValAcc: 80.90%, TrainLoss: 0.5809, ValLoss: 0.6916, LR: 0.0005
[2025-07-29 14:06:03,823] [INFO] Epoch 24/50, ValAcc: 81.29%, TrainLoss: 0.5578, ValLoss: 0.6678, LR: 0.0005
[2025-07-29 14:07:59,273] [INFO] Epoch 25/50, ValAcc: 81.34%, TrainLoss: 0.5473, ValLoss: 0.6900, LR: 0.0005
[2025-07-29 14:09:54,713] [INFO] Epoch 26/50, ValAcc: 80.73%, TrainLoss: 0.5345, ValLoss: 0.7314, LR: 0.0005
[2025-07-29 14:11:50,117] [INFO] Epoch 27/50, ValAcc: 81.41%, TrainLoss: 0.5277, ValLoss: 0.6731, LR: 0.0005
[2025-07-29 14:13:45,379] [INFO] Epoch 28/50, ValAcc: 81.55%, TrainLoss: 0.4945, ValLoss: 0.7037, LR: 0.00025
[2025-07-29 14:15:40,676] [INFO] Epoch 29/50, ValAcc: 81.69%, TrainLoss: 0.4792, ValLoss: 0.6898, LR: 0.00025
[2025-07-29 14:17:36,154] [INFO] Epoch 30/50, ValAcc: 82.69%, TrainLoss: 0.4748, ValLoss: 0.7013, LR: 0.00025
[2025-07-29 14:19:31,576] [INFO] Epoch 31/50, ValAcc: 82.64%, TrainLoss: 0.4563, ValLoss: 0.7083, LR: 0.000125
[2025-07-29 14:21:27,107] [INFO] Epoch 32/50, ValAcc: 82.47%, TrainLoss: 0.4422, ValLoss: 0.6912, LR: 0.000125
[2025-07-29 14:23:22,588] [INFO] Epoch 33/50, ValAcc: 82.33%, TrainLoss: 0.4349, ValLoss: 0.7138, LR: 0.000125
[2025-07-29 14:25:18,071] [INFO] Epoch 34/50, ValAcc: 83.09%, TrainLoss: 0.4271, ValLoss: 0.6854, LR: 6.25e-05
[2025-07-29 14:25:18,071] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 14:25:29,679] [INFO] Namespace(path=['meta-free-apps/meta-ip_outlen/meta-ip_outlen.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777612.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777612.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipoutlen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8346,0.8345,0.8437,0.8335
[2025-07-29 14:25:30,424] [INFO] [(0.8369134268299899, 0.8391272022656399, 0.8458088377386431, 0.8372831569519512), (0.828392212556956, 0.832754721477852, 0.8423452452838665, 0.829755518190254), (0.8352467747662445, 0.8390284487294283, 0.8506848588057982, 0.8346782195186075), (0.8459581015504793, 0.8474733397398054, 0.8506438855784275, 0.8467745655228502), (0.8345958101550479, 0.8344861757304421, 0.8437085607073675, 0.8335143732718808)]
