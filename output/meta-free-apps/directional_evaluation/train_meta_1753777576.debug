[2025-07-29 08:26:18,407] [INFO] Namespace(path=['meta-free-apps/meta-ip_inlen/meta-ip_inlen.csv'], pktcount=1000, importance=False, kfold=5, model=['gru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777576.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777576.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'))
[2025-07-29 08:29:23,489] [INFO] Processed data from meta-free-apps/meta-ip_inlen/meta-ip_inlen.csv:
[2025-07-29 08:29:23,489] [INFO] (84492, 1000)
[2025-07-29 08:29:23,489] [INFO] [['94' '52' '83' ... <NA> <NA> <NA>]
 ['87' '241' '106' ... <NA> <NA> <NA>]
 ['87' '240' '106' ... <NA> <NA> <NA>]
 ...
 ['87' '653' '52' ... '653' '76' '52']
 ['60' '52' '270' ... '89' '83' '60']
 ['52' '223' '126' ... '94' '383' '181']]
[2025-07-29 08:29:23,828] [INFO] Training Fold 1/5
[2025-07-29 08:30:56,277] [INFO] Feature 0 normalized using token
[2025-07-29 08:30:56,277] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2025-07-29 08:30:56,344] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 08:30:56,344] [INFO] Training...
[2025-07-29 08:32:09,758] [INFO] Epoch 1/50, ValAcc: 92.28%, TrainLoss: 1.6919, ValLoss: 0.3017, LR: 0.001
[2025-07-29 08:33:50,516] [INFO] Epoch 2/50, ValAcc: 94.34%, TrainLoss: 0.2954, ValLoss: 0.2266, LR: 0.001
[2025-07-29 08:35:29,379] [INFO] Epoch 3/50, ValAcc: 94.39%, TrainLoss: 0.2368, ValLoss: 0.2261, LR: 0.001
[2025-07-29 08:37:09,252] [INFO] Epoch 4/50, ValAcc: 94.83%, TrainLoss: 0.2083, ValLoss: 0.2133, LR: 0.001
[2025-07-29 08:38:48,614] [INFO] Epoch 5/50, ValAcc: 94.60%, TrainLoss: 0.2017, ValLoss: 0.2281, LR: 0.001
[2025-07-29 08:40:28,557] [INFO] Epoch 6/50, ValAcc: 94.93%, TrainLoss: 0.1925, ValLoss: 0.1995, LR: 0.001
[2025-07-29 08:42:08,137] [INFO] Epoch 7/50, ValAcc: 95.15%, TrainLoss: 0.1843, ValLoss: 0.2048, LR: 0.001
[2025-07-29 08:43:47,926] [INFO] Epoch 8/50, ValAcc: 94.86%, TrainLoss: 0.1752, ValLoss: 0.2086, LR: 0.001
[2025-07-29 08:45:27,923] [INFO] Epoch 9/50, ValAcc: 95.42%, TrainLoss: 0.1712, ValLoss: 0.1916, LR: 0.001
[2025-07-29 08:47:07,157] [INFO] Epoch 10/50, ValAcc: 95.15%, TrainLoss: 0.1708, ValLoss: 0.1950, LR: 0.001
[2025-07-29 08:48:47,147] [INFO] Epoch 11/50, ValAcc: 95.34%, TrainLoss: 0.1664, ValLoss: 0.1933, LR: 0.001
[2025-07-29 08:50:26,479] [INFO] Epoch 12/50, ValAcc: 95.18%, TrainLoss: 0.1623, ValLoss: 0.2107, LR: 0.001
[2025-07-29 08:52:06,475] [INFO] Epoch 13/50, ValAcc: 95.34%, TrainLoss: 0.1372, ValLoss: 0.2079, LR: 0.0005
[2025-07-29 08:53:45,780] [INFO] Epoch 14/50, ValAcc: 95.20%, TrainLoss: 0.1307, ValLoss: 0.1843, LR: 0.0005
[2025-07-29 08:55:25,835] [INFO] Epoch 15/50, ValAcc: 95.47%, TrainLoss: 0.1295, ValLoss: 0.1966, LR: 0.0005
[2025-07-29 08:57:05,178] [INFO] Epoch 16/50, ValAcc: 95.42%, TrainLoss: 0.1275, ValLoss: 0.2069, LR: 0.0005
[2025-07-29 08:58:45,174] [INFO] Epoch 17/50, ValAcc: 95.24%, TrainLoss: 0.1303, ValLoss: 0.2018, LR: 0.0005
[2025-07-29 09:00:24,458] [INFO] Epoch 18/50, ValAcc: 95.72%, TrainLoss: 0.1189, ValLoss: 0.2052, LR: 0.00025
[2025-07-29 09:02:04,495] [INFO] Epoch 19/50, ValAcc: 95.72%, TrainLoss: 0.1143, ValLoss: 0.2099, LR: 0.00025
[2025-07-29 09:03:43,755] [INFO] Epoch 20/50, ValAcc: 95.44%, TrainLoss: 0.1117, ValLoss: 0.2127, LR: 0.00025
[2025-07-29 09:05:23,779] [INFO] Epoch 21/50, ValAcc: 95.44%, TrainLoss: 0.1074, ValLoss: 0.2297, LR: 0.000125
[2025-07-29 09:07:03,206] [INFO] Epoch 22/50, ValAcc: 95.68%, TrainLoss: 0.1059, ValLoss: 0.2202, LR: 0.000125
[2025-07-29 09:08:43,205] [INFO] Epoch 23/50, ValAcc: 95.61%, TrainLoss: 0.1031, ValLoss: 0.2227, LR: 0.000125
[2025-07-29 09:10:23,166] [INFO] Epoch 24/50, ValAcc: 95.50%, TrainLoss: 0.1019, ValLoss: 0.2276, LR: 6.25e-05
[2025-07-29 09:10:23,167] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 09:10:33,575] [INFO] Namespace(path=['meta-free-apps/meta-ip_inlen/meta-ip_inlen.csv'], pktcount=1000, importance=False, kfold=5, model=['gru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777576.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777576.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipinlen_token_gru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9584,0.9574,0.9619,0.9581
[2025-07-29 09:10:33,576] [INFO] Training Fold 2/5
[2025-07-29 09:12:06,038] [INFO] Feature 0 normalized using token
[2025-07-29 09:12:06,038] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2025-07-29 09:12:06,073] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 09:12:06,073] [INFO] Training...
[2025-07-29 09:13:45,465] [INFO] Epoch 1/50, ValAcc: 92.72%, TrainLoss: 1.6777, ValLoss: 0.2925, LR: 0.001
[2025-07-29 09:15:25,445] [INFO] Epoch 2/50, ValAcc: 93.87%, TrainLoss: 0.2760, ValLoss: 0.2488, LR: 0.001
[2025-07-29 09:17:04,870] [INFO] Epoch 3/50, ValAcc: 94.22%, TrainLoss: 0.2179, ValLoss: 0.2687, LR: 0.001
[2025-07-29 09:18:44,736] [INFO] Epoch 4/50, ValAcc: 94.56%, TrainLoss: 0.2032, ValLoss: 0.2337, LR: 0.001
[2025-07-29 09:20:23,966] [INFO] Epoch 5/50, ValAcc: 94.56%, TrainLoss: 0.1949, ValLoss: 0.2254, LR: 0.001
[2025-07-29 09:22:03,884] [INFO] Epoch 6/50, ValAcc: 94.46%, TrainLoss: 0.1853, ValLoss: 0.2361, LR: 0.001
[2025-07-29 09:23:43,223] [INFO] Epoch 7/50, ValAcc: 94.99%, TrainLoss: 0.1825, ValLoss: 0.2162, LR: 0.001
[2025-07-29 09:25:23,120] [INFO] Epoch 8/50, ValAcc: 94.40%, TrainLoss: 0.1729, ValLoss: 0.2526, LR: 0.001
[2025-07-29 09:27:02,440] [INFO] Epoch 9/50, ValAcc: 94.86%, TrainLoss: 0.1697, ValLoss: 0.2461, LR: 0.001
[2025-07-29 09:28:42,320] [INFO] Epoch 10/50, ValAcc: 94.92%, TrainLoss: 0.1644, ValLoss: 0.2275, LR: 0.001
[2025-07-29 09:30:22,406] [INFO] Epoch 11/50, ValAcc: 95.35%, TrainLoss: 0.1419, ValLoss: 0.2244, LR: 0.0005
[2025-07-29 09:32:01,601] [INFO] Epoch 12/50, ValAcc: 95.25%, TrainLoss: 0.1325, ValLoss: 0.2319, LR: 0.0005
[2025-07-29 09:33:41,471] [INFO] Epoch 13/50, ValAcc: 95.33%, TrainLoss: 0.1328, ValLoss: 0.2272, LR: 0.0005
[2025-07-29 09:35:20,804] [INFO] Epoch 14/50, ValAcc: 95.46%, TrainLoss: 0.1223, ValLoss: 0.2267, LR: 0.00025
[2025-07-29 09:37:00,786] [INFO] Epoch 15/50, ValAcc: 95.46%, TrainLoss: 0.1212, ValLoss: 0.2357, LR: 0.00025
[2025-07-29 09:38:40,038] [INFO] Epoch 16/50, ValAcc: 95.56%, TrainLoss: 0.1182, ValLoss: 0.2313, LR: 0.00025
[2025-07-29 09:40:19,892] [INFO] Epoch 17/50, ValAcc: 95.55%, TrainLoss: 0.1131, ValLoss: 0.2264, LR: 0.000125
[2025-07-29 09:41:59,135] [INFO] Epoch 18/50, ValAcc: 95.63%, TrainLoss: 0.1104, ValLoss: 0.2362, LR: 0.000125
[2025-07-29 09:43:39,084] [INFO] Epoch 19/50, ValAcc: 95.64%, TrainLoss: 0.1087, ValLoss: 0.2312, LR: 0.000125
[2025-07-29 09:45:18,310] [INFO] Epoch 20/50, ValAcc: 95.64%, TrainLoss: 0.1051, ValLoss: 0.2443, LR: 6.25e-05
[2025-07-29 09:45:18,310] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 09:45:29,160] [INFO] Namespace(path=['meta-free-apps/meta-ip_inlen/meta-ip_inlen.csv'], pktcount=1000, importance=False, kfold=5, model=['gru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777576.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777576.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipinlen_token_gru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9580,0.9587,0.9622,0.9583
[2025-07-29 09:45:29,161] [INFO] Training Fold 3/5
[2025-07-29 09:47:02,806] [INFO] Feature 0 normalized using token
[2025-07-29 09:47:02,806] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2025-07-29 09:47:02,852] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 09:47:02,852] [INFO] Training...
[2025-07-29 09:48:42,725] [INFO] Epoch 1/50, ValAcc: 93.59%, TrainLoss: 1.8152, ValLoss: 0.2559, LR: 0.001
[2025-07-29 09:50:22,667] [INFO] Epoch 2/50, ValAcc: 94.75%, TrainLoss: 0.2746, ValLoss: 0.1994, LR: 0.001
[2025-07-29 09:52:01,973] [INFO] Epoch 3/50, ValAcc: 94.85%, TrainLoss: 0.2065, ValLoss: 0.2151, LR: 0.001
[2025-07-29 09:53:41,940] [INFO] Epoch 4/50, ValAcc: 94.69%, TrainLoss: 0.1895, ValLoss: 0.2155, LR: 0.001
[2025-07-29 09:55:21,261] [INFO] Epoch 5/50, ValAcc: 95.03%, TrainLoss: 0.1858, ValLoss: 0.1989, LR: 0.001
[2025-07-29 09:57:01,275] [INFO] Epoch 6/50, ValAcc: 95.30%, TrainLoss: 0.1798, ValLoss: 0.1922, LR: 0.001
[2025-07-29 09:58:40,594] [INFO] Epoch 7/50, ValAcc: 95.29%, TrainLoss: 0.1701, ValLoss: 0.1954, LR: 0.001
[2025-07-29 10:00:20,567] [INFO] Epoch 8/50, ValAcc: 95.55%, TrainLoss: 0.1667, ValLoss: 0.1833, LR: 0.001
[2025-07-29 10:01:59,870] [INFO] Epoch 9/50, ValAcc: 95.66%, TrainLoss: 0.1664, ValLoss: 0.2077, LR: 0.001
[2025-07-29 10:03:39,743] [INFO] Epoch 10/50, ValAcc: 94.93%, TrainLoss: 0.1555, ValLoss: 0.2057, LR: 0.001
[2025-07-29 10:05:14,865] [INFO] Epoch 11/50, ValAcc: 95.36%, TrainLoss: 0.1610, ValLoss: 0.1928, LR: 0.001
[2025-07-29 10:06:08,890] [INFO] Epoch 12/50, ValAcc: 95.37%, TrainLoss: 0.1340, ValLoss: 0.1925, LR: 0.0005
[2025-07-29 10:07:25,288] [INFO] Epoch 13/50, ValAcc: 95.59%, TrainLoss: 0.1239, ValLoss: 0.1979, LR: 0.0005
[2025-07-29 10:09:05,793] [INFO] Epoch 14/50, ValAcc: 95.70%, TrainLoss: 0.1273, ValLoss: 0.1982, LR: 0.0005
[2025-07-29 10:10:45,607] [INFO] Epoch 15/50, ValAcc: 95.41%, TrainLoss: 0.1172, ValLoss: 0.2008, LR: 0.00025
[2025-07-29 10:12:26,059] [INFO] Epoch 16/50, ValAcc: 95.46%, TrainLoss: 0.1132, ValLoss: 0.2218, LR: 0.00025
[2025-07-29 10:14:05,996] [INFO] Epoch 17/50, ValAcc: 95.49%, TrainLoss: 0.1115, ValLoss: 0.2117, LR: 0.00025
[2025-07-29 10:15:46,475] [INFO] Epoch 18/50, ValAcc: 95.69%, TrainLoss: 0.1069, ValLoss: 0.2110, LR: 0.000125
[2025-07-29 10:17:26,233] [INFO] Epoch 19/50, ValAcc: 95.70%, TrainLoss: 0.1036, ValLoss: 0.2082, LR: 0.000125
[2025-07-29 10:19:06,692] [INFO] Epoch 20/50, ValAcc: 95.64%, TrainLoss: 0.1016, ValLoss: 0.2235, LR: 0.000125
[2025-07-29 10:20:46,740] [INFO] Epoch 21/50, ValAcc: 95.61%, TrainLoss: 0.0982, ValLoss: 0.2276, LR: 6.25e-05
[2025-07-29 10:20:46,741] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 10:20:57,528] [INFO] Namespace(path=['meta-free-apps/meta-ip_inlen/meta-ip_inlen.csv'], pktcount=1000, importance=False, kfold=5, model=['gru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777576.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777576.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipinlen_token_gru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9563,0.9570,0.9593,0.9564
[2025-07-29 10:20:57,529] [INFO] Training Fold 4/5
[2025-07-29 10:22:29,740] [INFO] Feature 0 normalized using token
[2025-07-29 10:22:29,741] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2025-07-29 10:22:29,797] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 10:22:29,797] [INFO] Training...
[2025-07-29 10:24:09,559] [INFO] Epoch 1/50, ValAcc: 92.21%, TrainLoss: 1.8037, ValLoss: 0.3071, LR: 0.001
[2025-07-29 10:25:50,057] [INFO] Epoch 2/50, ValAcc: 93.67%, TrainLoss: 0.2908, ValLoss: 0.2559, LR: 0.001
[2025-07-29 10:27:29,826] [INFO] Epoch 3/50, ValAcc: 94.43%, TrainLoss: 0.2230, ValLoss: 0.2330, LR: 0.001
[2025-07-29 10:29:10,356] [INFO] Epoch 4/50, ValAcc: 94.72%, TrainLoss: 0.1978, ValLoss: 0.2194, LR: 0.001
[2025-07-29 10:30:50,107] [INFO] Epoch 5/50, ValAcc: 94.88%, TrainLoss: 0.1901, ValLoss: 0.2130, LR: 0.001
[2025-07-29 10:32:30,633] [INFO] Epoch 6/50, ValAcc: 95.33%, TrainLoss: 0.1844, ValLoss: 0.1826, LR: 0.001
[2025-07-29 10:34:10,517] [INFO] Epoch 7/50, ValAcc: 95.23%, TrainLoss: 0.1755, ValLoss: 0.1877, LR: 0.001
[2025-07-29 10:35:51,033] [INFO] Epoch 8/50, ValAcc: 95.42%, TrainLoss: 0.1754, ValLoss: 0.1801, LR: 0.001
[2025-07-29 10:37:30,791] [INFO] Epoch 9/50, ValAcc: 94.91%, TrainLoss: 0.1640, ValLoss: 0.2043, LR: 0.001
[2025-07-29 10:39:11,249] [INFO] Epoch 10/50, ValAcc: 95.12%, TrainLoss: 0.1579, ValLoss: 0.2193, LR: 0.001
[2025-07-29 10:40:51,081] [INFO] Epoch 11/50, ValAcc: 94.90%, TrainLoss: 0.1624, ValLoss: 0.2041, LR: 0.001
[2025-07-29 10:42:31,520] [INFO] Epoch 12/50, ValAcc: 95.47%, TrainLoss: 0.1358, ValLoss: 0.1765, LR: 0.0005
[2025-07-29 10:44:11,617] [INFO] Epoch 13/50, ValAcc: 95.44%, TrainLoss: 0.1245, ValLoss: 0.1887, LR: 0.0005
[2025-07-29 10:45:51,304] [INFO] Epoch 14/50, ValAcc: 95.49%, TrainLoss: 0.1231, ValLoss: 0.1877, LR: 0.0005
[2025-07-29 10:47:31,898] [INFO] Epoch 15/50, ValAcc: 95.68%, TrainLoss: 0.1226, ValLoss: 0.1864, LR: 0.0005
[2025-07-29 10:49:11,396] [INFO] Epoch 16/50, ValAcc: 95.63%, TrainLoss: 0.1123, ValLoss: 0.1968, LR: 0.00025
[2025-07-29 10:50:51,395] [INFO] Epoch 17/50, ValAcc: 95.74%, TrainLoss: 0.1074, ValLoss: 0.2022, LR: 0.00025
[2025-07-29 10:52:30,888] [INFO] Epoch 18/50, ValAcc: 95.73%, TrainLoss: 0.1063, ValLoss: 0.1906, LR: 0.00025
[2025-07-29 10:54:10,926] [INFO] Epoch 19/50, ValAcc: 95.86%, TrainLoss: 0.1001, ValLoss: 0.2059, LR: 0.000125
[2025-07-29 10:55:50,427] [INFO] Epoch 20/50, ValAcc: 95.86%, TrainLoss: 0.0968, ValLoss: 0.2158, LR: 0.000125
[2025-07-29 10:57:30,479] [INFO] Epoch 21/50, ValAcc: 95.83%, TrainLoss: 0.0957, ValLoss: 0.2207, LR: 0.000125
[2025-07-29 10:59:09,955] [INFO] Epoch 22/50, ValAcc: 95.92%, TrainLoss: 0.0914, ValLoss: 0.2180, LR: 6.25e-05
[2025-07-29 10:59:09,955] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 10:59:20,791] [INFO] Namespace(path=['meta-free-apps/meta-ip_inlen/meta-ip_inlen.csv'], pktcount=1000, importance=False, kfold=5, model=['gru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777576.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777576.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipinlen_token_gru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9578,0.9585,0.9608,0.9578
[2025-07-29 10:59:20,792] [INFO] Training Fold 5/5
[2025-07-29 11:00:56,643] [INFO] Feature 0 normalized using token
[2025-07-29 11:00:56,644] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2025-07-29 11:00:56,678] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-29 11:00:56,678] [INFO] Training...
[2025-07-29 11:02:36,663] [INFO] Epoch 1/50, ValAcc: 91.03%, TrainLoss: 1.8032, ValLoss: 0.3765, LR: 0.001
[2025-07-29 11:04:16,197] [INFO] Epoch 2/50, ValAcc: 93.54%, TrainLoss: 0.3223, ValLoss: 0.2768, LR: 0.001
[2025-07-29 11:05:56,300] [INFO] Epoch 3/50, ValAcc: 94.15%, TrainLoss: 0.2490, ValLoss: 0.2404, LR: 0.001
[2025-07-29 11:07:36,367] [INFO] Epoch 4/50, ValAcc: 94.30%, TrainLoss: 0.2115, ValLoss: 0.2198, LR: 0.001
[2025-07-29 11:09:15,787] [INFO] Epoch 5/50, ValAcc: 94.47%, TrainLoss: 0.1959, ValLoss: 0.2250, LR: 0.001
[2025-07-29 11:10:55,835] [INFO] Epoch 6/50, ValAcc: 94.53%, TrainLoss: 0.2001, ValLoss: 0.2411, LR: 0.001
[2025-07-29 11:12:35,255] [INFO] Epoch 7/50, ValAcc: 94.62%, TrainLoss: 0.1943, ValLoss: 0.2351, LR: 0.001
[2025-07-29 11:14:15,306] [INFO] Epoch 8/50, ValAcc: 95.47%, TrainLoss: 0.1548, ValLoss: 0.2055, LR: 0.0005
[2025-07-29 11:15:54,736] [INFO] Epoch 9/50, ValAcc: 95.29%, TrainLoss: 0.1456, ValLoss: 0.2281, LR: 0.0005
[2025-07-29 11:17:34,693] [INFO] Epoch 10/50, ValAcc: 94.98%, TrainLoss: 0.1471, ValLoss: 0.2395, LR: 0.0005
[2025-07-29 11:19:14,144] [INFO] Epoch 11/50, ValAcc: 95.04%, TrainLoss: 0.1431, ValLoss: 0.2483, LR: 0.0005
[2025-07-29 11:20:54,188] [INFO] Epoch 12/50, ValAcc: 95.24%, TrainLoss: 0.1334, ValLoss: 0.2294, LR: 0.00025
[2025-07-29 11:22:33,685] [INFO] Epoch 13/50, ValAcc: 95.24%, TrainLoss: 0.1265, ValLoss: 0.2336, LR: 0.00025
[2025-07-29 11:24:13,858] [INFO] Epoch 14/50, ValAcc: 95.55%, TrainLoss: 0.1227, ValLoss: 0.2401, LR: 0.00025
[2025-07-29 11:25:53,277] [INFO] Epoch 15/50, ValAcc: 95.55%, TrainLoss: 0.1172, ValLoss: 0.2368, LR: 0.000125
[2025-07-29 11:27:33,343] [INFO] Epoch 16/50, ValAcc: 95.40%, TrainLoss: 0.1142, ValLoss: 0.2433, LR: 0.000125
[2025-07-29 11:29:12,865] [INFO] Epoch 17/50, ValAcc: 95.38%, TrainLoss: 0.1112, ValLoss: 0.2538, LR: 0.000125
[2025-07-29 11:30:52,839] [INFO] Epoch 18/50, ValAcc: 95.43%, TrainLoss: 0.1075, ValLoss: 0.2614, LR: 6.25e-05
[2025-07-29 11:30:52,839] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-29 11:31:03,659] [INFO] Namespace(path=['meta-free-apps/meta-ip_inlen/meta-ip_inlen.csv'], pktcount=1000, importance=False, kfold=5, model=['gru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753777576.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753777576.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='ipinlen_token_gru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9580,0.9582,0.9612,0.9578
[2025-07-29 11:31:04,532] [INFO] [(0.9583999053198414, 0.9574411131552747, 0.9618907228345336, 0.9581460876928489), (0.9579856796260133, 0.9587497099481888, 0.962165596831448, 0.9583260050130574), (0.9563261924488106, 0.957023802778848, 0.9592946696738552, 0.956422334695103), (0.9578056574742573, 0.9585390418784571, 0.9608220778894375, 0.957762213250336), (0.9580423718783287, 0.9581879615824515, 0.9611947705540376, 0.9577770490710505)]
