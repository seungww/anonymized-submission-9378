=== Step4. Script Execution Started at Sun Jul 27 12:29:13 AM UTC 2025 ===
Base directory: meta-free-apps
Data prefix: meta
Output directory: output/meta-free-apps/train
[INFO] ip_len â†’ norm: token, model: bigru
Running python vrscanner.py --train --debug_path output/meta-free-apps/train/train_meta_1753576153.debug --leaderboard_path output/meta-free-apps/train/train_meta_1753576153.csv --kfold 5 --pktcount 1000 --input_dim 512 --hidden_size 256 --num_layers 3 --dropout 0.3 --fusion_dim 256 --fc_hidden_size 128 --epoch 50 --lr 0.001 --path meta-free-apps/meta-ip_len/meta-ip_len.csv --norm token --model bigru
[2025-07-27 00:29:15,179] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753576153.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753576153.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'))
[2025-07-27 00:32:21,789] [INFO] Processed data from meta-free-apps/meta-ip_len/meta-ip_len.csv:
[2025-07-27 00:32:21,790] [INFO] (84492, 1000)
[2025-07-27 00:32:21,790] [INFO] [['656' '94' '52' ... '181' '52' '52']
 ['423' '87' '52' ... '60' '60' '52']
 ['423' '87' '52' ... '1432' '52' '1432']
 ...
 ['242' '87' '52' ... '424' '700' '52']
 ['60' '60' '52' ... '143' '52' '355']
 ['64' '52' '52' ... '91' '52' '91']]
[2025-07-27 00:32:22,106] [INFO] Training Fold 1/5
[2025-07-27 00:33:54,593] [INFO] Feature 0 normalized using token
[2025-07-27 00:33:54,593] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2025-07-27 00:33:54,658] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-27 00:33:54,659] [INFO] Training...
[2025-07-27 00:35:39,542] [INFO] Epoch 1/50, ValAcc: 65.42%, TrainLoss: 2.7581, ValLoss: 1.2870, LR: 0.001
[2025-07-27 00:37:22,293] [INFO] Epoch 2/50, ValAcc: 92.54%, TrainLoss: 0.6942, ValLoss: 0.3100, LR: 0.001
[2025-07-27 00:39:05,044] [INFO] Epoch 3/50, ValAcc: 93.91%, TrainLoss: 0.2963, ValLoss: 0.2588, LR: 0.001
[2025-07-27 00:40:47,845] [INFO] Epoch 4/50, ValAcc: 94.18%, TrainLoss: 0.2409, ValLoss: 0.2555, LR: 0.001
[2025-07-27 00:42:30,602] [INFO] Epoch 5/50, ValAcc: 93.99%, TrainLoss: 0.2365, ValLoss: 0.2497, LR: 0.001
[2025-07-27 00:44:13,351] [INFO] Epoch 6/50, ValAcc: 94.34%, TrainLoss: 0.2247, ValLoss: 0.2532, LR: 0.001
[2025-07-27 00:45:56,083] [INFO] Epoch 7/50, ValAcc: 93.93%, TrainLoss: 0.2178, ValLoss: 0.2558, LR: 0.001
[2025-07-27 00:47:38,771] [INFO] Epoch 8/50, ValAcc: 94.06%, TrainLoss: 0.2208, ValLoss: 0.2576, LR: 0.001
[2025-07-27 00:49:21,419] [INFO] Epoch 9/50, ValAcc: 94.69%, TrainLoss: 0.1897, ValLoss: 0.2305, LR: 0.0005
[2025-07-27 00:51:04,071] [INFO] Epoch 10/50, ValAcc: 94.58%, TrainLoss: 0.1768, ValLoss: 0.2337, LR: 0.0005
[2025-07-27 00:52:46,719] [INFO] Epoch 11/50, ValAcc: 94.66%, TrainLoss: 0.1758, ValLoss: 0.2400, LR: 0.0005
[2025-07-27 00:54:29,379] [INFO] Epoch 12/50, ValAcc: 94.63%, TrainLoss: 0.1751, ValLoss: 0.2426, LR: 0.0005
[2025-07-27 00:56:12,062] [INFO] Epoch 13/50, ValAcc: 94.80%, TrainLoss: 0.1642, ValLoss: 0.2495, LR: 0.00025
[2025-07-27 00:57:54,694] [INFO] Epoch 14/50, ValAcc: 94.62%, TrainLoss: 0.1600, ValLoss: 0.2390, LR: 0.00025
[2025-07-27 00:59:37,343] [INFO] Epoch 15/50, ValAcc: 94.64%, TrainLoss: 0.1565, ValLoss: 0.2471, LR: 0.00025
[2025-07-27 01:01:19,697] [INFO] Epoch 16/50, ValAcc: 94.79%, TrainLoss: 0.1525, ValLoss: 0.2521, LR: 0.000125
[2025-07-27 01:03:02,018] [INFO] Epoch 17/50, ValAcc: 94.66%, TrainLoss: 0.1503, ValLoss: 0.2628, LR: 0.000125
[2025-07-27 01:04:44,482] [INFO] Epoch 18/50, ValAcc: 94.67%, TrainLoss: 0.1482, ValLoss: 0.2774, LR: 0.000125
[2025-07-27 01:06:26,981] [INFO] Epoch 19/50, ValAcc: 94.66%, TrainLoss: 0.1437, ValLoss: 0.2781, LR: 6.25e-05
[2025-07-27 01:06:26,981] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-27 01:06:37,442] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753576153.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753576153.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9479,0.9495,0.9542,0.9478
[2025-07-27 01:06:37,443] [INFO] Training Fold 2/5
[2025-07-27 01:08:13,300] [INFO] Feature 0 normalized using token
[2025-07-27 01:08:13,300] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2025-07-27 01:08:13,342] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-27 01:08:13,342] [INFO] Training...
[2025-07-27 01:09:55,848] [INFO] Epoch 1/50, ValAcc: 75.35%, TrainLoss: 2.4660, ValLoss: 0.9317, LR: 0.001
[2025-07-27 01:11:38,334] [INFO] Epoch 2/50, ValAcc: 93.04%, TrainLoss: 0.4977, ValLoss: 0.2691, LR: 0.001
[2025-07-27 01:13:20,786] [INFO] Epoch 3/50, ValAcc: 93.69%, TrainLoss: 0.2538, ValLoss: 0.2459, LR: 0.001
[2025-07-27 01:15:03,296] [INFO] Epoch 4/50, ValAcc: 93.85%, TrainLoss: 0.2316, ValLoss: 0.2519, LR: 0.001
[2025-07-27 01:16:45,808] [INFO] Epoch 5/50, ValAcc: 94.00%, TrainLoss: 0.2252, ValLoss: 0.2427, LR: 0.001
[2025-07-27 01:18:28,309] [INFO] Epoch 6/50, ValAcc: 94.11%, TrainLoss: 0.2108, ValLoss: 0.2531, LR: 0.001
[2025-07-27 01:20:10,758] [INFO] Epoch 7/50, ValAcc: 94.17%, TrainLoss: 0.2138, ValLoss: 0.2404, LR: 0.001
[2025-07-27 01:21:53,225] [INFO] Epoch 8/50, ValAcc: 94.27%, TrainLoss: 0.2215, ValLoss: 0.2429, LR: 0.001
[2025-07-27 01:23:35,698] [INFO] Epoch 9/50, ValAcc: 94.40%, TrainLoss: 0.2075, ValLoss: 0.2490, LR: 0.001
[2025-07-27 01:25:18,219] [INFO] Epoch 10/50, ValAcc: 94.13%, TrainLoss: 0.2037, ValLoss: 0.2543, LR: 0.001
[2025-07-27 01:27:00,707] [INFO] Epoch 11/50, ValAcc: 94.47%, TrainLoss: 0.1781, ValLoss: 0.2367, LR: 0.0005
[2025-07-27 01:28:43,195] [INFO] Epoch 12/50, ValAcc: 94.50%, TrainLoss: 0.1689, ValLoss: 0.2499, LR: 0.0005
[2025-07-27 01:30:26,049] [INFO] Epoch 13/50, ValAcc: 94.43%, TrainLoss: 0.1676, ValLoss: 0.2597, LR: 0.0005
[2025-07-27 01:32:08,987] [INFO] Epoch 14/50, ValAcc: 94.49%, TrainLoss: 0.1652, ValLoss: 0.2574, LR: 0.0005
[2025-07-27 01:33:51,860] [INFO] Epoch 15/50, ValAcc: 94.57%, TrainLoss: 0.1569, ValLoss: 0.2487, LR: 0.00025
[2025-07-27 01:35:34,759] [INFO] Epoch 16/50, ValAcc: 94.54%, TrainLoss: 0.1517, ValLoss: 0.2502, LR: 0.00025
[2025-07-27 01:37:17,628] [INFO] Epoch 17/50, ValAcc: 94.57%, TrainLoss: 0.1491, ValLoss: 0.2640, LR: 0.00025
[2025-07-27 01:39:00,474] [INFO] Epoch 18/50, ValAcc: 94.67%, TrainLoss: 0.1444, ValLoss: 0.2574, LR: 0.000125
[2025-07-27 01:40:43,392] [INFO] Epoch 19/50, ValAcc: 94.58%, TrainLoss: 0.1406, ValLoss: 0.2597, LR: 0.000125
[2025-07-27 01:42:26,281] [INFO] Epoch 20/50, ValAcc: 94.54%, TrainLoss: 0.1382, ValLoss: 0.2643, LR: 0.000125
[2025-07-27 01:44:09,183] [INFO] Epoch 21/50, ValAcc: 94.63%, TrainLoss: 0.1354, ValLoss: 0.2745, LR: 6.25e-05
[2025-07-27 01:44:09,183] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-27 01:44:19,618] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753576153.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753576153.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9498,0.9514,0.9565,0.9502
[2025-07-27 01:44:19,619] [INFO] Training Fold 3/5
[2025-07-27 01:45:54,564] [INFO] Feature 0 normalized using token
[2025-07-27 01:45:54,565] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2025-07-27 01:45:54,605] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-27 01:45:54,605] [INFO] Training...
[2025-07-27 01:47:37,501] [INFO] Epoch 1/50, ValAcc: 18.00%, TrainLoss: 4.0692, ValLoss: 2.7808, LR: 0.001
[2025-07-27 01:49:20,362] [INFO] Epoch 2/50, ValAcc: 87.46%, TrainLoss: 1.6524, ValLoss: 0.4724, LR: 0.001
[2025-07-27 01:51:03,210] [INFO] Epoch 3/50, ValAcc: 92.49%, TrainLoss: 0.5686, ValLoss: 0.3357, LR: 0.001
[2025-07-27 01:52:46,088] [INFO] Epoch 4/50, ValAcc: 93.31%, TrainLoss: 0.3789, ValLoss: 0.2910, LR: 0.001
[2025-07-27 01:54:28,998] [INFO] Epoch 5/50, ValAcc: 93.94%, TrainLoss: 0.3349, ValLoss: 0.2786, LR: 0.001
[2025-07-27 01:56:11,883] [INFO] Epoch 6/50, ValAcc: 93.57%, TrainLoss: 0.3103, ValLoss: 0.2884, LR: 0.001
[2025-07-27 01:57:54,803] [INFO] Epoch 7/50, ValAcc: 93.91%, TrainLoss: 0.2825, ValLoss: 0.2775, LR: 0.001
[2025-07-27 01:59:37,703] [INFO] Epoch 8/50, ValAcc: 93.62%, TrainLoss: 0.2697, ValLoss: 0.2711, LR: 0.001
[2025-07-27 02:01:20,571] [INFO] Epoch 9/50, ValAcc: 94.41%, TrainLoss: 0.2627, ValLoss: 0.2584, LR: 0.001
[2025-07-27 02:03:03,477] [INFO] Epoch 10/50, ValAcc: 93.93%, TrainLoss: 0.2629, ValLoss: 0.3028, LR: 0.001
[2025-07-27 02:04:46,376] [INFO] Epoch 11/50, ValAcc: 94.01%, TrainLoss: 0.2506, ValLoss: 0.2650, LR: 0.001
[2025-07-27 02:06:29,299] [INFO] Epoch 12/50, ValAcc: 93.92%, TrainLoss: 0.2504, ValLoss: 0.2617, LR: 0.001
[2025-07-27 02:08:12,144] [INFO] Epoch 13/50, ValAcc: 94.20%, TrainLoss: 0.2122, ValLoss: 0.2647, LR: 0.0005
[2025-07-27 02:09:55,013] [INFO] Epoch 14/50, ValAcc: 94.20%, TrainLoss: 0.2003, ValLoss: 0.2694, LR: 0.0005
[2025-07-27 02:11:37,926] [INFO] Epoch 15/50, ValAcc: 94.30%, TrainLoss: 0.1954, ValLoss: 0.2797, LR: 0.0005
[2025-07-27 02:13:20,654] [INFO] Epoch 16/50, ValAcc: 94.71%, TrainLoss: 0.1863, ValLoss: 0.2675, LR: 0.00025
[2025-07-27 02:15:03,398] [INFO] Epoch 17/50, ValAcc: 94.47%, TrainLoss: 0.1810, ValLoss: 0.2691, LR: 0.00025
[2025-07-27 02:16:46,057] [INFO] Epoch 18/50, ValAcc: 94.50%, TrainLoss: 0.1769, ValLoss: 0.2635, LR: 0.00025
[2025-07-27 02:18:28,750] [INFO] Epoch 19/50, ValAcc: 94.57%, TrainLoss: 0.1709, ValLoss: 0.2780, LR: 0.000125
[2025-07-27 02:20:10,884] [INFO] Epoch 20/50, ValAcc: 94.65%, TrainLoss: 0.1683, ValLoss: 0.2877, LR: 0.000125
[2025-07-27 02:21:53,292] [INFO] Epoch 21/50, ValAcc: 94.59%, TrainLoss: 0.1648, ValLoss: 0.2811, LR: 0.000125
[2025-07-27 02:23:35,720] [INFO] Epoch 22/50, ValAcc: 95.05%, TrainLoss: 0.1624, ValLoss: 0.2857, LR: 6.25e-05
[2025-07-27 02:23:35,720] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-27 02:23:46,145] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753576153.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753576153.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9428,0.9461,0.9596,0.9425
[2025-07-27 02:23:46,146] [INFO] Training Fold 4/5
[2025-07-27 02:25:18,409] [INFO] Feature 0 normalized using token
[2025-07-27 02:25:18,410] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2025-07-27 02:25:18,450] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-27 02:25:18,450] [INFO] Training...
[2025-07-27 02:27:00,895] [INFO] Epoch 1/50, ValAcc: 1.11%, TrainLoss: 4.5132, ValLoss: 4.5118, LR: 0.001
[2025-07-27 02:28:43,276] [INFO] Epoch 2/50, ValAcc: 1.61%, TrainLoss: 4.5110, ValLoss: 4.4841, LR: 0.001
[2025-07-27 02:30:24,997] [INFO] Epoch 3/50, ValAcc: 3.53%, TrainLoss: 4.3653, ValLoss: 4.3148, LR: 0.001
[2025-07-27 02:32:06,709] [INFO] Epoch 4/50, ValAcc: 7.21%, TrainLoss: 4.2826, ValLoss: 4.1200, LR: 0.001
[2025-07-27 02:33:48,423] [INFO] Epoch 5/50, ValAcc: 8.43%, TrainLoss: 4.1140, ValLoss: 4.0404, LR: 0.001
[2025-07-27 02:35:30,184] [INFO] Epoch 6/50, ValAcc: 9.70%, TrainLoss: 4.0159, ValLoss: 3.9354, LR: 0.001
[2025-07-27 02:37:11,913] [INFO] Epoch 7/50, ValAcc: 20.62%, TrainLoss: 3.7426, ValLoss: 3.2473, LR: 0.001
[2025-07-27 02:38:53,631] [INFO] Epoch 8/50, ValAcc: 29.82%, TrainLoss: 3.1568, ValLoss: 2.8423, LR: 0.001
[2025-07-27 02:40:35,397] [INFO] Epoch 9/50, ValAcc: 43.59%, TrainLoss: 2.7210, ValLoss: 2.2994, LR: 0.001
[2025-07-27 02:42:17,143] [INFO] Epoch 10/50, ValAcc: 54.28%, TrainLoss: 2.1476, ValLoss: 1.7648, LR: 0.001
[2025-07-27 02:43:58,908] [INFO] Epoch 11/50, ValAcc: 58.56%, TrainLoss: 1.7766, ValLoss: 1.5897, LR: 0.001
[2025-07-27 02:45:40,671] [INFO] Epoch 12/50, ValAcc: 60.82%, TrainLoss: 1.6072, ValLoss: 1.4591, LR: 0.001
[2025-07-27 02:47:22,411] [INFO] Epoch 13/50, ValAcc: 66.04%, TrainLoss: 1.4746, ValLoss: 1.3015, LR: 0.001
[2025-07-27 02:49:04,183] [INFO] Epoch 14/50, ValAcc: 69.76%, TrainLoss: 1.3128, ValLoss: 1.1542, LR: 0.001
[2025-07-27 02:50:45,954] [INFO] Epoch 15/50, ValAcc: 73.02%, TrainLoss: 1.1721, ValLoss: 1.0227, LR: 0.001
[2025-07-27 02:52:27,887] [INFO] Epoch 16/50, ValAcc: 73.55%, TrainLoss: 1.0716, ValLoss: 0.9755, LR: 0.001
[2025-07-27 02:54:09,745] [INFO] Epoch 17/50, ValAcc: 77.41%, TrainLoss: 0.9985, ValLoss: 0.8709, LR: 0.001
[2025-07-27 02:55:51,693] [INFO] Epoch 18/50, ValAcc: 79.98%, TrainLoss: 0.9106, ValLoss: 0.7878, LR: 0.001
[2025-07-27 02:57:34,108] [INFO] Epoch 19/50, ValAcc: 80.52%, TrainLoss: 0.8279, ValLoss: 0.7399, LR: 0.001
[2025-07-27 02:59:16,530] [INFO] Epoch 20/50, ValAcc: 82.02%, TrainLoss: 0.7820, ValLoss: 0.7001, LR: 0.001
[2025-07-27 03:00:58,993] [INFO] Epoch 21/50, ValAcc: 84.02%, TrainLoss: 0.7229, ValLoss: 0.6376, LR: 0.001
[2025-07-27 03:02:41,463] [INFO] Epoch 22/50, ValAcc: 85.63%, TrainLoss: 0.6529, ValLoss: 0.5605, LR: 0.001
[2025-07-27 03:04:23,940] [INFO] Epoch 23/50, ValAcc: 87.96%, TrainLoss: 0.5863, ValLoss: 0.4985, LR: 0.001
[2025-07-27 03:06:06,391] [INFO] Epoch 24/50, ValAcc: 89.02%, TrainLoss: 0.5314, ValLoss: 0.4517, LR: 0.001
[2025-07-27 03:07:48,848] [INFO] Epoch 25/50, ValAcc: 89.37%, TrainLoss: 0.4846, ValLoss: 0.4241, LR: 0.001
[2025-07-27 03:09:31,295] [INFO] Epoch 26/50, ValAcc: 90.28%, TrainLoss: 0.4536, ValLoss: 0.4053, LR: 0.001
[2025-07-27 03:11:13,743] [INFO] Epoch 27/50, ValAcc: 91.27%, TrainLoss: 0.4193, ValLoss: 0.3518, LR: 0.001
[2025-07-27 03:12:56,185] [INFO] Epoch 28/50, ValAcc: 92.27%, TrainLoss: 0.3961, ValLoss: 0.3323, LR: 0.001
[2025-07-27 03:14:38,632] [INFO] Epoch 29/50, ValAcc: 91.80%, TrainLoss: 0.3664, ValLoss: 0.3353, LR: 0.001
[2025-07-27 03:16:21,086] [INFO] Epoch 30/50, ValAcc: 92.65%, TrainLoss: 0.3438, ValLoss: 0.3167, LR: 0.001
[2025-07-27 03:18:03,560] [INFO] Epoch 31/50, ValAcc: 92.66%, TrainLoss: 0.3320, ValLoss: 0.3213, LR: 0.001
[2025-07-27 03:19:46,015] [INFO] Epoch 32/50, ValAcc: 93.02%, TrainLoss: 0.3341, ValLoss: 0.3231, LR: 0.001
[2025-07-27 03:21:28,517] [INFO] Epoch 33/50, ValAcc: 93.11%, TrainLoss: 0.3183, ValLoss: 0.2964, LR: 0.001
[2025-07-27 03:23:10,982] [INFO] Epoch 34/50, ValAcc: 92.86%, TrainLoss: 0.3053, ValLoss: 0.3089, LR: 0.001
[2025-07-27 03:24:53,471] [INFO] Epoch 35/50, ValAcc: 93.31%, TrainLoss: 0.3024, ValLoss: 0.3021, LR: 0.001
[2025-07-27 03:26:35,922] [INFO] Epoch 36/50, ValAcc: 92.93%, TrainLoss: 0.2912, ValLoss: 0.2976, LR: 0.001
[2025-07-27 03:28:18,779] [INFO] Epoch 37/50, ValAcc: 93.61%, TrainLoss: 0.2535, ValLoss: 0.3040, LR: 0.0005
[2025-07-27 03:30:01,632] [INFO] Epoch 38/50, ValAcc: 93.74%, TrainLoss: 0.2367, ValLoss: 0.3277, LR: 0.0005
[2025-07-27 03:31:44,500] [INFO] Epoch 39/50, ValAcc: 93.60%, TrainLoss: 0.2327, ValLoss: 0.2861, LR: 0.0005
[2025-07-27 03:33:27,394] [INFO] Epoch 40/50, ValAcc: 93.79%, TrainLoss: 0.2286, ValLoss: 0.2836, LR: 0.0005
[2025-07-27 03:35:10,253] [INFO] Epoch 41/50, ValAcc: 94.04%, TrainLoss: 0.2249, ValLoss: 0.2874, LR: 0.0005
[2025-07-27 03:36:53,143] [INFO] Epoch 42/50, ValAcc: 93.75%, TrainLoss: 0.2254, ValLoss: 0.2750, LR: 0.0005
[2025-07-27 03:38:35,875] [INFO] Epoch 43/50, ValAcc: 94.09%, TrainLoss: 0.2280, ValLoss: 0.2837, LR: 0.0005
[2025-07-27 03:40:18,034] [INFO] Epoch 44/50, ValAcc: 93.91%, TrainLoss: 0.2217, ValLoss: 0.2852, LR: 0.0005
[2025-07-27 03:42:00,240] [INFO] Epoch 45/50, ValAcc: 93.92%, TrainLoss: 0.2189, ValLoss: 0.2697, LR: 0.0005
[2025-07-27 03:43:42,409] [INFO] Epoch 46/50, ValAcc: 93.95%, TrainLoss: 0.2189, ValLoss: 0.2935, LR: 0.0005
[2025-07-27 03:45:24,576] [INFO] Epoch 47/50, ValAcc: 93.73%, TrainLoss: 0.2168, ValLoss: 0.2989, LR: 0.0005
[2025-07-27 03:47:06,750] [INFO] Epoch 48/50, ValAcc: 93.98%, TrainLoss: 0.2172, ValLoss: 0.2893, LR: 0.0005
[2025-07-27 03:48:48,903] [INFO] Epoch 49/50, ValAcc: 94.15%, TrainLoss: 0.2065, ValLoss: 0.2903, LR: 0.00025
[2025-07-27 03:50:31,045] [INFO] Epoch 50/50, ValAcc: 94.14%, TrainLoss: 0.2005, ValLoss: 0.2892, LR: 0.00025
[2025-07-27 03:50:41,348] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753576153.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753576153.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9414,0.9486,0.9667,0.9417
[2025-07-27 03:50:41,349] [INFO] Training Fold 5/5
[2025-07-27 03:52:10,165] [INFO] Feature 0 normalized using token
[2025-07-27 03:52:10,165] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2025-07-27 03:52:10,216] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-27 03:52:10,216] [INFO] Training...
[2025-07-27 03:53:52,556] [INFO] Epoch 1/50, ValAcc: 35.87%, TrainLoss: 4.0148, ValLoss: 2.2026, LR: 0.001
[2025-07-27 03:55:34,928] [INFO] Epoch 2/50, ValAcc: 78.18%, TrainLoss: 1.5293, ValLoss: 0.8564, LR: 0.001
[2025-07-27 03:57:17,252] [INFO] Epoch 3/50, ValAcc: 88.04%, TrainLoss: 0.7287, ValLoss: 0.4533, LR: 0.001
[2025-07-27 03:58:59,588] [INFO] Epoch 4/50, ValAcc: 92.31%, TrainLoss: 0.4007, ValLoss: 0.3216, LR: 0.001
[2025-07-27 04:00:41,887] [INFO] Epoch 5/50, ValAcc: 92.98%, TrainLoss: 0.2891, ValLoss: 0.2999, LR: 0.001
[2025-07-27 04:02:24,192] [INFO] Epoch 6/50, ValAcc: 93.49%, TrainLoss: 0.2658, ValLoss: 0.2800, LR: 0.001
[2025-07-27 04:04:06,504] [INFO] Epoch 7/50, ValAcc: 93.54%, TrainLoss: 0.2492, ValLoss: 0.2839, LR: 0.001
[2025-07-27 04:05:48,814] [INFO] Epoch 8/50, ValAcc: 93.67%, TrainLoss: 0.2400, ValLoss: 0.2792, LR: 0.001
[2025-07-27 04:07:31,637] [INFO] Epoch 9/50, ValAcc: 93.50%, TrainLoss: 0.2330, ValLoss: 0.2814, LR: 0.001
[2025-07-27 04:09:14,684] [INFO] Epoch 10/50, ValAcc: 94.17%, TrainLoss: 0.2262, ValLoss: 0.2773, LR: 0.001
[2025-07-27 04:10:57,684] [INFO] Epoch 11/50, ValAcc: 93.92%, TrainLoss: 0.2297, ValLoss: 0.3007, LR: 0.001
[2025-07-27 04:12:40,689] [INFO] Epoch 12/50, ValAcc: 93.56%, TrainLoss: 0.2245, ValLoss: 0.2920, LR: 0.001
[2025-07-27 04:14:23,706] [INFO] Epoch 13/50, ValAcc: 93.50%, TrainLoss: 0.2334, ValLoss: 0.2926, LR: 0.001
[2025-07-27 04:16:06,646] [INFO] Epoch 14/50, ValAcc: 93.99%, TrainLoss: 0.1932, ValLoss: 0.2778, LR: 0.0005
[2025-07-27 04:17:49,582] [INFO] Epoch 15/50, ValAcc: 94.46%, TrainLoss: 0.1826, ValLoss: 0.2687, LR: 0.0005
[2025-07-27 04:19:32,571] [INFO] Epoch 16/50, ValAcc: 94.50%, TrainLoss: 0.1811, ValLoss: 0.2604, LR: 0.0005
[2025-07-27 04:21:15,570] [INFO] Epoch 17/50, ValAcc: 93.98%, TrainLoss: 0.1785, ValLoss: 0.2755, LR: 0.0005
[2025-07-27 04:22:58,579] [INFO] Epoch 18/50, ValAcc: 94.33%, TrainLoss: 0.1772, ValLoss: 0.2873, LR: 0.0005
[2025-07-27 04:24:41,565] [INFO] Epoch 19/50, ValAcc: 94.15%, TrainLoss: 0.1761, ValLoss: 0.2948, LR: 0.0005
[2025-07-27 04:26:24,525] [INFO] Epoch 20/50, ValAcc: 94.13%, TrainLoss: 0.1659, ValLoss: 0.3130, LR: 0.00025
[2025-07-27 04:28:07,501] [INFO] Epoch 21/50, ValAcc: 94.25%, TrainLoss: 0.1601, ValLoss: 0.2964, LR: 0.00025
[2025-07-27 04:29:50,432] [INFO] Epoch 22/50, ValAcc: 94.57%, TrainLoss: 0.1599, ValLoss: 0.2949, LR: 0.00025
[2025-07-27 04:31:33,415] [INFO] Epoch 23/50, ValAcc: 94.38%, TrainLoss: 0.1542, ValLoss: 0.3121, LR: 0.000125
[2025-07-27 04:33:16,415] [INFO] Epoch 24/50, ValAcc: 94.38%, TrainLoss: 0.1518, ValLoss: 0.3248, LR: 0.000125
[2025-07-27 04:34:59,402] [INFO] Epoch 25/50, ValAcc: 94.44%, TrainLoss: 0.1515, ValLoss: 0.3197, LR: 0.000125
[2025-07-27 04:36:42,337] [INFO] Epoch 26/50, ValAcc: 94.71%, TrainLoss: 0.1481, ValLoss: 0.3221, LR: 6.25e-05
[2025-07-27 04:36:42,337] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-27 04:36:52,772] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=1000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753576153.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753576153.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9490,0.9489,0.9559,0.9483
[2025-07-27 04:36:53,702] [INFO] [(0.9478667376767856, 0.9495283498005642, 0.9542047144736605, 0.9477888354905021), (0.949760340848571, 0.9514444232231934, 0.9564949459269945, 0.950154332545906), (0.9427742928157178, 0.9460983308589463, 0.9595644522939439, 0.9425054749729657), (0.9413540063912889, 0.9485937765838521, 0.9666825829818594, 0.9417401155267606), (0.9489880459225943, 0.9489256277673894, 0.9559293309742736, 0.9482846371871873)]
Running python vrscanner.py --train --debug_path output/meta-free-apps/train/train_meta_1753591015.debug --leaderboard_path output/meta-free-apps/train/train_meta_1753591015.csv --kfold 5 --pktcount 2000 --input_dim 512 --hidden_size 256 --num_layers 3 --dropout 0.3 --fusion_dim 256 --fc_hidden_size 128 --epoch 50 --lr 0.001 --path meta-free-apps/meta-ip_len/meta-ip_len.csv --norm token --model bigru
[2025-07-27 04:36:57,099] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=2000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753591015.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753591015.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'))
[2025-07-27 04:40:09,797] [INFO] Processed data from meta-free-apps/meta-ip_len/meta-ip_len.csv:
[2025-07-27 04:40:09,798] [INFO] (84492, 2000)
[2025-07-27 04:40:09,798] [INFO] [['656' '94' '52' ... <NA> <NA> <NA>]
 ['423' '87' '52' ... <NA> <NA> <NA>]
 ['423' '87' '52' ... <NA> <NA> <NA>]
 ...
 ['242' '87' '52' ... '1432' '64' '64']
 ['60' '60' '52' ... '83' '52' '355']
 ['64' '52' '52' ... '1432' '1432' '1432']]
[2025-07-27 04:40:10,532] [INFO] Training Fold 1/5
[2025-07-27 04:43:43,392] [INFO] Feature 0 normalized using token
[2025-07-27 04:43:43,393] [INFO] Train shape: (59143, 2000), Val shape: (8450, 2000), Test shape: (16899, 2000)
[2025-07-27 04:43:43,510] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-27 04:43:43,511] [INFO] Training...
[2025-07-27 04:47:09,778] [INFO] Epoch 1/50, ValAcc: 65.42%, TrainLoss: 2.5721, ValLoss: 1.1495, LR: 0.001
[2025-07-27 04:50:33,056] [INFO] Epoch 2/50, ValAcc: 80.83%, TrainLoss: 1.0937, ValLoss: 0.7328, LR: 0.001
[2025-07-27 04:53:56,294] [INFO] Epoch 3/50, ValAcc: 89.66%, TrainLoss: 0.5973, ValLoss: 0.4111, LR: 0.001
[2025-07-27 04:57:19,556] [INFO] Epoch 4/50, ValAcc: 93.28%, TrainLoss: 0.3688, ValLoss: 0.2839, LR: 0.001
[2025-07-27 05:00:42,808] [INFO] Epoch 5/50, ValAcc: 93.82%, TrainLoss: 0.2651, ValLoss: 0.2688, LR: 0.001
[2025-07-27 05:04:05,244] [INFO] Epoch 6/50, ValAcc: 94.56%, TrainLoss: 0.2221, ValLoss: 0.2296, LR: 0.001
[2025-07-27 05:07:27,655] [INFO] Epoch 7/50, ValAcc: 94.75%, TrainLoss: 0.2136, ValLoss: 0.2298, LR: 0.001
[2025-07-27 05:10:50,024] [INFO] Epoch 8/50, ValAcc: 94.44%, TrainLoss: 0.2028, ValLoss: 0.2453, LR: 0.001
[2025-07-27 05:14:12,316] [INFO] Epoch 9/50, ValAcc: 94.65%, TrainLoss: 0.1951, ValLoss: 0.2182, LR: 0.001
[2025-07-27 05:17:34,738] [INFO] Epoch 10/50, ValAcc: 95.11%, TrainLoss: 0.1865, ValLoss: 0.2453, LR: 0.001
[2025-07-27 05:20:57,147] [INFO] Epoch 11/50, ValAcc: 94.88%, TrainLoss: 0.1955, ValLoss: 0.2253, LR: 0.001
[2025-07-27 05:24:19,467] [INFO] Epoch 12/50, ValAcc: 95.02%, TrainLoss: 0.1901, ValLoss: 0.2398, LR: 0.001
[2025-07-27 05:27:41,808] [INFO] Epoch 13/50, ValAcc: 95.38%, TrainLoss: 0.1566, ValLoss: 0.2346, LR: 0.0005
[2025-07-27 05:31:04,150] [INFO] Epoch 14/50, ValAcc: 95.31%, TrainLoss: 0.1494, ValLoss: 0.2256, LR: 0.0005
[2025-07-27 05:34:26,747] [INFO] Epoch 15/50, ValAcc: 95.37%, TrainLoss: 0.1478, ValLoss: 0.2372, LR: 0.0005
[2025-07-27 05:37:49,613] [INFO] Epoch 16/50, ValAcc: 95.35%, TrainLoss: 0.1401, ValLoss: 0.2331, LR: 0.00025
[2025-07-27 05:41:12,512] [INFO] Epoch 17/50, ValAcc: 95.15%, TrainLoss: 0.1355, ValLoss: 0.2406, LR: 0.00025
[2025-07-27 05:44:35,382] [INFO] Epoch 18/50, ValAcc: 95.33%, TrainLoss: 0.1326, ValLoss: 0.2501, LR: 0.00025
[2025-07-27 05:47:58,255] [INFO] Epoch 19/50, ValAcc: 95.34%, TrainLoss: 0.1304, ValLoss: 0.2445, LR: 0.000125
[2025-07-27 05:51:21,048] [INFO] Epoch 20/50, ValAcc: 95.23%, TrainLoss: 0.1286, ValLoss: 0.2529, LR: 0.000125
[2025-07-27 05:54:43,945] [INFO] Epoch 21/50, ValAcc: 95.21%, TrainLoss: 0.1261, ValLoss: 0.2565, LR: 0.000125
[2025-07-27 05:58:06,768] [INFO] Epoch 22/50, ValAcc: 95.34%, TrainLoss: 0.1247, ValLoss: 0.2510, LR: 6.25e-05
[2025-07-27 05:58:06,769] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-27 05:58:27,434] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=2000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753591015.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753591015.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_2000_64_50_0.001_512_256_3_0.3_256_128'),0.9538,0.9546,0.9606,0.9542
[2025-07-27 05:58:27,435] [INFO] Training Fold 2/5
[2025-07-27 06:01:38,370] [INFO] Feature 0 normalized using token
[2025-07-27 06:01:38,371] [INFO] Train shape: (59143, 2000), Val shape: (8450, 2000), Test shape: (16899, 2000)
[2025-07-27 06:01:38,478] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-27 06:01:38,478] [INFO] Training...
[2025-07-27 06:05:01,068] [INFO] Epoch 1/50, ValAcc: 1.08%, TrainLoss: 4.5127, ValLoss: 4.5112, LR: 0.001
[2025-07-27 06:08:23,512] [INFO] Epoch 2/50, ValAcc: 1.08%, TrainLoss: 4.5112, ValLoss: 4.5113, LR: 0.001
[2025-07-27 06:11:45,931] [INFO] Epoch 3/50, ValAcc: 1.08%, TrainLoss: 4.5111, ValLoss: 4.5113, LR: 0.001
[2025-07-27 06:15:08,368] [INFO] Epoch 4/50, ValAcc: 1.08%, TrainLoss: 4.5111, ValLoss: 4.5113, LR: 0.001
[2025-07-27 06:18:30,840] [INFO] Epoch 5/50, ValAcc: 1.08%, TrainLoss: 4.5108, ValLoss: 4.5113, LR: 0.0005
[2025-07-27 06:21:53,340] [INFO] Epoch 6/50, ValAcc: 1.08%, TrainLoss: 4.5108, ValLoss: 4.5114, LR: 0.0005
[2025-07-27 06:25:15,804] [INFO] Epoch 7/50, ValAcc: 1.08%, TrainLoss: 4.5108, ValLoss: 4.5113, LR: 0.0005
[2025-07-27 06:28:38,263] [INFO] Epoch 8/50, ValAcc: 1.08%, TrainLoss: 4.5107, ValLoss: 4.5114, LR: 0.00025
[2025-07-27 06:32:00,744] [INFO] Epoch 9/50, ValAcc: 1.08%, TrainLoss: 4.5107, ValLoss: 4.5114, LR: 0.00025
[2025-07-27 06:35:23,116] [INFO] Epoch 10/50, ValAcc: 1.08%, TrainLoss: 4.5107, ValLoss: 4.5114, LR: 0.00025
[2025-07-27 06:38:45,593] [INFO] Epoch 11/50, ValAcc: 1.08%, TrainLoss: 4.5106, ValLoss: 4.5114, LR: 0.000125
[2025-07-27 06:42:08,045] [INFO] Epoch 12/50, ValAcc: 1.08%, TrainLoss: 4.5106, ValLoss: 4.5114, LR: 0.000125
[2025-07-27 06:45:30,503] [INFO] Epoch 13/50, ValAcc: 1.08%, TrainLoss: 4.5106, ValLoss: 4.5114, LR: 0.000125
[2025-07-27 06:48:52,902] [INFO] Epoch 14/50, ValAcc: 1.08%, TrainLoss: 4.5106, ValLoss: 4.5114, LR: 6.25e-05
[2025-07-27 06:48:52,902] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-27 06:49:13,571] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=2000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753591015.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753591015.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_2000_64_50_0.001_512_256_3_0.3_256_128'),0.0111,0.0002,0.0001,0.0110
[2025-07-27 06:49:13,573] [INFO] Training Fold 3/5
[2025-07-27 06:52:36,113] [INFO] Feature 0 normalized using token
[2025-07-27 06:52:36,114] [INFO] Train shape: (59144, 2000), Val shape: (8450, 2000), Test shape: (16898, 2000)
[2025-07-27 06:52:36,174] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-27 06:52:36,174] [INFO] Training...
[2025-07-27 06:55:58,528] [INFO] Epoch 1/50, ValAcc: 77.01%, TrainLoss: 2.4854, ValLoss: 0.8021, LR: 0.001
[2025-07-27 06:59:21,701] [INFO] Epoch 2/50, ValAcc: 93.83%, TrainLoss: 0.4977, ValLoss: 0.2194, LR: 0.001
[2025-07-27 07:02:44,566] [INFO] Epoch 3/50, ValAcc: 94.46%, TrainLoss: 0.2425, ValLoss: 0.1924, LR: 0.001
[2025-07-27 07:06:07,526] [INFO] Epoch 4/50, ValAcc: 95.03%, TrainLoss: 0.1958, ValLoss: 0.1759, LR: 0.001
[2025-07-27 07:09:30,456] [INFO] Epoch 5/50, ValAcc: 95.30%, TrainLoss: 0.1918, ValLoss: 0.1785, LR: 0.001
[2025-07-27 07:12:53,380] [INFO] Epoch 6/50, ValAcc: 95.17%, TrainLoss: 0.1775, ValLoss: 0.1656, LR: 0.001
[2025-07-27 07:16:16,254] [INFO] Epoch 7/50, ValAcc: 94.84%, TrainLoss: 0.1736, ValLoss: 0.1847, LR: 0.001
[2025-07-27 07:19:39,170] [INFO] Epoch 8/50, ValAcc: 95.61%, TrainLoss: 0.1763, ValLoss: 0.1703, LR: 0.001
[2025-07-27 07:23:02,243] [INFO] Epoch 9/50, ValAcc: 95.09%, TrainLoss: 0.1677, ValLoss: 0.1831, LR: 0.001
[2025-07-27 07:26:25,399] [INFO] Epoch 10/50, ValAcc: 95.53%, TrainLoss: 0.1398, ValLoss: 0.1730, LR: 0.0005
[2025-07-27 07:29:48,506] [INFO] Epoch 11/50, ValAcc: 95.47%, TrainLoss: 0.1339, ValLoss: 0.1763, LR: 0.0005
[2025-07-27 07:33:11,643] [INFO] Epoch 12/50, ValAcc: 95.83%, TrainLoss: 0.1303, ValLoss: 0.1709, LR: 0.0005
[2025-07-27 07:36:34,644] [INFO] Epoch 13/50, ValAcc: 95.69%, TrainLoss: 0.1213, ValLoss: 0.1642, LR: 0.00025
[2025-07-27 07:39:57,791] [INFO] Epoch 14/50, ValAcc: 95.55%, TrainLoss: 0.1168, ValLoss: 0.1682, LR: 0.00025
[2025-07-27 07:43:19,476] [INFO] Epoch 15/50, ValAcc: 95.56%, TrainLoss: 0.1151, ValLoss: 0.1708, LR: 0.00025
[2025-07-27 07:46:42,074] [INFO] Epoch 16/50, ValAcc: 95.50%, TrainLoss: 0.1125, ValLoss: 0.1734, LR: 0.00025
[2025-07-27 07:50:04,241] [INFO] Epoch 17/50, ValAcc: 95.60%, TrainLoss: 0.1078, ValLoss: 0.1752, LR: 0.000125
[2025-07-27 07:53:26,897] [INFO] Epoch 18/50, ValAcc: 95.83%, TrainLoss: 0.1042, ValLoss: 0.1852, LR: 0.000125
[2025-07-27 07:56:49,500] [INFO] Epoch 19/50, ValAcc: 95.62%, TrainLoss: 0.1024, ValLoss: 0.1808, LR: 0.000125
[2025-07-27 08:00:12,150] [INFO] Epoch 20/50, ValAcc: 95.63%, TrainLoss: 0.0979, ValLoss: 0.1959, LR: 6.25e-05
[2025-07-27 08:00:12,150] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-27 08:00:32,645] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], pktcount=2000, importance=False, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, debug_path='output/meta-free-apps/train/train_meta_1753591015.debug', leaderboard_path='output/meta-free-apps/train/train_meta_1753591015.csv', step1=False, step2=False, step3=False, train=True, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_2000_64_50_0.001_512_256_3_0.3_256_128'),0.9553,0.9546,0.9580,0.9555
[2025-07-27 08:00:32,646] [INFO] Training Fold 4/5
[2025-07-27 08:03:51,982] [INFO] Feature 0 normalized using token
[2025-07-27 08:03:51,983] [INFO] Train shape: (59144, 2000), Val shape: (8450, 2000), Test shape: (16898, 2000)
[2025-07-27 08:03:52,066] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2025-07-27 08:03:52,066] [INFO] Training...
[2025-07-27 08:07:15,118] [INFO] Epoch 1/50, ValAcc: 66.41%, TrainLoss: 3.4390, ValLoss: 1.2048, LR: 0.001
[2025-07-27 08:10:37,879] [INFO] Epoch 2/50, ValAcc: 90.06%, TrainLoss: 0.8039, ValLoss: 0.3739, LR: 0.001
[2025-07-27 08:13:58,184] [INFO] Epoch 3/50, ValAcc: 92.67%, TrainLoss: 0.3847, ValLoss: 0.2912, LR: 0.001
[2025-07-27 08:17:17,612] [INFO] Epoch 4/50, ValAcc: 93.87%, TrainLoss: 0.2721, ValLoss: 0.2434, LR: 0.001
