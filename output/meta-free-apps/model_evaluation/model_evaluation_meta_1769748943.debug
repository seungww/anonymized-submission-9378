[2026-01-30 04:55:45,296] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['rnn'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'))
[2026-01-30 04:58:49,864] [INFO] Processed data from meta-free-apps/meta-ip_len/meta-ip_len.csv:
[2026-01-30 04:58:49,864] [INFO] (84492, 1000)
[2026-01-30 04:58:49,864] [INFO] [['656' '94' '52' ... '181' '52' '52']
 ['423' '87' '52' ... '60' '60' '52']
 ['423' '87' '52' ... '1432' '52' '1432']
 ...
 ['242' '87' '52' ... '424' '700' '52']
 ['60' '60' '52' ... '143' '52' '355']
 ['64' '52' '52' ... '91' '52' '91']]
[2026-01-30 04:58:50,206] [INFO] Training Fold 1/5
[2026-01-30 05:00:25,832] [INFO] Feature 0 normalized using token
[2026-01-30 05:00:25,832] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2026-01-30 05:00:25,892] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): RNN(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (transformer_input_proj): ModuleList(
    (0): Identity()
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-30 05:00:25,892] [INFO] Training...
[2026-01-30 05:01:08,799] [INFO] Epoch 1/50, ValAcc: 9.49%, TrainLoss: 4.1104, ValLoss: 3.4754, LR: 0.001
[2026-01-30 05:01:50,101] [INFO] Epoch 2/50, ValAcc: 35.63%, TrainLoss: 2.9875, ValLoss: 2.3715, LR: 0.001
[2026-01-30 05:02:31,883] [INFO] Epoch 3/50, ValAcc: 51.89%, TrainLoss: 2.2781, ValLoss: 1.8397, LR: 0.001
[2026-01-30 05:03:13,653] [INFO] Epoch 4/50, ValAcc: 57.60%, TrainLoss: 1.8649, ValLoss: 1.6031, LR: 0.001
[2026-01-30 05:03:54,342] [INFO] Epoch 5/50, ValAcc: 63.67%, TrainLoss: 1.5875, ValLoss: 1.3568, LR: 0.001
[2026-01-30 05:04:36,206] [INFO] Epoch 6/50, ValAcc: 67.88%, TrainLoss: 1.4011, ValLoss: 1.1978, LR: 0.001
[2026-01-30 05:05:17,508] [INFO] Epoch 7/50, ValAcc: 71.22%, TrainLoss: 1.2685, ValLoss: 1.1052, LR: 0.001
[2026-01-30 05:05:59,367] [INFO] Epoch 8/50, ValAcc: 73.11%, TrainLoss: 1.1563, ValLoss: 1.0116, LR: 0.001
[2026-01-30 05:06:41,210] [INFO] Epoch 9/50, ValAcc: 77.31%, TrainLoss: 1.0558, ValLoss: 0.9072, LR: 0.001
[2026-01-30 05:07:22,552] [INFO] Epoch 10/50, ValAcc: 79.63%, TrainLoss: 0.9521, ValLoss: 0.8228, LR: 0.001
[2026-01-30 05:08:04,394] [INFO] Epoch 11/50, ValAcc: 80.79%, TrainLoss: 0.8990, ValLoss: 0.7763, LR: 0.001
[2026-01-30 05:08:45,713] [INFO] Epoch 12/50, ValAcc: 81.91%, TrainLoss: 0.8380, ValLoss: 0.7596, LR: 0.001
[2026-01-30 05:09:27,561] [INFO] Epoch 13/50, ValAcc: 81.40%, TrainLoss: 0.8328, ValLoss: 0.7573, LR: 0.001
[2026-01-30 05:10:08,919] [INFO] Epoch 14/50, ValAcc: 80.98%, TrainLoss: 0.7695, ValLoss: 0.8418, LR: 0.001
[2026-01-30 05:10:50,773] [INFO] Epoch 15/50, ValAcc: 80.45%, TrainLoss: 0.7439, ValLoss: 0.8635, LR: 0.001
[2026-01-30 05:11:32,617] [INFO] Epoch 16/50, ValAcc: 84.60%, TrainLoss: 0.7199, ValLoss: 0.6274, LR: 0.001
[2026-01-30 05:12:13,957] [INFO] Epoch 17/50, ValAcc: 85.59%, TrainLoss: 0.6663, ValLoss: 0.5947, LR: 0.001
[2026-01-30 05:12:55,827] [INFO] Epoch 18/50, ValAcc: 85.89%, TrainLoss: 0.6579, ValLoss: 0.5826, LR: 0.001
[2026-01-30 05:13:36,573] [INFO] Epoch 19/50, ValAcc: 85.88%, TrainLoss: 0.6535, ValLoss: 0.6098, LR: 0.001
[2026-01-30 05:14:18,407] [INFO] Epoch 20/50, ValAcc: 86.97%, TrainLoss: 0.6220, ValLoss: 0.5592, LR: 0.001
[2026-01-30 05:15:00,261] [INFO] Epoch 21/50, ValAcc: 87.27%, TrainLoss: 0.5988, ValLoss: 0.5826, LR: 0.001
[2026-01-30 05:15:41,545] [INFO] Epoch 22/50, ValAcc: 86.89%, TrainLoss: 0.6143, ValLoss: 0.5597, LR: 0.001
[2026-01-30 05:16:23,351] [INFO] Epoch 23/50, ValAcc: 87.81%, TrainLoss: 0.5735, ValLoss: 0.5067, LR: 0.001
[2026-01-30 05:17:04,631] [INFO] Epoch 24/50, ValAcc: 87.49%, TrainLoss: 0.5815, ValLoss: 0.5089, LR: 0.001
[2026-01-30 05:17:46,454] [INFO] Epoch 25/50, ValAcc: 88.11%, TrainLoss: 0.5520, ValLoss: 0.5382, LR: 0.001
[2026-01-30 05:18:28,305] [INFO] Epoch 26/50, ValAcc: 84.71%, TrainLoss: 0.6285, ValLoss: 0.6672, LR: 0.001
[2026-01-30 05:19:09,615] [INFO] Epoch 27/50, ValAcc: 87.25%, TrainLoss: 0.6267, ValLoss: 0.5340, LR: 0.0005
[2026-01-30 05:19:51,418] [INFO] Epoch 28/50, ValAcc: 88.07%, TrainLoss: 0.5433, ValLoss: 0.4970, LR: 0.0005
[2026-01-30 05:20:32,738] [INFO] Epoch 29/50, ValAcc: 88.40%, TrainLoss: 0.5107, ValLoss: 0.4956, LR: 0.0005
[2026-01-30 05:21:14,543] [INFO] Epoch 30/50, ValAcc: 89.33%, TrainLoss: 0.4927, ValLoss: 0.4706, LR: 0.0005
[2026-01-30 05:21:55,830] [INFO] Epoch 31/50, ValAcc: 89.34%, TrainLoss: 0.4849, ValLoss: 0.4548, LR: 0.0005
[2026-01-30 05:22:37,642] [INFO] Epoch 32/50, ValAcc: 90.77%, TrainLoss: 0.4246, ValLoss: 0.4085, LR: 0.0005
[2026-01-30 05:23:19,434] [INFO] Epoch 33/50, ValAcc: 89.85%, TrainLoss: 0.4126, ValLoss: 0.4528, LR: 0.0005
[2026-01-30 05:24:00,050] [INFO] Epoch 34/50, ValAcc: 90.27%, TrainLoss: 0.4480, ValLoss: 0.4262, LR: 0.0005
[2026-01-30 05:24:41,884] [INFO] Epoch 35/50, ValAcc: 90.89%, TrainLoss: 0.4526, ValLoss: 0.3828, LR: 0.0005
[2026-01-30 05:25:23,187] [INFO] Epoch 36/50, ValAcc: 89.67%, TrainLoss: 0.4249, ValLoss: 0.4323, LR: 0.0005
[2026-01-30 05:26:04,946] [INFO] Epoch 37/50, ValAcc: 90.90%, TrainLoss: 0.4204, ValLoss: 0.4025, LR: 0.0005
[2026-01-30 05:26:46,730] [INFO] Epoch 38/50, ValAcc: 90.71%, TrainLoss: 0.4312, ValLoss: 0.4176, LR: 0.0005
[2026-01-30 05:27:28,023] [INFO] Epoch 39/50, ValAcc: 91.09%, TrainLoss: 0.3681, ValLoss: 0.3875, LR: 0.00025
[2026-01-30 05:28:09,847] [INFO] Epoch 40/50, ValAcc: 92.05%, TrainLoss: 0.3460, ValLoss: 0.3192, LR: 0.00025
[2026-01-30 05:28:51,172] [INFO] Epoch 41/50, ValAcc: 91.93%, TrainLoss: 0.3463, ValLoss: 0.3300, LR: 0.00025
[2026-01-30 05:29:33,027] [INFO] Epoch 42/50, ValAcc: 90.28%, TrainLoss: 0.3264, ValLoss: 0.4547, LR: 0.00025
[2026-01-30 05:30:14,867] [INFO] Epoch 43/50, ValAcc: 90.63%, TrainLoss: 0.4140, ValLoss: 0.4121, LR: 0.00025
[2026-01-30 05:30:56,165] [INFO] Epoch 44/50, ValAcc: 91.03%, TrainLoss: 0.3827, ValLoss: 0.4034, LR: 0.000125
[2026-01-30 05:31:37,982] [INFO] Epoch 45/50, ValAcc: 90.99%, TrainLoss: 0.3689, ValLoss: 0.4010, LR: 0.000125
[2026-01-30 05:32:19,333] [INFO] Epoch 46/50, ValAcc: 91.36%, TrainLoss: 0.3625, ValLoss: 0.3881, LR: 0.000125
[2026-01-30 05:33:01,181] [INFO] Epoch 47/50, ValAcc: 91.41%, TrainLoss: 0.3512, ValLoss: 0.3913, LR: 6.25e-05
[2026-01-30 05:33:01,181] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2026-01-30 05:33:09,114] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['rnn'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_rnn_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9110,0.9143,0.9225,0.9109
[2026-01-30 05:33:09,115] [INFO] Training Fold 2/5
[2026-01-30 05:34:47,354] [INFO] Feature 0 normalized using token
[2026-01-30 05:34:47,354] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2026-01-30 05:34:47,385] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): RNN(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (transformer_input_proj): ModuleList(
    (0): Identity()
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-30 05:34:47,386] [INFO] Training...
[2026-01-30 05:35:20,618] [INFO] Epoch 1/50, ValAcc: 6.70%, TrainLoss: 4.1355, ValLoss: 3.6772, LR: 0.001
[2026-01-30 05:35:38,957] [INFO] Epoch 2/50, ValAcc: 14.06%, TrainLoss: 3.5636, ValLoss: 3.2135, LR: 0.001
[2026-01-30 05:35:57,268] [INFO] Epoch 3/50, ValAcc: 30.58%, TrainLoss: 3.0123, ValLoss: 2.5127, LR: 0.001
[2026-01-30 05:36:15,825] [INFO] Epoch 4/50, ValAcc: 44.80%, TrainLoss: 2.4542, ValLoss: 2.0392, LR: 0.001
[2026-01-30 05:36:34,267] [INFO] Epoch 5/50, ValAcc: 51.41%, TrainLoss: 2.0656, ValLoss: 1.7978, LR: 0.001
[2026-01-30 05:36:52,871] [INFO] Epoch 6/50, ValAcc: 56.86%, TrainLoss: 1.7987, ValLoss: 1.6114, LR: 0.001
[2026-01-30 05:37:11,361] [INFO] Epoch 7/50, ValAcc: 61.34%, TrainLoss: 1.6377, ValLoss: 1.4721, LR: 0.001
[2026-01-30 05:37:29,948] [INFO] Epoch 8/50, ValAcc: 62.79%, TrainLoss: 1.5156, ValLoss: 1.3695, LR: 0.001
[2026-01-30 05:37:48,298] [INFO] Epoch 9/50, ValAcc: 67.41%, TrainLoss: 1.3973, ValLoss: 1.2332, LR: 0.001
[2026-01-30 05:38:06,659] [INFO] Epoch 10/50, ValAcc: 69.09%, TrainLoss: 1.2994, ValLoss: 1.1701, LR: 0.001
[2026-01-30 05:38:24,971] [INFO] Epoch 11/50, ValAcc: 72.75%, TrainLoss: 1.2063, ValLoss: 1.0410, LR: 0.001
[2026-01-30 05:38:43,308] [INFO] Epoch 12/50, ValAcc: 74.18%, TrainLoss: 1.1216, ValLoss: 0.9827, LR: 0.001
[2026-01-30 05:39:01,634] [INFO] Epoch 13/50, ValAcc: 75.69%, TrainLoss: 1.0628, ValLoss: 0.9359, LR: 0.001
[2026-01-30 05:39:19,955] [INFO] Epoch 14/50, ValAcc: 78.50%, TrainLoss: 0.9913, ValLoss: 0.8416, LR: 0.001
[2026-01-30 05:39:38,307] [INFO] Epoch 15/50, ValAcc: 80.05%, TrainLoss: 0.9233, ValLoss: 0.8011, LR: 0.001
[2026-01-30 05:39:56,624] [INFO] Epoch 16/50, ValAcc: 82.22%, TrainLoss: 0.8632, ValLoss: 0.7249, LR: 0.001
[2026-01-30 05:40:14,975] [INFO] Epoch 17/50, ValAcc: 82.78%, TrainLoss: 0.8147, ValLoss: 0.6945, LR: 0.001
[2026-01-30 05:40:33,283] [INFO] Epoch 18/50, ValAcc: 83.37%, TrainLoss: 0.7708, ValLoss: 0.6719, LR: 0.001
[2026-01-30 05:40:51,646] [INFO] Epoch 19/50, ValAcc: 84.02%, TrainLoss: 0.7227, ValLoss: 0.6368, LR: 0.001
[2026-01-30 05:41:09,933] [INFO] Epoch 20/50, ValAcc: 84.60%, TrainLoss: 0.7082, ValLoss: 0.6015, LR: 0.001
[2026-01-30 05:41:28,299] [INFO] Epoch 21/50, ValAcc: 85.40%, TrainLoss: 0.6775, ValLoss: 0.5810, LR: 0.001
[2026-01-30 05:41:46,593] [INFO] Epoch 22/50, ValAcc: 85.95%, TrainLoss: 0.6523, ValLoss: 0.5815, LR: 0.001
[2026-01-30 05:42:04,960] [INFO] Epoch 23/50, ValAcc: 86.06%, TrainLoss: 0.6371, ValLoss: 0.5682, LR: 0.001
[2026-01-30 05:42:23,244] [INFO] Epoch 24/50, ValAcc: 86.88%, TrainLoss: 0.6103, ValLoss: 0.5521, LR: 0.001
[2026-01-30 05:42:41,606] [INFO] Epoch 25/50, ValAcc: 87.11%, TrainLoss: 0.5979, ValLoss: 0.5353, LR: 0.001
[2026-01-30 05:42:59,901] [INFO] Epoch 26/50, ValAcc: 86.51%, TrainLoss: 0.5811, ValLoss: 0.5493, LR: 0.001
[2026-01-30 05:43:18,253] [INFO] Epoch 27/50, ValAcc: 87.30%, TrainLoss: 0.5798, ValLoss: 0.5145, LR: 0.001
[2026-01-30 05:43:36,551] [INFO] Epoch 28/50, ValAcc: 87.56%, TrainLoss: 0.5640, ValLoss: 0.5094, LR: 0.001
[2026-01-30 05:43:54,899] [INFO] Epoch 29/50, ValAcc: 88.19%, TrainLoss: 0.5540, ValLoss: 0.4899, LR: 0.001
[2026-01-30 05:44:13,214] [INFO] Epoch 30/50, ValAcc: 88.24%, TrainLoss: 0.5458, ValLoss: 0.4813, LR: 0.001
[2026-01-30 05:44:31,546] [INFO] Epoch 31/50, ValAcc: 88.67%, TrainLoss: 0.5357, ValLoss: 0.4633, LR: 0.001
[2026-01-30 05:44:49,871] [INFO] Epoch 32/50, ValAcc: 88.64%, TrainLoss: 0.5229, ValLoss: 0.4752, LR: 0.001
[2026-01-30 05:45:08,200] [INFO] Epoch 33/50, ValAcc: 88.79%, TrainLoss: 0.5231, ValLoss: 0.4866, LR: 0.001
[2026-01-30 05:45:26,511] [INFO] Epoch 34/50, ValAcc: 88.70%, TrainLoss: 0.5171, ValLoss: 0.4588, LR: 0.001
[2026-01-30 05:45:44,792] [INFO] Epoch 35/50, ValAcc: 88.76%, TrainLoss: 0.5213, ValLoss: 0.4921, LR: 0.001
[2026-01-30 05:46:03,108] [INFO] Epoch 36/50, ValAcc: 89.28%, TrainLoss: 0.5160, ValLoss: 0.4414, LR: 0.001
[2026-01-30 05:46:21,384] [INFO] Epoch 37/50, ValAcc: 89.17%, TrainLoss: 0.5236, ValLoss: 0.4357, LR: 0.001
[2026-01-30 05:46:39,710] [INFO] Epoch 38/50, ValAcc: 89.36%, TrainLoss: 0.5070, ValLoss: 0.4511, LR: 0.001
[2026-01-30 05:46:57,962] [INFO] Epoch 39/50, ValAcc: 88.85%, TrainLoss: 0.5380, ValLoss: 0.4747, LR: 0.001
[2026-01-30 05:47:16,287] [INFO] Epoch 40/50, ValAcc: 88.39%, TrainLoss: 0.5232, ValLoss: 0.4714, LR: 0.001
[2026-01-30 05:47:34,548] [INFO] Epoch 41/50, ValAcc: 89.60%, TrainLoss: 0.4618, ValLoss: 0.4385, LR: 0.0005
[2026-01-30 05:47:52,861] [INFO] Epoch 42/50, ValAcc: 89.44%, TrainLoss: 0.4349, ValLoss: 0.4353, LR: 0.0005
[2026-01-30 05:48:11,143] [INFO] Epoch 43/50, ValAcc: 90.04%, TrainLoss: 0.4253, ValLoss: 0.4448, LR: 0.0005
[2026-01-30 05:48:29,482] [INFO] Epoch 44/50, ValAcc: 90.76%, TrainLoss: 0.4175, ValLoss: 0.3881, LR: 0.0005
[2026-01-30 05:48:47,790] [INFO] Epoch 45/50, ValAcc: 90.53%, TrainLoss: 0.4049, ValLoss: 0.3886, LR: 0.0005
[2026-01-30 05:49:06,119] [INFO] Epoch 46/50, ValAcc: 89.99%, TrainLoss: 0.4370, ValLoss: 0.4367, LR: 0.0005
[2026-01-30 05:49:24,438] [INFO] Epoch 47/50, ValAcc: 90.49%, TrainLoss: 0.4092, ValLoss: 0.4201, LR: 0.0005
[2026-01-30 05:49:42,753] [INFO] Epoch 48/50, ValAcc: 90.64%, TrainLoss: 0.3786, ValLoss: 0.4103, LR: 0.00025
[2026-01-30 05:50:01,040] [INFO] Epoch 49/50, ValAcc: 91.44%, TrainLoss: 0.3671, ValLoss: 0.3610, LR: 0.00025
[2026-01-30 05:50:19,308] [INFO] Epoch 50/50, ValAcc: 91.10%, TrainLoss: 0.3406, ValLoss: 0.3645, LR: 0.00025
[2026-01-30 05:50:22,384] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['rnn'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_rnn_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9149,0.9176,0.9235,0.9156
[2026-01-30 05:50:22,385] [INFO] Training Fold 3/5
[2026-01-30 05:52:00,490] [INFO] Feature 0 normalized using token
[2026-01-30 05:52:00,491] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2026-01-30 05:52:00,523] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): RNN(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (transformer_input_proj): ModuleList(
    (0): Identity()
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-30 05:52:00,523] [INFO] Training...
[2026-01-30 05:52:18,709] [INFO] Epoch 1/50, ValAcc: 14.13%, TrainLoss: 4.0264, ValLoss: 3.1777, LR: 0.001
[2026-01-30 05:52:36,906] [INFO] Epoch 2/50, ValAcc: 39.94%, TrainLoss: 2.8024, ValLoss: 2.1788, LR: 0.001
[2026-01-30 05:52:55,098] [INFO] Epoch 3/50, ValAcc: 53.14%, TrainLoss: 2.0178, ValLoss: 1.7403, LR: 0.001
[2026-01-30 05:53:13,227] [INFO] Epoch 4/50, ValAcc: 59.60%, TrainLoss: 1.6533, ValLoss: 1.4849, LR: 0.001
[2026-01-30 05:53:31,407] [INFO] Epoch 5/50, ValAcc: 64.52%, TrainLoss: 1.5105, ValLoss: 1.3551, LR: 0.001
[2026-01-30 05:53:49,639] [INFO] Epoch 6/50, ValAcc: 67.44%, TrainLoss: 1.3638, ValLoss: 1.2224, LR: 0.001
[2026-01-30 05:54:07,850] [INFO] Epoch 7/50, ValAcc: 68.69%, TrainLoss: 1.2612, ValLoss: 1.1578, LR: 0.001
[2026-01-30 05:54:26,106] [INFO] Epoch 8/50, ValAcc: 72.13%, TrainLoss: 1.1658, ValLoss: 1.0360, LR: 0.001
[2026-01-30 05:54:44,288] [INFO] Epoch 9/50, ValAcc: 72.69%, TrainLoss: 1.0838, ValLoss: 0.9964, LR: 0.001
[2026-01-30 05:55:02,546] [INFO] Epoch 10/50, ValAcc: 76.40%, TrainLoss: 1.0267, ValLoss: 0.9021, LR: 0.001
[2026-01-30 05:55:20,782] [INFO] Epoch 11/50, ValAcc: 79.21%, TrainLoss: 0.9493, ValLoss: 0.8088, LR: 0.001
[2026-01-30 05:55:38,995] [INFO] Epoch 12/50, ValAcc: 80.66%, TrainLoss: 0.8757, ValLoss: 0.7392, LR: 0.001
[2026-01-30 05:55:57,215] [INFO] Epoch 13/50, ValAcc: 83.05%, TrainLoss: 0.8063, ValLoss: 0.6770, LR: 0.001
[2026-01-30 05:56:15,628] [INFO] Epoch 14/50, ValAcc: 84.15%, TrainLoss: 0.7734, ValLoss: 0.6246, LR: 0.001
[2026-01-30 05:56:33,874] [INFO] Epoch 15/50, ValAcc: 84.57%, TrainLoss: 0.7327, ValLoss: 0.6475, LR: 0.001
[2026-01-30 05:56:52,056] [INFO] Epoch 16/50, ValAcc: 85.51%, TrainLoss: 0.6746, ValLoss: 0.5732, LR: 0.001
[2026-01-30 05:57:10,206] [INFO] Epoch 17/50, ValAcc: 86.11%, TrainLoss: 0.6476, ValLoss: 0.5900, LR: 0.001
[2026-01-30 05:57:28,378] [INFO] Epoch 18/50, ValAcc: 85.35%, TrainLoss: 0.6229, ValLoss: 0.6073, LR: 0.001
[2026-01-30 05:57:46,520] [INFO] Epoch 19/50, ValAcc: 86.89%, TrainLoss: 0.6515, ValLoss: 0.5373, LR: 0.001
[2026-01-30 05:58:04,682] [INFO] Epoch 20/50, ValAcc: 84.62%, TrainLoss: 0.6039, ValLoss: 0.6335, LR: 0.001
[2026-01-30 05:58:22,809] [INFO] Epoch 21/50, ValAcc: 87.95%, TrainLoss: 0.5776, ValLoss: 0.4933, LR: 0.001
[2026-01-30 05:58:40,967] [INFO] Epoch 22/50, ValAcc: 87.93%, TrainLoss: 0.5613, ValLoss: 0.5046, LR: 0.001
[2026-01-30 05:58:59,100] [INFO] Epoch 23/50, ValAcc: 88.67%, TrainLoss: 0.5520, ValLoss: 0.4873, LR: 0.001
[2026-01-30 05:59:17,236] [INFO] Epoch 24/50, ValAcc: 88.11%, TrainLoss: 0.5520, ValLoss: 0.5055, LR: 0.001
[2026-01-30 05:59:35,372] [INFO] Epoch 25/50, ValAcc: 87.48%, TrainLoss: 0.5333, ValLoss: 0.5368, LR: 0.001
[2026-01-30 05:59:53,476] [INFO] Epoch 26/50, ValAcc: 88.52%, TrainLoss: 0.5385, ValLoss: 0.4722, LR: 0.001
[2026-01-30 06:00:11,601] [INFO] Epoch 27/50, ValAcc: 90.22%, TrainLoss: 0.4582, ValLoss: 0.3704, LR: 0.001
[2026-01-30 06:00:29,705] [INFO] Epoch 28/50, ValAcc: 90.70%, TrainLoss: 0.4484, ValLoss: 0.3605, LR: 0.001
[2026-01-30 06:00:47,823] [INFO] Epoch 29/50, ValAcc: 90.47%, TrainLoss: 0.4507, ValLoss: 0.3593, LR: 0.001
[2026-01-30 06:01:05,918] [INFO] Epoch 30/50, ValAcc: 88.92%, TrainLoss: 0.4698, ValLoss: 0.5257, LR: 0.001
[2026-01-30 06:01:24,035] [INFO] Epoch 31/50, ValAcc: 89.67%, TrainLoss: 0.5063, ValLoss: 0.4468, LR: 0.001
[2026-01-30 06:01:42,130] [INFO] Epoch 32/50, ValAcc: 91.11%, TrainLoss: 0.4903, ValLoss: 0.3734, LR: 0.001
[2026-01-30 06:02:00,233] [INFO] Epoch 33/50, ValAcc: 90.89%, TrainLoss: 0.4000, ValLoss: 0.3773, LR: 0.0005
[2026-01-30 06:02:18,317] [INFO] Epoch 34/50, ValAcc: 91.93%, TrainLoss: 0.4245, ValLoss: 0.3319, LR: 0.0005
[2026-01-30 06:02:36,418] [INFO] Epoch 35/50, ValAcc: 91.10%, TrainLoss: 0.3622, ValLoss: 0.3421, LR: 0.0005
[2026-01-30 06:02:54,519] [INFO] Epoch 36/50, ValAcc: 91.79%, TrainLoss: 0.3795, ValLoss: 0.3141, LR: 0.0005
[2026-01-30 06:03:12,623] [INFO] Epoch 37/50, ValAcc: 92.19%, TrainLoss: 0.3387, ValLoss: 0.3130, LR: 0.0005
[2026-01-30 06:03:30,743] [INFO] Epoch 38/50, ValAcc: 92.34%, TrainLoss: 0.3297, ValLoss: 0.3007, LR: 0.0005
[2026-01-30 06:03:48,860] [INFO] Epoch 39/50, ValAcc: 92.25%, TrainLoss: 0.3640, ValLoss: 0.2962, LR: 0.0005
[2026-01-30 06:04:06,980] [INFO] Epoch 40/50, ValAcc: 92.11%, TrainLoss: 0.3214, ValLoss: 0.2946, LR: 0.0005
[2026-01-30 06:04:25,141] [INFO] Epoch 41/50, ValAcc: 91.55%, TrainLoss: 0.3872, ValLoss: 0.3542, LR: 0.0005
[2026-01-30 06:04:43,277] [INFO] Epoch 42/50, ValAcc: 92.15%, TrainLoss: 0.3609, ValLoss: 0.3112, LR: 0.0005
[2026-01-30 06:05:01,396] [INFO] Epoch 43/50, ValAcc: 91.95%, TrainLoss: 0.3185, ValLoss: 0.3112, LR: 0.0005
[2026-01-30 06:05:19,512] [INFO] Epoch 44/50, ValAcc: 92.30%, TrainLoss: 0.3054, ValLoss: 0.2989, LR: 0.00025
[2026-01-30 06:05:37,620] [INFO] Epoch 45/50, ValAcc: 92.27%, TrainLoss: 0.2940, ValLoss: 0.2952, LR: 0.00025
[2026-01-30 06:05:55,743] [INFO] Epoch 46/50, ValAcc: 92.69%, TrainLoss: 0.2843, ValLoss: 0.2926, LR: 0.00025
[2026-01-30 06:06:13,860] [INFO] Epoch 47/50, ValAcc: 92.96%, TrainLoss: 0.2768, ValLoss: 0.2921, LR: 0.00025
[2026-01-30 06:06:31,987] [INFO] Epoch 48/50, ValAcc: 92.27%, TrainLoss: 0.2753, ValLoss: 0.2890, LR: 0.00025
[2026-01-30 06:06:50,096] [INFO] Epoch 49/50, ValAcc: 92.57%, TrainLoss: 0.2768, ValLoss: 0.2875, LR: 0.00025
[2026-01-30 06:07:08,206] [INFO] Epoch 50/50, ValAcc: 92.50%, TrainLoss: 0.2694, ValLoss: 0.2823, LR: 0.00025
[2026-01-30 06:07:11,262] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['rnn'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_rnn_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9222,0.9230,0.9331,0.9227
[2026-01-30 06:07:11,263] [INFO] Training Fold 4/5
[2026-01-30 06:08:47,929] [INFO] Feature 0 normalized using token
[2026-01-30 06:08:47,929] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2026-01-30 06:08:47,971] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): RNN(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (transformer_input_proj): ModuleList(
    (0): Identity()
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-30 06:08:47,971] [INFO] Training...
[2026-01-30 06:09:06,176] [INFO] Epoch 1/50, ValAcc: 6.90%, TrainLoss: 4.1099, ValLoss: 3.6931, LR: 0.001
[2026-01-30 06:09:24,431] [INFO] Epoch 2/50, ValAcc: 19.17%, TrainLoss: 3.4957, ValLoss: 3.0225, LR: 0.001
[2026-01-30 06:09:42,591] [INFO] Epoch 3/50, ValAcc: 35.99%, TrainLoss: 2.7403, ValLoss: 2.3374, LR: 0.001
[2026-01-30 06:10:00,793] [INFO] Epoch 4/50, ValAcc: 47.59%, TrainLoss: 2.2461, ValLoss: 1.9372, LR: 0.001
[2026-01-30 06:10:19,012] [INFO] Epoch 5/50, ValAcc: 50.63%, TrainLoss: 1.9973, ValLoss: 1.8160, LR: 0.001
[2026-01-30 06:10:37,196] [INFO] Epoch 6/50, ValAcc: 55.07%, TrainLoss: 1.8400, ValLoss: 1.6351, LR: 0.001
[2026-01-30 06:10:55,383] [INFO] Epoch 7/50, ValAcc: 57.15%, TrainLoss: 1.6928, ValLoss: 1.5853, LR: 0.001
[2026-01-30 06:11:13,557] [INFO] Epoch 8/50, ValAcc: 59.78%, TrainLoss: 1.6064, ValLoss: 1.4644, LR: 0.001
[2026-01-30 06:11:31,728] [INFO] Epoch 9/50, ValAcc: 60.47%, TrainLoss: 1.5374, ValLoss: 1.4341, LR: 0.001
[2026-01-30 06:11:49,897] [INFO] Epoch 10/50, ValAcc: 63.67%, TrainLoss: 1.4616, ValLoss: 1.3344, LR: 0.001
[2026-01-30 06:12:08,069] [INFO] Epoch 11/50, ValAcc: 65.60%, TrainLoss: 1.3922, ValLoss: 1.2463, LR: 0.001
[2026-01-30 06:12:26,243] [INFO] Epoch 12/50, ValAcc: 67.28%, TrainLoss: 1.3184, ValLoss: 1.1708, LR: 0.001
[2026-01-30 06:12:44,398] [INFO] Epoch 13/50, ValAcc: 69.87%, TrainLoss: 1.2346, ValLoss: 1.1188, LR: 0.001
[2026-01-30 06:13:02,598] [INFO] Epoch 14/50, ValAcc: 74.64%, TrainLoss: 1.1286, ValLoss: 0.9806, LR: 0.001
[2026-01-30 06:13:20,759] [INFO] Epoch 15/50, ValAcc: 78.05%, TrainLoss: 1.0446, ValLoss: 0.8787, LR: 0.001
[2026-01-30 06:13:38,932] [INFO] Epoch 16/50, ValAcc: 77.95%, TrainLoss: 0.9699, ValLoss: 0.8610, LR: 0.001
[2026-01-30 06:13:57,092] [INFO] Epoch 17/50, ValAcc: 80.38%, TrainLoss: 0.9175, ValLoss: 0.7792, LR: 0.001
[2026-01-30 06:14:15,343] [INFO] Epoch 18/50, ValAcc: 80.21%, TrainLoss: 0.8713, ValLoss: 0.7776, LR: 0.001
[2026-01-30 06:14:33,455] [INFO] Epoch 19/50, ValAcc: 81.16%, TrainLoss: 0.8432, ValLoss: 0.7528, LR: 0.001
[2026-01-30 06:14:51,659] [INFO] Epoch 20/50, ValAcc: 81.86%, TrainLoss: 0.8131, ValLoss: 0.7152, LR: 0.001
[2026-01-30 06:15:09,788] [INFO] Epoch 21/50, ValAcc: 82.44%, TrainLoss: 0.7945, ValLoss: 0.6927, LR: 0.001
[2026-01-30 06:15:27,974] [INFO] Epoch 22/50, ValAcc: 81.33%, TrainLoss: 0.7622, ValLoss: 0.7428, LR: 0.001
[2026-01-30 06:15:46,092] [INFO] Epoch 23/50, ValAcc: 83.34%, TrainLoss: 0.7504, ValLoss: 0.6682, LR: 0.001
[2026-01-30 06:16:17,061] [INFO] Epoch 24/50, ValAcc: 83.38%, TrainLoss: 0.7211, ValLoss: 0.6546, LR: 0.001
[2026-01-30 06:16:59,012] [INFO] Epoch 25/50, ValAcc: 84.38%, TrainLoss: 0.7010, ValLoss: 0.6537, LR: 0.001
[2026-01-30 06:17:40,963] [INFO] Epoch 26/50, ValAcc: 84.63%, TrainLoss: 0.6841, ValLoss: 0.6265, LR: 0.001
[2026-01-30 06:18:22,928] [INFO] Epoch 27/50, ValAcc: 84.02%, TrainLoss: 0.6915, ValLoss: 0.5970, LR: 0.001
[2026-01-30 06:19:04,869] [INFO] Epoch 28/50, ValAcc: 85.69%, TrainLoss: 0.6690, ValLoss: 0.5870, LR: 0.001
[2026-01-30 06:19:46,350] [INFO] Epoch 29/50, ValAcc: 85.40%, TrainLoss: 0.6548, ValLoss: 0.5886, LR: 0.001
[2026-01-30 06:20:28,300] [INFO] Epoch 30/50, ValAcc: 85.80%, TrainLoss: 0.6321, ValLoss: 0.5988, LR: 0.001
[2026-01-30 06:21:10,249] [INFO] Epoch 31/50, ValAcc: 85.61%, TrainLoss: 0.6166, ValLoss: 0.5629, LR: 0.001
[2026-01-30 06:21:52,191] [INFO] Epoch 32/50, ValAcc: 85.99%, TrainLoss: 0.6308, ValLoss: 0.5709, LR: 0.001
[2026-01-30 06:22:34,026] [INFO] Epoch 33/50, ValAcc: 85.88%, TrainLoss: 0.6158, ValLoss: 0.5552, LR: 0.001
[2026-01-30 06:23:15,636] [INFO] Epoch 34/50, ValAcc: 85.59%, TrainLoss: 0.6759, ValLoss: 0.5913, LR: 0.001
[2026-01-30 06:23:57,584] [INFO] Epoch 35/50, ValAcc: 86.52%, TrainLoss: 0.6079, ValLoss: 0.5472, LR: 0.001
[2026-01-30 06:24:39,541] [INFO] Epoch 36/50, ValAcc: 86.40%, TrainLoss: 0.6063, ValLoss: 0.5368, LR: 0.001
[2026-01-30 06:25:21,476] [INFO] Epoch 37/50, ValAcc: 86.60%, TrainLoss: 0.5767, ValLoss: 0.5410, LR: 0.001
[2026-01-30 06:26:03,062] [INFO] Epoch 38/50, ValAcc: 86.51%, TrainLoss: 0.6016, ValLoss: 0.5754, LR: 0.001
[2026-01-30 06:26:44,848] [INFO] Epoch 39/50, ValAcc: 87.31%, TrainLoss: 0.5728, ValLoss: 0.5375, LR: 0.001
[2026-01-30 06:27:26,717] [INFO] Epoch 40/50, ValAcc: 88.04%, TrainLoss: 0.4918, ValLoss: 0.4872, LR: 0.0005
[2026-01-30 06:28:08,617] [INFO] Epoch 41/50, ValAcc: 88.56%, TrainLoss: 0.4656, ValLoss: 0.4616, LR: 0.0005
[2026-01-30 06:28:50,412] [INFO] Epoch 42/50, ValAcc: 88.85%, TrainLoss: 0.4494, ValLoss: 0.4491, LR: 0.0005
[2026-01-30 06:29:31,856] [INFO] Epoch 43/50, ValAcc: 88.77%, TrainLoss: 0.4917, ValLoss: 0.4645, LR: 0.0005
[2026-01-30 06:30:13,735] [INFO] Epoch 44/50, ValAcc: 89.08%, TrainLoss: 0.4459, ValLoss: 0.4442, LR: 0.0005
[2026-01-30 06:30:55,668] [INFO] Epoch 45/50, ValAcc: 88.93%, TrainLoss: 0.4291, ValLoss: 0.4472, LR: 0.0005
[2026-01-30 06:31:37,620] [INFO] Epoch 46/50, ValAcc: 89.23%, TrainLoss: 0.4313, ValLoss: 0.4480, LR: 0.0005
[2026-01-30 06:32:19,565] [INFO] Epoch 47/50, ValAcc: 89.40%, TrainLoss: 0.4131, ValLoss: 0.4323, LR: 0.0005
[2026-01-30 06:33:01,031] [INFO] Epoch 48/50, ValAcc: 89.83%, TrainLoss: 0.4012, ValLoss: 0.4075, LR: 0.0005
[2026-01-30 06:33:42,893] [INFO] Epoch 49/50, ValAcc: 89.62%, TrainLoss: 0.4119, ValLoss: 0.4288, LR: 0.0005
[2026-01-30 06:34:24,810] [INFO] Epoch 50/50, ValAcc: 89.54%, TrainLoss: 0.4137, ValLoss: 0.4427, LR: 0.0005
[2026-01-30 06:34:32,760] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['rnn'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_rnn_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8997,0.9015,0.9103,0.8999
[2026-01-30 06:34:32,761] [INFO] Training Fold 5/5
[2026-01-30 06:36:08,484] [INFO] Feature 0 normalized using token
[2026-01-30 06:36:08,484] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2026-01-30 06:36:08,539] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): RNN(512, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (transformer_input_proj): ModuleList(
    (0): Identity()
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-30 06:36:08,539] [INFO] Training...
[2026-01-30 06:36:50,542] [INFO] Epoch 1/50, ValAcc: 25.85%, TrainLoss: 3.9120, ValLoss: 2.6280, LR: 0.001
[2026-01-30 06:37:32,539] [INFO] Epoch 2/50, ValAcc: 55.80%, TrainLoss: 2.0254, ValLoss: 1.6273, LR: 0.001
[2026-01-30 06:38:14,555] [INFO] Epoch 3/50, ValAcc: 62.18%, TrainLoss: 1.5771, ValLoss: 1.4252, LR: 0.001
[2026-01-30 06:38:56,536] [INFO] Epoch 4/50, ValAcc: 67.11%, TrainLoss: 1.3613, ValLoss: 1.2368, LR: 0.001
[2026-01-30 06:39:38,073] [INFO] Epoch 5/50, ValAcc: 71.72%, TrainLoss: 1.2003, ValLoss: 1.0767, LR: 0.001
[2026-01-30 06:40:20,084] [INFO] Epoch 6/50, ValAcc: 75.61%, TrainLoss: 1.0601, ValLoss: 0.9193, LR: 0.001
[2026-01-30 06:41:02,087] [INFO] Epoch 7/50, ValAcc: 80.13%, TrainLoss: 0.9240, ValLoss: 0.7832, LR: 0.001
[2026-01-30 06:41:44,076] [INFO] Epoch 8/50, ValAcc: 80.98%, TrainLoss: 0.8358, ValLoss: 0.7448, LR: 0.001
[2026-01-30 06:42:25,848] [INFO] Epoch 9/50, ValAcc: 81.37%, TrainLoss: 0.7768, ValLoss: 0.7071, LR: 0.001
[2026-01-30 06:43:07,603] [INFO] Epoch 10/50, ValAcc: 83.41%, TrainLoss: 0.7397, ValLoss: 0.6525, LR: 0.001
[2026-01-30 06:43:49,592] [INFO] Epoch 11/50, ValAcc: 83.93%, TrainLoss: 0.7006, ValLoss: 0.6300, LR: 0.001
[2026-01-30 06:44:31,569] [INFO] Epoch 12/50, ValAcc: 83.76%, TrainLoss: 0.6695, ValLoss: 0.6352, LR: 0.001
[2026-01-30 06:45:13,556] [INFO] Epoch 13/50, ValAcc: 84.83%, TrainLoss: 0.6559, ValLoss: 0.6277, LR: 0.001
[2026-01-30 06:45:55,103] [INFO] Epoch 14/50, ValAcc: 85.59%, TrainLoss: 0.6315, ValLoss: 0.5788, LR: 0.001
[2026-01-30 06:46:37,071] [INFO] Epoch 15/50, ValAcc: 85.01%, TrainLoss: 0.6268, ValLoss: 0.5809, LR: 0.001
[2026-01-30 06:47:19,048] [INFO] Epoch 16/50, ValAcc: 85.83%, TrainLoss: 0.5897, ValLoss: 0.5697, LR: 0.001
[2026-01-30 06:48:01,039] [INFO] Epoch 17/50, ValAcc: 86.30%, TrainLoss: 0.6015, ValLoss: 0.5649, LR: 0.001
[2026-01-30 06:48:43,029] [INFO] Epoch 18/50, ValAcc: 86.38%, TrainLoss: 0.5777, ValLoss: 0.5581, LR: 0.001
[2026-01-30 06:49:24,532] [INFO] Epoch 19/50, ValAcc: 86.95%, TrainLoss: 0.5537, ValLoss: 0.5358, LR: 0.001
[2026-01-30 06:50:06,518] [INFO] Epoch 20/50, ValAcc: 87.37%, TrainLoss: 0.5416, ValLoss: 0.5527, LR: 0.001
[2026-01-30 06:50:48,505] [INFO] Epoch 21/50, ValAcc: 84.22%, TrainLoss: 0.5354, ValLoss: 0.6433, LR: 0.001
[2026-01-30 06:51:30,480] [INFO] Epoch 22/50, ValAcc: 87.61%, TrainLoss: 0.5479, ValLoss: 0.5522, LR: 0.001
[2026-01-30 06:52:12,458] [INFO] Epoch 23/50, ValAcc: 88.30%, TrainLoss: 0.4606, ValLoss: 0.4794, LR: 0.0005
[2026-01-30 06:52:53,951] [INFO] Epoch 24/50, ValAcc: 88.53%, TrainLoss: 0.4519, ValLoss: 0.4694, LR: 0.0005
[2026-01-30 06:53:35,927] [INFO] Epoch 25/50, ValAcc: 88.41%, TrainLoss: 0.4375, ValLoss: 0.5136, LR: 0.0005
[2026-01-30 06:54:17,874] [INFO] Epoch 26/50, ValAcc: 88.53%, TrainLoss: 0.4313, ValLoss: 0.4972, LR: 0.0005
[2026-01-30 06:54:59,858] [INFO] Epoch 27/50, ValAcc: 88.92%, TrainLoss: 0.4325, ValLoss: 0.4659, LR: 0.0005
[2026-01-30 06:55:41,836] [INFO] Epoch 28/50, ValAcc: 89.34%, TrainLoss: 0.4098, ValLoss: 0.4427, LR: 0.0005
[2026-01-30 06:56:23,330] [INFO] Epoch 29/50, ValAcc: 89.10%, TrainLoss: 0.4236, ValLoss: 0.4714, LR: 0.0005
[2026-01-30 06:57:05,259] [INFO] Epoch 30/50, ValAcc: 89.40%, TrainLoss: 0.4060, ValLoss: 0.4561, LR: 0.0005
[2026-01-30 06:57:47,171] [INFO] Epoch 31/50, ValAcc: 89.76%, TrainLoss: 0.3919, ValLoss: 0.4580, LR: 0.0005
[2026-01-30 06:58:29,090] [INFO] Epoch 32/50, ValAcc: 89.94%, TrainLoss: 0.3571, ValLoss: 0.4394, LR: 0.00025
[2026-01-30 06:59:11,024] [INFO] Epoch 33/50, ValAcc: 90.67%, TrainLoss: 0.3482, ValLoss: 0.4123, LR: 0.00025
[2026-01-30 06:59:52,534] [INFO] Epoch 34/50, ValAcc: 90.80%, TrainLoss: 0.3607, ValLoss: 0.4261, LR: 0.00025
[2026-01-30 07:00:34,514] [INFO] Epoch 35/50, ValAcc: 90.70%, TrainLoss: 0.3479, ValLoss: 0.4313, LR: 0.00025
[2026-01-30 07:01:16,494] [INFO] Epoch 36/50, ValAcc: 89.92%, TrainLoss: 0.3715, ValLoss: 0.4469, LR: 0.00025
[2026-01-30 07:01:58,479] [INFO] Epoch 37/50, ValAcc: 90.26%, TrainLoss: 0.3285, ValLoss: 0.4245, LR: 0.000125
[2026-01-30 07:02:40,437] [INFO] Epoch 38/50, ValAcc: 90.93%, TrainLoss: 0.3243, ValLoss: 0.4238, LR: 0.000125
[2026-01-30 07:03:21,921] [INFO] Epoch 39/50, ValAcc: 90.90%, TrainLoss: 0.3139, ValLoss: 0.4203, LR: 0.000125
[2026-01-30 07:04:03,875] [INFO] Epoch 40/50, ValAcc: 91.09%, TrainLoss: 0.3016, ValLoss: 0.3990, LR: 6.25e-05
[2026-01-30 07:04:03,875] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2026-01-30 07:04:11,819] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['rnn'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769748943.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_rnn_1000_64_50_0.001_512_256_3_0.3_256_128'),0.9150,0.9147,0.9223,0.9150
[2026-01-30 07:04:12,718] [INFO] [(0.9110006509260903, 0.9143231696097046, 0.9224823189720764, 0.9108505818603478), (0.9149062074678975, 0.91756344691001, 0.9234936956192027, 0.9155689596475142), (0.9222393182625163, 0.9229503584470854, 0.9330530427540108, 0.9226764043090215), (0.8996922712747071, 0.9014709249656229, 0.910251301138673, 0.8999434052386828), (0.914960350337318, 0.9147453587555248, 0.9222686892162327, 0.9149649325429798)]
