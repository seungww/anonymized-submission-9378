=== Step4. Script Execution Started at Sat Jan 31 08:15:56 AM UTC 2026 ===
Base directory: meta-free-apps
Data prefix: meta
Output directory: output/meta-free-apps/model_evaluation
Running python vrscanner.py --train --path meta-free-apps/meta-ip_len/meta-ip_len.csv --debug_path output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.debug --leaderboard_path output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.csv --norm token --model transformer --kfold 5 --lr 0.0001 --strict
[2026-01-31 08:15:58,696] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['transformer'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.0001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'))
[2026-01-31 08:19:09,734] [INFO] Processed data from meta-free-apps/meta-ip_len/meta-ip_len.csv:
[2026-01-31 08:19:09,734] [INFO] (84492, 1000)
[2026-01-31 08:19:09,734] [INFO] [['656' '94' '52' ... '181' '52' '52']
 ['423' '87' '52' ... '60' '60' '52']
 ['423' '87' '52' ... '1432' '52' '1432']
 ...
 ['242' '87' '52' ... '424' '700' '52']
 ['60' '60' '52' ... '143' '52' '355']
 ['64' '52' '52' ... '91' '52' '91']]
[2026-01-31 08:19:10,069] [INFO] Training Fold 1/5
[2026-01-31 08:20:45,230] [INFO] Feature 0 normalized using token
[2026-01-31 08:20:45,230] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2026-01-31 08:20:45,373] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (transformer_input_proj): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-31 08:20:45,374] [INFO] Training...
[2026-01-31 08:21:34,906] [INFO] Epoch 1/50, ValAcc: 12.32%, TrainLoss: 4.2584, ValLoss: 3.4930, LR: 0.0001
[2026-01-31 08:22:24,651] [INFO] Epoch 2/50, ValAcc: 65.66%, TrainLoss: 2.7524, ValLoss: 1.5079, LR: 0.0001
[2026-01-31 08:23:14,491] [INFO] Epoch 3/50, ValAcc: 82.60%, TrainLoss: 1.2154, ValLoss: 0.7490, LR: 0.0001
[2026-01-31 08:24:04,225] [INFO] Epoch 4/50, ValAcc: 86.37%, TrainLoss: 0.7200, ValLoss: 0.5763, LR: 0.0001
[2026-01-31 08:24:53,880] [INFO] Epoch 5/50, ValAcc: 88.90%, TrainLoss: 0.5434, ValLoss: 0.4797, LR: 0.0001
[2026-01-31 08:25:43,440] [INFO] Epoch 6/50, ValAcc: 90.09%, TrainLoss: 0.4360, ValLoss: 0.4308, LR: 0.0001
[2026-01-31 08:26:32,983] [INFO] Epoch 7/50, ValAcc: 90.84%, TrainLoss: 0.3731, ValLoss: 0.3938, LR: 0.0001
[2026-01-31 08:27:22,488] [INFO] Epoch 8/50, ValAcc: 91.28%, TrainLoss: 0.3355, ValLoss: 0.3884, LR: 0.0001
[2026-01-31 08:28:12,000] [INFO] Epoch 9/50, ValAcc: 91.87%, TrainLoss: 0.2993, ValLoss: 0.3683, LR: 0.0001
[2026-01-31 08:29:01,494] [INFO] Epoch 10/50, ValAcc: 92.08%, TrainLoss: 0.2758, ValLoss: 0.3369, LR: 0.0001
[2026-01-31 08:29:50,973] [INFO] Epoch 11/50, ValAcc: 92.36%, TrainLoss: 0.2598, ValLoss: 0.3330, LR: 0.0001
[2026-01-31 08:30:40,469] [INFO] Epoch 12/50, ValAcc: 92.63%, TrainLoss: 0.2420, ValLoss: 0.3237, LR: 0.0001
[2026-01-31 08:31:30,001] [INFO] Epoch 13/50, ValAcc: 92.76%, TrainLoss: 0.2327, ValLoss: 0.3098, LR: 0.0001
[2026-01-31 08:32:19,538] [INFO] Epoch 14/50, ValAcc: 92.89%, TrainLoss: 0.2154, ValLoss: 0.2986, LR: 0.0001
[2026-01-31 08:33:09,057] [INFO] Epoch 15/50, ValAcc: 92.65%, TrainLoss: 0.2110, ValLoss: 0.3183, LR: 0.0001
[2026-01-31 08:33:58,559] [INFO] Epoch 16/50, ValAcc: 92.83%, TrainLoss: 0.1973, ValLoss: 0.3258, LR: 0.0001
[2026-01-31 08:34:48,055] [INFO] Epoch 17/50, ValAcc: 93.34%, TrainLoss: 0.1917, ValLoss: 0.2706, LR: 0.0001
[2026-01-31 08:35:37,536] [INFO] Epoch 18/50, ValAcc: 93.53%, TrainLoss: 0.1925, ValLoss: 0.2923, LR: 0.0001
[2026-01-31 08:36:27,002] [INFO] Epoch 19/50, ValAcc: 93.31%, TrainLoss: 0.1829, ValLoss: 0.3282, LR: 0.0001
[2026-01-31 08:37:16,428] [INFO] Epoch 20/50, ValAcc: 93.29%, TrainLoss: 0.1799, ValLoss: 0.3044, LR: 0.0001
[2026-01-31 08:38:05,840] [INFO] Epoch 21/50, ValAcc: 93.85%, TrainLoss: 0.1552, ValLoss: 0.3111, LR: 5e-05
[2026-01-31 08:38:55,239] [INFO] Epoch 22/50, ValAcc: 93.61%, TrainLoss: 0.1447, ValLoss: 0.3033, LR: 5e-05
[2026-01-31 08:39:44,665] [INFO] Epoch 23/50, ValAcc: 93.94%, TrainLoss: 0.1409, ValLoss: 0.3634, LR: 5e-05
[2026-01-31 08:40:34,116] [INFO] Epoch 24/50, ValAcc: 93.75%, TrainLoss: 0.1307, ValLoss: 0.3328, LR: 2.5e-05
[2026-01-31 08:41:23,591] [INFO] Epoch 25/50, ValAcc: 94.09%, TrainLoss: 0.1264, ValLoss: 0.3855, LR: 2.5e-05
[2026-01-31 08:42:13,081] [INFO] Epoch 26/50, ValAcc: 94.01%, TrainLoss: 0.1220, ValLoss: 0.3927, LR: 2.5e-05
[2026-01-31 08:43:02,559] [INFO] Epoch 27/50, ValAcc: 93.78%, TrainLoss: 0.1170, ValLoss: 0.3712, LR: 1.25e-05
[2026-01-31 08:43:52,019] [INFO] Epoch 28/50, ValAcc: 93.82%, TrainLoss: 0.1143, ValLoss: 0.3671, LR: 1.25e-05
[2026-01-31 08:44:41,463] [INFO] Epoch 29/50, ValAcc: 93.93%, TrainLoss: 0.1117, ValLoss: 0.3652, LR: 1.25e-05
[2026-01-31 08:45:30,895] [INFO] Epoch 30/50, ValAcc: 94.14%, TrainLoss: 0.1085, ValLoss: 0.3723, LR: 6.25e-06
[2026-01-31 08:45:30,895] [INFO] Learning rate 0.000006 is below threshold. Stopping early.
[2026-01-31 08:45:36,364] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['transformer'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.0001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_transformer_1000_64_50_0.0001_512_256_3_0.3_256_128'),0.9386,0.9386,0.9392,0.9388
[2026-01-31 08:45:36,365] [INFO] Training Fold 2/5
[2026-01-31 08:47:09,391] [INFO] Feature 0 normalized using token
[2026-01-31 08:47:09,391] [INFO] Train shape: (59143, 1000), Val shape: (8450, 1000), Test shape: (16899, 1000)
[2026-01-31 08:47:09,453] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (transformer_input_proj): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-31 08:47:09,453] [INFO] Training...
[2026-01-31 08:47:58,832] [INFO] Epoch 1/50, ValAcc: 17.40%, TrainLoss: 4.2200, ValLoss: 3.3157, LR: 0.0001
[2026-01-31 08:48:49,293] [INFO] Epoch 2/50, ValAcc: 75.38%, TrainLoss: 2.2435, ValLoss: 1.0802, LR: 0.0001
[2026-01-31 08:49:39,413] [INFO] Epoch 3/50, ValAcc: 85.86%, TrainLoss: 0.9305, ValLoss: 0.6042, LR: 0.0001
[2026-01-31 08:50:29,160] [INFO] Epoch 4/50, ValAcc: 88.86%, TrainLoss: 0.5873, ValLoss: 0.4803, LR: 0.0001
[2026-01-31 08:51:18,754] [INFO] Epoch 5/50, ValAcc: 90.30%, TrainLoss: 0.4511, ValLoss: 0.4123, LR: 0.0001
[2026-01-31 08:52:08,231] [INFO] Epoch 6/50, ValAcc: 91.12%, TrainLoss: 0.3812, ValLoss: 0.3781, LR: 0.0001
[2026-01-31 08:52:57,665] [INFO] Epoch 7/50, ValAcc: 91.62%, TrainLoss: 0.3319, ValLoss: 0.3665, LR: 0.0001
[2026-01-31 08:53:47,101] [INFO] Epoch 8/50, ValAcc: 91.99%, TrainLoss: 0.3002, ValLoss: 0.3565, LR: 0.0001
[2026-01-31 08:54:36,522] [INFO] Epoch 9/50, ValAcc: 91.85%, TrainLoss: 0.2773, ValLoss: 0.3420, LR: 0.0001
[2026-01-31 08:55:25,927] [INFO] Epoch 10/50, ValAcc: 92.34%, TrainLoss: 0.2530, ValLoss: 0.3317, LR: 0.0001
[2026-01-31 08:56:15,318] [INFO] Epoch 11/50, ValAcc: 92.47%, TrainLoss: 0.2370, ValLoss: 0.3365, LR: 0.0001
[2026-01-31 08:57:04,690] [INFO] Epoch 12/50, ValAcc: 92.69%, TrainLoss: 0.2264, ValLoss: 0.3189, LR: 0.0001
[2026-01-31 08:57:54,097] [INFO] Epoch 13/50, ValAcc: 92.58%, TrainLoss: 0.2179, ValLoss: 0.3206, LR: 0.0001
[2026-01-31 08:58:43,484] [INFO] Epoch 14/50, ValAcc: 93.01%, TrainLoss: 0.2012, ValLoss: 0.3147, LR: 0.0001
[2026-01-31 08:59:32,880] [INFO] Epoch 15/50, ValAcc: 92.95%, TrainLoss: 0.2007, ValLoss: 0.3140, LR: 0.0001
[2026-01-31 09:00:22,270] [INFO] Epoch 16/50, ValAcc: 92.89%, TrainLoss: 0.1899, ValLoss: 0.3074, LR: 0.0001
[2026-01-31 09:01:11,660] [INFO] Epoch 17/50, ValAcc: 92.97%, TrainLoss: 0.1833, ValLoss: 0.3218, LR: 0.0001
[2026-01-31 09:02:01,041] [INFO] Epoch 18/50, ValAcc: 93.20%, TrainLoss: 0.1768, ValLoss: 0.3052, LR: 0.0001
[2026-01-31 09:02:50,429] [INFO] Epoch 19/50, ValAcc: 93.40%, TrainLoss: 0.1710, ValLoss: 0.3045, LR: 0.0001
[2026-01-31 09:03:39,815] [INFO] Epoch 20/50, ValAcc: 93.24%, TrainLoss: 0.1648, ValLoss: 0.3185, LR: 0.0001
[2026-01-31 09:04:29,210] [INFO] Epoch 21/50, ValAcc: 93.31%, TrainLoss: 0.1637, ValLoss: 0.3143, LR: 0.0001
[2026-01-31 09:05:18,596] [INFO] Epoch 22/50, ValAcc: 93.43%, TrainLoss: 0.1642, ValLoss: 0.2969, LR: 0.0001
[2026-01-31 09:06:07,958] [INFO] Epoch 23/50, ValAcc: 93.30%, TrainLoss: 0.1511, ValLoss: 0.3005, LR: 0.0001
[2026-01-31 09:06:57,314] [INFO] Epoch 24/50, ValAcc: 93.37%, TrainLoss: 0.1469, ValLoss: 0.3102, LR: 0.0001
[2026-01-31 09:07:46,641] [INFO] Epoch 25/50, ValAcc: 93.17%, TrainLoss: 0.1477, ValLoss: 0.3353, LR: 0.0001
[2026-01-31 09:08:35,983] [INFO] Epoch 26/50, ValAcc: 93.67%, TrainLoss: 0.1278, ValLoss: 0.3148, LR: 5e-05
[2026-01-31 09:09:25,329] [INFO] Epoch 27/50, ValAcc: 93.53%, TrainLoss: 0.1201, ValLoss: 0.3297, LR: 5e-05
[2026-01-31 09:10:14,660] [INFO] Epoch 28/50, ValAcc: 93.54%, TrainLoss: 0.1150, ValLoss: 0.3354, LR: 5e-05
[2026-01-31 09:11:03,995] [INFO] Epoch 29/50, ValAcc: 93.64%, TrainLoss: 0.1052, ValLoss: 0.3401, LR: 2.5e-05
[2026-01-31 09:11:53,356] [INFO] Epoch 30/50, ValAcc: 93.72%, TrainLoss: 0.1012, ValLoss: 0.3588, LR: 2.5e-05
[2026-01-31 09:12:42,709] [INFO] Epoch 31/50, ValAcc: 93.72%, TrainLoss: 0.0974, ValLoss: 0.3617, LR: 2.5e-05
[2026-01-31 09:13:32,068] [INFO] Epoch 32/50, ValAcc: 93.69%, TrainLoss: 0.0919, ValLoss: 0.3728, LR: 1.25e-05
[2026-01-31 09:14:21,423] [INFO] Epoch 33/50, ValAcc: 93.76%, TrainLoss: 0.0887, ValLoss: 0.3747, LR: 1.25e-05
[2026-01-31 09:15:10,752] [INFO] Epoch 34/50, ValAcc: 93.70%, TrainLoss: 0.0875, ValLoss: 0.3656, LR: 1.25e-05
[2026-01-31 09:16:00,079] [INFO] Epoch 35/50, ValAcc: 93.82%, TrainLoss: 0.0838, ValLoss: 0.3733, LR: 6.25e-06
[2026-01-31 09:16:00,080] [INFO] Learning rate 0.000006 is below threshold. Stopping early.
[2026-01-31 09:16:05,545] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['transformer'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.0001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_transformer_1000_64_50_0.0001_512_256_3_0.3_256_128'),0.9398,0.9402,0.9406,0.9403
[2026-01-31 09:16:05,546] [INFO] Training Fold 3/5
[2026-01-31 09:17:37,639] [INFO] Feature 0 normalized using token
[2026-01-31 09:17:37,640] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2026-01-31 09:17:37,676] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (transformer_input_proj): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-31 09:17:37,676] [INFO] Training...
[2026-01-31 09:18:26,970] [INFO] Epoch 1/50, ValAcc: 13.60%, TrainLoss: 4.3255, ValLoss: 3.4149, LR: 0.0001
[2026-01-31 09:19:17,417] [INFO] Epoch 2/50, ValAcc: 68.17%, TrainLoss: 2.3346, ValLoss: 1.3063, LR: 0.0001
[2026-01-31 09:20:07,528] [INFO] Epoch 3/50, ValAcc: 81.75%, TrainLoss: 1.1215, ValLoss: 0.7431, LR: 0.0001
[2026-01-31 09:20:57,316] [INFO] Epoch 4/50, ValAcc: 87.08%, TrainLoss: 0.6986, ValLoss: 0.5236, LR: 0.0001
[2026-01-31 09:21:46,958] [INFO] Epoch 5/50, ValAcc: 89.48%, TrainLoss: 0.5162, ValLoss: 0.4191, LR: 0.0001
[2026-01-31 09:22:36,471] [INFO] Epoch 6/50, ValAcc: 90.58%, TrainLoss: 0.4110, ValLoss: 0.3606, LR: 0.0001
[2026-01-31 09:23:25,953] [INFO] Epoch 7/50, ValAcc: 91.86%, TrainLoss: 0.3467, ValLoss: 0.3114, LR: 0.0001
[2026-01-31 09:24:15,466] [INFO] Epoch 8/50, ValAcc: 92.60%, TrainLoss: 0.3011, ValLoss: 0.3138, LR: 0.0001
[2026-01-31 09:25:04,934] [INFO] Epoch 9/50, ValAcc: 92.47%, TrainLoss: 0.2720, ValLoss: 0.2816, LR: 0.0001
[2026-01-31 09:25:54,366] [INFO] Epoch 10/50, ValAcc: 92.93%, TrainLoss: 0.2496, ValLoss: 0.2752, LR: 0.0001
[2026-01-31 09:26:43,774] [INFO] Epoch 11/50, ValAcc: 92.89%, TrainLoss: 0.2282, ValLoss: 0.2739, LR: 0.0001
[2026-01-31 09:27:33,193] [INFO] Epoch 12/50, ValAcc: 92.95%, TrainLoss: 0.2239, ValLoss: 0.2790, LR: 0.0001
[2026-01-31 09:28:22,644] [INFO] Epoch 13/50, ValAcc: 93.28%, TrainLoss: 0.2096, ValLoss: 0.2674, LR: 0.0001
[2026-01-31 09:29:12,103] [INFO] Epoch 14/50, ValAcc: 93.76%, TrainLoss: 0.1994, ValLoss: 0.2625, LR: 0.0001
[2026-01-31 09:30:01,552] [INFO] Epoch 15/50, ValAcc: 93.18%, TrainLoss: 0.1939, ValLoss: 0.2611, LR: 0.0001
[2026-01-31 09:30:50,989] [INFO] Epoch 16/50, ValAcc: 93.70%, TrainLoss: 0.1909, ValLoss: 0.2507, LR: 0.0001
[2026-01-31 09:31:40,457] [INFO] Epoch 17/50, ValAcc: 93.74%, TrainLoss: 0.1807, ValLoss: 0.2728, LR: 0.0001
[2026-01-31 09:32:29,911] [INFO] Epoch 18/50, ValAcc: 93.57%, TrainLoss: 0.1750, ValLoss: 0.2602, LR: 0.0001
[2026-01-31 09:33:19,351] [INFO] Epoch 19/50, ValAcc: 93.18%, TrainLoss: 0.1685, ValLoss: 0.2782, LR: 0.0001
[2026-01-31 09:34:08,779] [INFO] Epoch 20/50, ValAcc: 93.87%, TrainLoss: 0.1486, ValLoss: 0.2542, LR: 5e-05
[2026-01-31 09:34:58,195] [INFO] Epoch 21/50, ValAcc: 94.06%, TrainLoss: 0.1402, ValLoss: 0.2588, LR: 5e-05
[2026-01-31 09:35:47,609] [INFO] Epoch 22/50, ValAcc: 93.60%, TrainLoss: 0.1392, ValLoss: 0.2709, LR: 5e-05
[2026-01-31 09:36:37,025] [INFO] Epoch 23/50, ValAcc: 93.94%, TrainLoss: 0.1265, ValLoss: 0.2732, LR: 2.5e-05
[2026-01-31 09:37:26,446] [INFO] Epoch 24/50, ValAcc: 93.95%, TrainLoss: 0.1216, ValLoss: 0.2764, LR: 2.5e-05
[2026-01-31 09:38:15,877] [INFO] Epoch 25/50, ValAcc: 93.95%, TrainLoss: 0.1190, ValLoss: 0.2775, LR: 2.5e-05
[2026-01-31 09:39:05,317] [INFO] Epoch 26/50, ValAcc: 93.95%, TrainLoss: 0.1124, ValLoss: 0.2828, LR: 1.25e-05
[2026-01-31 09:39:54,761] [INFO] Epoch 27/50, ValAcc: 94.08%, TrainLoss: 0.1109, ValLoss: 0.2806, LR: 1.25e-05
[2026-01-31 09:40:44,207] [INFO] Epoch 28/50, ValAcc: 94.04%, TrainLoss: 0.1065, ValLoss: 0.2902, LR: 1.25e-05
[2026-01-31 09:41:33,674] [INFO] Epoch 29/50, ValAcc: 94.01%, TrainLoss: 0.1048, ValLoss: 0.2948, LR: 6.25e-06
[2026-01-31 09:41:33,674] [INFO] Learning rate 0.000006 is below threshold. Stopping early.
[2026-01-31 09:41:39,162] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['transformer'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.0001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_transformer_1000_64_50_0.0001_512_256_3_0.3_256_128'),0.9357,0.9353,0.9368,0.9359
[2026-01-31 09:41:39,163] [INFO] Training Fold 4/5
[2026-01-31 09:43:16,012] [INFO] Feature 0 normalized using token
[2026-01-31 09:43:16,012] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2026-01-31 09:43:16,061] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (transformer_input_proj): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-31 09:43:16,062] [INFO] Training...
[2026-01-31 09:44:05,483] [INFO] Epoch 1/50, ValAcc: 22.65%, TrainLoss: 4.1411, ValLoss: 3.0306, LR: 0.0001
[2026-01-31 09:44:55,758] [INFO] Epoch 2/50, ValAcc: 55.15%, TrainLoss: 2.5645, ValLoss: 1.8088, LR: 0.0001
[2026-01-31 09:45:45,809] [INFO] Epoch 3/50, ValAcc: 77.68%, TrainLoss: 1.4697, ValLoss: 0.9601, LR: 0.0001
[2026-01-31 09:46:35,598] [INFO] Epoch 4/50, ValAcc: 84.13%, TrainLoss: 0.8734, ValLoss: 0.6964, LR: 0.0001
[2026-01-31 09:47:25,261] [INFO] Epoch 5/50, ValAcc: 87.20%, TrainLoss: 0.6553, ValLoss: 0.5643, LR: 0.0001
[2026-01-31 09:48:14,877] [INFO] Epoch 6/50, ValAcc: 88.20%, TrainLoss: 0.5227, ValLoss: 0.5137, LR: 0.0001
[2026-01-31 09:49:04,463] [INFO] Epoch 7/50, ValAcc: 89.16%, TrainLoss: 0.4492, ValLoss: 0.4793, LR: 0.0001
[2026-01-31 09:49:54,053] [INFO] Epoch 8/50, ValAcc: 89.64%, TrainLoss: 0.4113, ValLoss: 0.4577, LR: 0.0001
[2026-01-31 09:50:43,644] [INFO] Epoch 9/50, ValAcc: 90.20%, TrainLoss: 0.3699, ValLoss: 0.4414, LR: 0.0001
[2026-01-31 09:51:33,229] [INFO] Epoch 10/50, ValAcc: 90.41%, TrainLoss: 0.3462, ValLoss: 0.4221, LR: 0.0001
[2026-01-31 09:52:22,789] [INFO] Epoch 11/50, ValAcc: 91.09%, TrainLoss: 0.3242, ValLoss: 0.3998, LR: 0.0001
[2026-01-31 09:53:12,355] [INFO] Epoch 12/50, ValAcc: 90.95%, TrainLoss: 0.3017, ValLoss: 0.3988, LR: 0.0001
[2026-01-31 09:54:01,908] [INFO] Epoch 13/50, ValAcc: 91.25%, TrainLoss: 0.2866, ValLoss: 0.3949, LR: 0.0001
[2026-01-31 09:54:51,448] [INFO] Epoch 14/50, ValAcc: 91.51%, TrainLoss: 0.2675, ValLoss: 0.3866, LR: 0.0001
[2026-01-31 09:55:40,982] [INFO] Epoch 15/50, ValAcc: 91.60%, TrainLoss: 0.2548, ValLoss: 0.3924, LR: 0.0001
[2026-01-31 09:56:30,481] [INFO] Epoch 16/50, ValAcc: 92.00%, TrainLoss: 0.2396, ValLoss: 0.3704, LR: 0.0001
[2026-01-31 09:57:19,990] [INFO] Epoch 17/50, ValAcc: 92.09%, TrainLoss: 0.2275, ValLoss: 0.3685, LR: 0.0001
[2026-01-31 09:58:09,498] [INFO] Epoch 18/50, ValAcc: 91.67%, TrainLoss: 0.2183, ValLoss: 0.3796, LR: 0.0001
[2026-01-31 09:58:58,996] [INFO] Epoch 19/50, ValAcc: 92.37%, TrainLoss: 0.2100, ValLoss: 0.3556, LR: 0.0001
[2026-01-31 09:59:48,539] [INFO] Epoch 20/50, ValAcc: 91.88%, TrainLoss: 0.2033, ValLoss: 0.3889, LR: 0.0001
[2026-01-31 10:00:38,071] [INFO] Epoch 21/50, ValAcc: 92.20%, TrainLoss: 0.1939, ValLoss: 0.3709, LR: 0.0001
[2026-01-31 10:01:27,609] [INFO] Epoch 22/50, ValAcc: 92.05%, TrainLoss: 0.1857, ValLoss: 0.3626, LR: 0.0001
[2026-01-31 10:02:17,150] [INFO] Epoch 23/50, ValAcc: 92.83%, TrainLoss: 0.1633, ValLoss: 0.3635, LR: 5e-05
[2026-01-31 10:03:06,673] [INFO] Epoch 24/50, ValAcc: 92.62%, TrainLoss: 0.1526, ValLoss: 0.3799, LR: 5e-05
[2026-01-31 10:03:56,202] [INFO] Epoch 25/50, ValAcc: 92.63%, TrainLoss: 0.1499, ValLoss: 0.3941, LR: 5e-05
[2026-01-31 10:04:45,716] [INFO] Epoch 26/50, ValAcc: 92.66%, TrainLoss: 0.1381, ValLoss: 0.3961, LR: 2.5e-05
[2026-01-31 10:05:35,233] [INFO] Epoch 27/50, ValAcc: 92.69%, TrainLoss: 0.1317, ValLoss: 0.3909, LR: 2.5e-05
[2026-01-31 10:06:24,763] [INFO] Epoch 28/50, ValAcc: 92.63%, TrainLoss: 0.1288, ValLoss: 0.4099, LR: 2.5e-05
[2026-01-31 10:07:14,271] [INFO] Epoch 29/50, ValAcc: 92.59%, TrainLoss: 0.1229, ValLoss: 0.4100, LR: 1.25e-05
[2026-01-31 10:08:03,772] [INFO] Epoch 30/50, ValAcc: 92.64%, TrainLoss: 0.1190, ValLoss: 0.4214, LR: 1.25e-05
[2026-01-31 10:08:53,267] [INFO] Epoch 31/50, ValAcc: 92.77%, TrainLoss: 0.1188, ValLoss: 0.4281, LR: 1.25e-05
[2026-01-31 10:09:42,783] [INFO] Epoch 32/50, ValAcc: 92.71%, TrainLoss: 0.1146, ValLoss: 0.4319, LR: 6.25e-06
[2026-01-31 10:09:42,784] [INFO] Learning rate 0.000006 is below threshold. Stopping early.
[2026-01-31 10:09:48,274] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['transformer'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.0001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_transformer_1000_64_50_0.0001_512_256_3_0.3_256_128'),0.9296,0.9298,0.9308,0.9295
[2026-01-31 10:09:48,275] [INFO] Training Fold 5/5
[2026-01-31 10:11:23,857] [INFO] Feature 0 normalized using token
[2026-01-31 10:11:23,857] [INFO] Train shape: (59144, 1000), Val shape: (8450, 1000), Test shape: (16898, 1000)
[2026-01-31 10:11:23,930] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1386, 512)
  )
  (sequence_encoders): ModuleList(
    (0): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (transformer_input_proj): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=256, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=256, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=91, bias=True)
  )
)
[2026-01-31 10:11:23,930] [INFO] Training...
[2026-01-31 10:12:13,838] [INFO] Epoch 1/50, ValAcc: 14.45%, TrainLoss: 4.3167, ValLoss: 3.4826, LR: 0.0001
[2026-01-31 10:13:04,306] [INFO] Epoch 2/50, ValAcc: 76.37%, TrainLoss: 2.2895, ValLoss: 1.0907, LR: 0.0001
[2026-01-31 10:13:54,402] [INFO] Epoch 3/50, ValAcc: 83.87%, TrainLoss: 0.9349, ValLoss: 0.7007, LR: 0.0001
[2026-01-31 10:14:44,233] [INFO] Epoch 4/50, ValAcc: 87.56%, TrainLoss: 0.6291, ValLoss: 0.5190, LR: 0.0001
[2026-01-31 10:15:33,918] [INFO] Epoch 5/50, ValAcc: 89.63%, TrainLoss: 0.4770, ValLoss: 0.4348, LR: 0.0001
[2026-01-31 10:16:23,527] [INFO] Epoch 6/50, ValAcc: 90.47%, TrainLoss: 0.4016, ValLoss: 0.3990, LR: 0.0001
[2026-01-31 10:17:13,115] [INFO] Epoch 7/50, ValAcc: 91.27%, TrainLoss: 0.3500, ValLoss: 0.3656, LR: 0.0001
[2026-01-31 10:18:02,663] [INFO] Epoch 8/50, ValAcc: 91.22%, TrainLoss: 0.3120, ValLoss: 0.3542, LR: 0.0001
[2026-01-31 10:18:52,186] [INFO] Epoch 9/50, ValAcc: 91.93%, TrainLoss: 0.2857, ValLoss: 0.3541, LR: 0.0001
[2026-01-31 10:19:41,701] [INFO] Epoch 10/50, ValAcc: 92.21%, TrainLoss: 0.2612, ValLoss: 0.3223, LR: 0.0001
[2026-01-31 10:20:31,229] [INFO] Epoch 11/50, ValAcc: 92.65%, TrainLoss: 0.2516, ValLoss: 0.3146, LR: 0.0001
[2026-01-31 10:21:20,774] [INFO] Epoch 12/50, ValAcc: 92.30%, TrainLoss: 0.2319, ValLoss: 0.3269, LR: 0.0001
[2026-01-31 10:22:10,329] [INFO] Epoch 13/50, ValAcc: 92.52%, TrainLoss: 0.2252, ValLoss: 0.3126, LR: 0.0001
[2026-01-31 10:22:59,869] [INFO] Epoch 14/50, ValAcc: 92.71%, TrainLoss: 0.2143, ValLoss: 0.2963, LR: 0.0001
[2026-01-31 10:23:49,408] [INFO] Epoch 15/50, ValAcc: 92.80%, TrainLoss: 0.2038, ValLoss: 0.3143, LR: 0.0001
[2026-01-31 10:24:38,922] [INFO] Epoch 16/50, ValAcc: 92.73%, TrainLoss: 0.1941, ValLoss: 0.3134, LR: 0.0001
[2026-01-31 10:25:28,401] [INFO] Epoch 17/50, ValAcc: 92.75%, TrainLoss: 0.1915, ValLoss: 0.3272, LR: 0.0001
[2026-01-31 10:26:17,885] [INFO] Epoch 18/50, ValAcc: 93.36%, TrainLoss: 0.1700, ValLoss: 0.3086, LR: 5e-05
[2026-01-31 10:27:07,359] [INFO] Epoch 19/50, ValAcc: 93.28%, TrainLoss: 0.1565, ValLoss: 0.3177, LR: 5e-05
[2026-01-31 10:27:56,813] [INFO] Epoch 20/50, ValAcc: 93.42%, TrainLoss: 0.1519, ValLoss: 0.3158, LR: 5e-05
[2026-01-31 10:28:46,279] [INFO] Epoch 21/50, ValAcc: 93.33%, TrainLoss: 0.1404, ValLoss: 0.3160, LR: 2.5e-05
[2026-01-31 10:29:35,762] [INFO] Epoch 22/50, ValAcc: 93.31%, TrainLoss: 0.1349, ValLoss: 0.3160, LR: 2.5e-05
[2026-01-31 10:30:25,244] [INFO] Epoch 23/50, ValAcc: 93.15%, TrainLoss: 0.1324, ValLoss: 0.3233, LR: 2.5e-05
[2026-01-31 10:31:14,730] [INFO] Epoch 24/50, ValAcc: 93.20%, TrainLoss: 0.1266, ValLoss: 0.3318, LR: 1.25e-05
[2026-01-31 10:32:04,232] [INFO] Epoch 25/50, ValAcc: 93.44%, TrainLoss: 0.1235, ValLoss: 0.3322, LR: 1.25e-05
[2026-01-31 10:32:53,713] [INFO] Epoch 26/50, ValAcc: 93.66%, TrainLoss: 0.1219, ValLoss: 0.3374, LR: 1.25e-05
[2026-01-31 10:33:43,209] [INFO] Epoch 27/50, ValAcc: 93.50%, TrainLoss: 0.1171, ValLoss: 0.3386, LR: 6.25e-06
[2026-01-31 10:33:43,209] [INFO] Learning rate 0.000006 is below threshold. Stopping early.
[2026-01-31 10:33:48,699] [INFO] Namespace(path=['meta-free-apps/meta-ip_len/meta-ip_len.csv'], unknown_path=None, pktcount=1000, kfold=5, model=['transformer'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, cnn_kernel_size=3, cnn_layers=2, transformer_heads=4, transformer_ff=512, transformer_layers=2, transformer_dropout=0.1, batch_size=64, epoch=50, lr=0.0001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.debug', leaderboard_path='output/meta-free-apps/model_evaluation/model_evaluation_meta_1769847356.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, app_launch_detection_evaluation=False, device=device(type='cuda'), name='iplen_token_transformer_1000_64_50_0.0001_512_256_3_0.3_256_128'),0.9376,0.9371,0.9376,0.9372
[2026-01-31 10:33:49,623] [INFO] [(0.9386354222143322, 0.9385757117270193, 0.9392368404252168, 0.9388229036608531), (0.939818924196698, 0.9402114379365636, 0.940598109567701, 0.9402774697553823), (0.935732039294591, 0.9353316927924734, 0.9367554846305993, 0.9358708578744732), (0.9295774647887324, 0.929796114105348, 0.9307915238121383, 0.9294965884595701), (0.937625754527163, 0.937073934029768, 0.9375599940016424, 0.9372013626118192)]
=== Step4. Script Execution Finished at Sat Jan 31 10:33:51 AM UTC 2026 ===
