[2025-07-30 08:13:42,645] [INFO] Namespace(path=['vrchat-worlds/meta-tcp_window/meta-tcp_window.csv'], pktcount=2000, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753863221.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753863221.csv', step1=False, step2=False, step3=False, train=True, train_sliding=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'))
[2025-07-30 08:14:16,710] [INFO] Processed data from vrchat-worlds/meta-tcp_window/meta-tcp_window.csv:
[2025-07-30 08:14:16,710] [INFO] (15228, 2000)
[2025-07-30 08:14:16,710] [INFO] [['1281' '1281' '580' ... '300' '300' '300']
 ['65535' '65535' '256' ... '774' '2566' '2566']
 ['2038' '2038' '2038' ... '289' '289' '300']
 ...
 ['265' '774' '774' ... '268' '268' '310']
 ['265' '265' '332' ... '282' '282' '310']
 ['343' '761' '300' ... '774' '65535' '65535']]
[2025-07-30 08:14:16,818] [INFO] Training Fold 1/5
[2025-07-30 08:14:47,130] [INFO] Feature 0 normalized using token
[2025-07-30 08:14:47,131] [INFO] Train shape: (10659, 2000), Val shape: (1523, 2000), Test shape: (3046, 2000)
[2025-07-30 08:14:47,199] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(5717, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-30 08:14:47,199] [INFO] Training...
[2025-07-30 08:15:25,002] [INFO] Epoch 1/50, ValAcc: 50.23%, TrainLoss: 2.9392, ValLoss: 1.8875, LR: 0.001
[2025-07-30 08:16:01,490] [INFO] Epoch 2/50, ValAcc: 72.62%, TrainLoss: 1.4853, ValLoss: 1.0091, LR: 0.001
[2025-07-30 08:16:37,993] [INFO] Epoch 3/50, ValAcc: 81.16%, TrainLoss: 0.8438, ValLoss: 0.7128, LR: 0.001
[2025-07-30 08:17:14,457] [INFO] Epoch 4/50, ValAcc: 81.94%, TrainLoss: 0.5632, ValLoss: 0.7055, LR: 0.001
[2025-07-30 08:17:50,937] [INFO] Epoch 5/50, ValAcc: 85.49%, TrainLoss: 0.4070, ValLoss: 0.5682, LR: 0.001
[2025-07-30 08:18:27,386] [INFO] Epoch 6/50, ValAcc: 85.49%, TrainLoss: 0.3296, ValLoss: 0.6058, LR: 0.001
[2025-07-30 08:19:03,826] [INFO] Epoch 7/50, ValAcc: 86.61%, TrainLoss: 0.2551, ValLoss: 0.6637, LR: 0.001
[2025-07-30 08:19:39,954] [INFO] Epoch 8/50, ValAcc: 87.66%, TrainLoss: 0.2009, ValLoss: 0.6199, LR: 0.001
[2025-07-30 08:20:16,046] [INFO] Epoch 9/50, ValAcc: 90.15%, TrainLoss: 0.0995, ValLoss: 0.6277, LR: 0.0005
[2025-07-30 08:20:52,119] [INFO] Epoch 10/50, ValAcc: 90.22%, TrainLoss: 0.0623, ValLoss: 0.6501, LR: 0.0005
[2025-07-30 08:21:28,187] [INFO] Epoch 11/50, ValAcc: 91.07%, TrainLoss: 0.0516, ValLoss: 0.6533, LR: 0.0005
[2025-07-30 08:22:04,270] [INFO] Epoch 12/50, ValAcc: 90.41%, TrainLoss: 0.0380, ValLoss: 0.6654, LR: 0.00025
[2025-07-30 08:22:40,346] [INFO] Epoch 13/50, ValAcc: 90.54%, TrainLoss: 0.0333, ValLoss: 0.7061, LR: 0.00025
[2025-07-30 08:23:16,419] [INFO] Epoch 14/50, ValAcc: 90.74%, TrainLoss: 0.0288, ValLoss: 0.7181, LR: 0.00025
[2025-07-30 08:23:52,402] [INFO] Epoch 15/50, ValAcc: 90.48%, TrainLoss: 0.0257, ValLoss: 0.7280, LR: 0.000125
[2025-07-30 08:24:28,341] [INFO] Epoch 16/50, ValAcc: 90.54%, TrainLoss: 0.0241, ValLoss: 0.7434, LR: 0.000125
[2025-07-30 08:25:04,267] [INFO] Epoch 17/50, ValAcc: 90.54%, TrainLoss: 0.0227, ValLoss: 0.7448, LR: 0.000125
[2025-07-30 08:25:40,210] [INFO] Epoch 18/50, ValAcc: 90.68%, TrainLoss: 0.0209, ValLoss: 0.7601, LR: 6.25e-05
[2025-07-30 08:25:40,210] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-30 08:25:43,923] [INFO] Namespace(path=['vrchat-worlds/meta-tcp_window/meta-tcp_window.csv'], pktcount=2000, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753863221.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753863221.csv', step1=False, step2=False, step3=False, train=True, train_sliding=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='tcpwindow_token_bigru_2000_64_50_0.001_512_256_3_0.3_256_128'),0.9058,0.9038,0.9053,0.9049
[2025-07-30 08:25:43,924] [INFO] Training Fold 2/5
[2025-07-30 08:26:14,861] [INFO] Feature 0 normalized using token
[2025-07-30 08:26:14,861] [INFO] Train shape: (10659, 2000), Val shape: (1523, 2000), Test shape: (3046, 2000)
[2025-07-30 08:26:14,908] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(5660, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-30 08:26:14,909] [INFO] Training...
[2025-07-30 08:26:50,868] [INFO] Epoch 1/50, ValAcc: 52.99%, TrainLoss: 2.7855, ValLoss: 1.7292, LR: 0.001
[2025-07-30 08:27:26,820] [INFO] Epoch 2/50, ValAcc: 71.24%, TrainLoss: 1.2864, ValLoss: 0.9655, LR: 0.001
[2025-07-30 08:28:02,762] [INFO] Epoch 3/50, ValAcc: 80.89%, TrainLoss: 0.7357, ValLoss: 0.6823, LR: 0.001
[2025-07-30 08:28:38,711] [INFO] Epoch 4/50, ValAcc: 83.19%, TrainLoss: 0.4824, ValLoss: 0.6496, LR: 0.001
[2025-07-30 08:29:14,668] [INFO] Epoch 5/50, ValAcc: 84.83%, TrainLoss: 0.3435, ValLoss: 0.6320, LR: 0.001
[2025-07-30 08:29:50,636] [INFO] Epoch 6/50, ValAcc: 85.82%, TrainLoss: 0.2632, ValLoss: 0.5795, LR: 0.001
[2025-07-30 08:30:26,580] [INFO] Epoch 7/50, ValAcc: 87.92%, TrainLoss: 0.2092, ValLoss: 0.6097, LR: 0.001
[2025-07-30 08:31:02,533] [INFO] Epoch 8/50, ValAcc: 85.88%, TrainLoss: 0.1836, ValLoss: 0.6401, LR: 0.001
[2025-07-30 08:31:38,471] [INFO] Epoch 9/50, ValAcc: 86.28%, TrainLoss: 0.1700, ValLoss: 0.6771, LR: 0.001
[2025-07-30 08:32:14,431] [INFO] Epoch 10/50, ValAcc: 88.44%, TrainLoss: 0.0847, ValLoss: 0.6238, LR: 0.0005
[2025-07-30 08:32:50,380] [INFO] Epoch 11/50, ValAcc: 88.58%, TrainLoss: 0.0525, ValLoss: 0.6617, LR: 0.0005
[2025-07-30 08:33:26,328] [INFO] Epoch 12/50, ValAcc: 89.23%, TrainLoss: 0.0408, ValLoss: 0.6319, LR: 0.0005
[2025-07-30 08:34:02,283] [INFO] Epoch 13/50, ValAcc: 89.36%, TrainLoss: 0.0288, ValLoss: 0.6438, LR: 0.00025
[2025-07-30 08:34:38,234] [INFO] Epoch 14/50, ValAcc: 89.03%, TrainLoss: 0.0250, ValLoss: 0.6632, LR: 0.00025
[2025-07-30 08:35:14,175] [INFO] Epoch 15/50, ValAcc: 89.43%, TrainLoss: 0.0229, ValLoss: 0.6731, LR: 0.00025
[2025-07-30 08:35:50,132] [INFO] Epoch 16/50, ValAcc: 89.43%, TrainLoss: 0.0196, ValLoss: 0.6805, LR: 0.000125
[2025-07-30 08:36:26,093] [INFO] Epoch 17/50, ValAcc: 89.17%, TrainLoss: 0.0175, ValLoss: 0.6955, LR: 0.000125
[2025-07-30 08:37:02,045] [INFO] Epoch 18/50, ValAcc: 89.49%, TrainLoss: 0.0167, ValLoss: 0.6998, LR: 0.000125
[2025-07-30 08:37:38,003] [INFO] Epoch 19/50, ValAcc: 89.82%, TrainLoss: 0.0151, ValLoss: 0.7010, LR: 6.25e-05
[2025-07-30 08:37:38,003] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-30 08:37:41,723] [INFO] Namespace(path=['vrchat-worlds/meta-tcp_window/meta-tcp_window.csv'], pktcount=2000, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753863221.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753863221.csv', step1=False, step2=False, step3=False, train=True, train_sliding=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='tcpwindow_token_bigru_2000_64_50_0.001_512_256_3_0.3_256_128'),0.9028,0.9036,0.9051,0.9044
[2025-07-30 08:37:41,723] [INFO] Training Fold 3/5
[2025-07-30 08:38:12,094] [INFO] Feature 0 normalized using token
[2025-07-30 08:38:12,094] [INFO] Train shape: (10659, 2000), Val shape: (1523, 2000), Test shape: (3046, 2000)
[2025-07-30 08:38:12,138] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(5654, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-30 08:38:12,138] [INFO] Training...
[2025-07-30 08:38:48,066] [INFO] Epoch 1/50, ValAcc: 52.20%, TrainLoss: 2.8519, ValLoss: 1.7908, LR: 0.001
[2025-07-30 08:39:24,014] [INFO] Epoch 2/50, ValAcc: 71.31%, TrainLoss: 1.4138, ValLoss: 1.0106, LR: 0.001
[2025-07-30 08:39:59,952] [INFO] Epoch 3/50, ValAcc: 77.61%, TrainLoss: 0.8199, ValLoss: 0.7766, LR: 0.001
[2025-07-30 08:40:35,863] [INFO] Epoch 4/50, ValAcc: 82.07%, TrainLoss: 0.5717, ValLoss: 0.6657, LR: 0.001
[2025-07-30 08:41:11,901] [INFO] Epoch 5/50, ValAcc: 83.32%, TrainLoss: 0.3915, ValLoss: 0.6894, LR: 0.001
[2025-07-30 08:41:48,318] [INFO] Epoch 6/50, ValAcc: 85.55%, TrainLoss: 0.3194, ValLoss: 0.6210, LR: 0.001
[2025-07-30 08:42:24,762] [INFO] Epoch 7/50, ValAcc: 86.01%, TrainLoss: 0.2446, ValLoss: 0.6004, LR: 0.001
[2025-07-30 08:43:01,190] [INFO] Epoch 8/50, ValAcc: 86.01%, TrainLoss: 0.2150, ValLoss: 0.6665, LR: 0.001
[2025-07-30 08:43:37,614] [INFO] Epoch 9/50, ValAcc: 87.33%, TrainLoss: 0.1759, ValLoss: 0.6412, LR: 0.001
[2025-07-30 08:44:14,098] [INFO] Epoch 10/50, ValAcc: 87.85%, TrainLoss: 0.1403, ValLoss: 0.7135, LR: 0.001
[2025-07-30 08:44:50,534] [INFO] Epoch 11/50, ValAcc: 88.90%, TrainLoss: 0.0778, ValLoss: 0.6509, LR: 0.0005
[2025-07-30 08:45:26,965] [INFO] Epoch 12/50, ValAcc: 89.63%, TrainLoss: 0.0439, ValLoss: 0.6797, LR: 0.0005
[2025-07-30 08:46:03,412] [INFO] Epoch 13/50, ValAcc: 89.43%, TrainLoss: 0.0339, ValLoss: 0.6919, LR: 0.0005
[2025-07-30 08:46:39,843] [INFO] Epoch 14/50, ValAcc: 89.43%, TrainLoss: 0.0309, ValLoss: 0.6920, LR: 0.00025
[2025-07-30 08:47:16,288] [INFO] Epoch 15/50, ValAcc: 89.63%, TrainLoss: 0.0254, ValLoss: 0.7025, LR: 0.00025
[2025-07-30 08:47:52,733] [INFO] Epoch 16/50, ValAcc: 89.30%, TrainLoss: 0.0216, ValLoss: 0.7325, LR: 0.00025
[2025-07-30 08:48:29,158] [INFO] Epoch 17/50, ValAcc: 89.49%, TrainLoss: 0.0203, ValLoss: 0.7205, LR: 0.000125
[2025-07-30 08:49:05,661] [INFO] Epoch 18/50, ValAcc: 89.95%, TrainLoss: 0.0176, ValLoss: 0.7307, LR: 0.000125
[2025-07-30 08:49:42,093] [INFO] Epoch 19/50, ValAcc: 90.09%, TrainLoss: 0.0164, ValLoss: 0.7560, LR: 0.000125
[2025-07-30 08:50:18,528] [INFO] Epoch 20/50, ValAcc: 89.63%, TrainLoss: 0.0156, ValLoss: 0.7565, LR: 6.25e-05
[2025-07-30 08:50:18,528] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-30 08:50:22,242] [INFO] Namespace(path=['vrchat-worlds/meta-tcp_window/meta-tcp_window.csv'], pktcount=2000, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753863221.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753863221.csv', step1=False, step2=False, step3=False, train=True, train_sliding=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='tcpwindow_token_bigru_2000_64_50_0.001_512_256_3_0.3_256_128'),0.8936,0.8940,0.8959,0.8949
[2025-07-30 08:50:22,242] [INFO] Training Fold 4/5
[2025-07-30 08:50:53,515] [INFO] Feature 0 normalized using token
[2025-07-30 08:50:53,516] [INFO] Train shape: (10660, 2000), Val shape: (1523, 2000), Test shape: (3045, 2000)
[2025-07-30 08:50:53,562] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(5720, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-30 08:50:53,562] [INFO] Training...
[2025-07-30 08:51:30,029] [INFO] Epoch 1/50, ValAcc: 53.51%, TrainLoss: 2.8032, ValLoss: 1.6889, LR: 0.001
[2025-07-30 08:52:06,469] [INFO] Epoch 2/50, ValAcc: 73.21%, TrainLoss: 1.3649, ValLoss: 1.0012, LR: 0.001
[2025-07-30 08:52:42,910] [INFO] Epoch 3/50, ValAcc: 80.04%, TrainLoss: 0.8016, ValLoss: 0.7355, LR: 0.001
[2025-07-30 08:53:19,337] [INFO] Epoch 4/50, ValAcc: 81.35%, TrainLoss: 0.5532, ValLoss: 0.6552, LR: 0.001
[2025-07-30 08:53:55,794] [INFO] Epoch 5/50, ValAcc: 83.72%, TrainLoss: 0.3977, ValLoss: 0.6492, LR: 0.001
[2025-07-30 08:54:32,237] [INFO] Epoch 6/50, ValAcc: 84.77%, TrainLoss: 0.3069, ValLoss: 0.6420, LR: 0.001
[2025-07-30 08:55:08,681] [INFO] Epoch 7/50, ValAcc: 85.75%, TrainLoss: 0.2373, ValLoss: 0.6080, LR: 0.001
[2025-07-30 08:55:45,144] [INFO] Epoch 8/50, ValAcc: 86.21%, TrainLoss: 0.1980, ValLoss: 0.6995, LR: 0.001
[2025-07-30 08:56:21,578] [INFO] Epoch 9/50, ValAcc: 85.69%, TrainLoss: 0.1687, ValLoss: 0.6640, LR: 0.001
[2025-07-30 08:56:58,069] [INFO] Epoch 10/50, ValAcc: 87.46%, TrainLoss: 0.1406, ValLoss: 0.7522, LR: 0.001
[2025-07-30 08:57:34,525] [INFO] Epoch 11/50, ValAcc: 89.03%, TrainLoss: 0.0787, ValLoss: 0.6456, LR: 0.0005
[2025-07-30 08:58:10,967] [INFO] Epoch 12/50, ValAcc: 89.36%, TrainLoss: 0.0457, ValLoss: 0.6876, LR: 0.0005
[2025-07-30 08:58:47,415] [INFO] Epoch 13/50, ValAcc: 89.49%, TrainLoss: 0.0424, ValLoss: 0.6928, LR: 0.0005
[2025-07-30 08:59:23,859] [INFO] Epoch 14/50, ValAcc: 89.89%, TrainLoss: 0.0296, ValLoss: 0.6710, LR: 0.00025
[2025-07-30 09:00:00,327] [INFO] Epoch 15/50, ValAcc: 89.95%, TrainLoss: 0.0271, ValLoss: 0.7073, LR: 0.00025
[2025-07-30 09:00:36,823] [INFO] Epoch 16/50, ValAcc: 89.69%, TrainLoss: 0.0250, ValLoss: 0.7145, LR: 0.00025
[2025-07-30 09:01:13,272] [INFO] Epoch 17/50, ValAcc: 90.02%, TrainLoss: 0.0211, ValLoss: 0.7125, LR: 0.000125
[2025-07-30 09:01:49,705] [INFO] Epoch 18/50, ValAcc: 89.76%, TrainLoss: 0.0201, ValLoss: 0.7234, LR: 0.000125
[2025-07-30 09:02:26,162] [INFO] Epoch 19/50, ValAcc: 89.69%, TrainLoss: 0.0190, ValLoss: 0.7294, LR: 0.000125
[2025-07-30 09:03:02,591] [INFO] Epoch 20/50, ValAcc: 89.95%, TrainLoss: 0.0171, ValLoss: 0.7367, LR: 6.25e-05
[2025-07-30 09:03:02,591] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-30 09:03:06,296] [INFO] Namespace(path=['vrchat-worlds/meta-tcp_window/meta-tcp_window.csv'], pktcount=2000, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753863221.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753863221.csv', step1=False, step2=False, step3=False, train=True, train_sliding=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='tcpwindow_token_bigru_2000_64_50_0.001_512_256_3_0.3_256_128'),0.9054,0.9046,0.9062,0.9058
[2025-07-30 09:03:06,297] [INFO] Training Fold 5/5
[2025-07-30 09:03:36,144] [INFO] Feature 0 normalized using token
[2025-07-30 09:03:36,144] [INFO] Train shape: (10660, 2000), Val shape: (1523, 2000), Test shape: (3045, 2000)
[2025-07-30 09:03:36,188] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(5707, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-30 09:03:36,188] [INFO] Training...
[2025-07-30 09:04:12,652] [INFO] Epoch 1/50, ValAcc: 56.99%, TrainLoss: 2.7644, ValLoss: 1.6404, LR: 0.001
[2025-07-30 09:04:49,086] [INFO] Epoch 2/50, ValAcc: 75.38%, TrainLoss: 1.3017, ValLoss: 0.8868, LR: 0.001
[2025-07-30 09:05:25,544] [INFO] Epoch 3/50, ValAcc: 80.63%, TrainLoss: 0.7367, ValLoss: 0.6547, LR: 0.001
[2025-07-30 09:06:01,986] [INFO] Epoch 4/50, ValAcc: 82.60%, TrainLoss: 0.5097, ValLoss: 0.6081, LR: 0.001
[2025-07-30 09:06:38,447] [INFO] Epoch 5/50, ValAcc: 85.82%, TrainLoss: 0.3750, ValLoss: 0.5283, LR: 0.001
[2025-07-30 09:07:14,907] [INFO] Epoch 6/50, ValAcc: 87.46%, TrainLoss: 0.2863, ValLoss: 0.5612, LR: 0.001
[2025-07-30 09:07:51,347] [INFO] Epoch 7/50, ValAcc: 87.92%, TrainLoss: 0.2209, ValLoss: 0.5684, LR: 0.001
[2025-07-30 09:08:27,769] [INFO] Epoch 8/50, ValAcc: 87.07%, TrainLoss: 0.1899, ValLoss: 0.5944, LR: 0.001
[2025-07-30 09:09:04,256] [INFO] Epoch 9/50, ValAcc: 88.97%, TrainLoss: 0.1093, ValLoss: 0.5730, LR: 0.0005
[2025-07-30 09:09:40,405] [INFO] Epoch 10/50, ValAcc: 90.28%, TrainLoss: 0.0615, ValLoss: 0.5582, LR: 0.0005
[2025-07-30 09:10:16,369] [INFO] Epoch 11/50, ValAcc: 90.09%, TrainLoss: 0.0532, ValLoss: 0.5503, LR: 0.0005
[2025-07-30 09:10:52,307] [INFO] Epoch 12/50, ValAcc: 90.15%, TrainLoss: 0.0362, ValLoss: 0.5664, LR: 0.00025
[2025-07-30 09:11:28,255] [INFO] Epoch 13/50, ValAcc: 90.22%, TrainLoss: 0.0301, ValLoss: 0.5912, LR: 0.00025
[2025-07-30 09:12:04,218] [INFO] Epoch 14/50, ValAcc: 90.35%, TrainLoss: 0.0285, ValLoss: 0.6231, LR: 0.00025
[2025-07-30 09:12:40,154] [INFO] Epoch 15/50, ValAcc: 90.54%, TrainLoss: 0.0248, ValLoss: 0.6298, LR: 0.000125
[2025-07-30 09:13:16,533] [INFO] Epoch 16/50, ValAcc: 90.48%, TrainLoss: 0.0223, ValLoss: 0.6399, LR: 0.000125
[2025-07-30 09:13:53,012] [INFO] Epoch 17/50, ValAcc: 90.48%, TrainLoss: 0.0205, ValLoss: 0.6508, LR: 0.000125
[2025-07-30 09:14:29,471] [INFO] Epoch 18/50, ValAcc: 90.48%, TrainLoss: 0.0201, ValLoss: 0.6527, LR: 6.25e-05
[2025-07-30 09:14:29,471] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-30 09:14:33,181] [INFO] Namespace(path=['vrchat-worlds/meta-tcp_window/meta-tcp_window.csv'], pktcount=2000, kfold=5, model=['bigru'], norm=['token'], attention=False, input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753863221.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753863221.csv', step1=False, step2=False, step3=False, train=True, train_sliding=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='tcpwindow_token_bigru_2000_64_50_0.001_512_256_3_0.3_256_128'),0.9044,0.9035,0.9047,0.9041
[2025-07-30 09:14:33,621] [INFO] [(0.9057780695994747, 0.9037937935781813, 0.9052870327488549, 0.9049092696157709), (0.9028233749179252, 0.9036331024186048, 0.9050855904700026, 0.9044410155412994), (0.8936309914642153, 0.8940434212939607, 0.895889154199935, 0.8948619229775855), (0.9054187192118227, 0.9046306582636618, 0.9062109097257307, 0.9058238951857444), (0.9044334975369458, 0.9034776880545443, 0.9046571950912142, 0.904072401537342)]
