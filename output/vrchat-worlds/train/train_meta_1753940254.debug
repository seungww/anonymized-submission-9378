[2025-07-31 05:37:36,144] [INFO] Namespace(path=['vrchat-worlds/meta-ip_len/meta-ip_len.csv'], pktcount=1000, kfold=5, model=['bigru'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753940254.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753940254.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'))
[2025-07-31 05:38:06,731] [INFO] Processed data from vrchat-worlds/meta-ip_len/meta-ip_len.csv:
[2025-07-31 05:38:06,731] [INFO] (15228, 1000)
[2025-07-31 05:38:06,731] [INFO] [['244' '141' '52' ... '52' '52' '100']
 ['60' '60' '52' ... '52' '52' '1432']
 ['1432' '1432' '1432' ... '52' '930' '76']
 ...
 ['52' '1432' '1432' ... '1432' '1432' '1432']
 ['1432' '340' '52' ... '1432' '1432' '1432']
 ['52' '52' '1432' ... '52' '254' '235']]
[2025-07-31 05:38:06,780] [INFO] Training Fold 1/5
[2025-07-31 05:38:18,453] [INFO] Feature 0 normalized using token
[2025-07-31 05:38:18,453] [INFO] Train shape: (10659, 1000), Val shape: (1523, 1000), Test shape: (3046, 1000)
[2025-07-31 05:38:18,507] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-31 05:38:18,507] [INFO] Training...
[2025-07-31 05:38:38,170] [INFO] Epoch 1/50, ValAcc: 20.42%, TrainLoss: 3.5269, ValLoss: 2.8748, LR: 0.001
[2025-07-31 05:38:56,647] [INFO] Epoch 2/50, ValAcc: 44.78%, TrainLoss: 2.4741, ValLoss: 1.7725, LR: 0.001
[2025-07-31 05:39:15,127] [INFO] Epoch 3/50, ValAcc: 65.92%, TrainLoss: 1.5542, ValLoss: 1.1199, LR: 0.001
[2025-07-31 05:39:33,600] [INFO] Epoch 4/50, ValAcc: 71.70%, TrainLoss: 0.9951, ValLoss: 0.8415, LR: 0.001
[2025-07-31 05:39:52,078] [INFO] Epoch 5/50, ValAcc: 75.25%, TrainLoss: 0.7285, ValLoss: 0.7325, LR: 0.001
[2025-07-31 05:40:10,556] [INFO] Epoch 6/50, ValAcc: 78.53%, TrainLoss: 0.5754, ValLoss: 0.6768, LR: 0.001
[2025-07-31 05:40:29,030] [INFO] Epoch 7/50, ValAcc: 78.27%, TrainLoss: 0.4558, ValLoss: 0.6868, LR: 0.001
[2025-07-31 05:40:47,497] [INFO] Epoch 8/50, ValAcc: 79.45%, TrainLoss: 0.3993, ValLoss: 0.7814, LR: 0.001
[2025-07-31 05:41:05,968] [INFO] Epoch 9/50, ValAcc: 79.71%, TrainLoss: 0.3493, ValLoss: 0.6990, LR: 0.001
[2025-07-31 05:41:24,450] [INFO] Epoch 10/50, ValAcc: 82.21%, TrainLoss: 0.2062, ValLoss: 0.6757, LR: 0.0005
[2025-07-31 05:41:42,910] [INFO] Epoch 11/50, ValAcc: 82.21%, TrainLoss: 0.1446, ValLoss: 0.7517, LR: 0.0005
[2025-07-31 05:42:01,384] [INFO] Epoch 12/50, ValAcc: 82.01%, TrainLoss: 0.1193, ValLoss: 0.7723, LR: 0.0005
[2025-07-31 05:42:19,859] [INFO] Epoch 13/50, ValAcc: 82.40%, TrainLoss: 0.1059, ValLoss: 0.8214, LR: 0.0005
[2025-07-31 05:42:38,307] [INFO] Epoch 14/50, ValAcc: 82.53%, TrainLoss: 0.0783, ValLoss: 0.8400, LR: 0.00025
[2025-07-31 05:42:56,767] [INFO] Epoch 15/50, ValAcc: 82.67%, TrainLoss: 0.0652, ValLoss: 0.8575, LR: 0.00025
[2025-07-31 05:43:15,213] [INFO] Epoch 16/50, ValAcc: 82.07%, TrainLoss: 0.0585, ValLoss: 0.9184, LR: 0.00025
[2025-07-31 05:43:33,657] [INFO] Epoch 17/50, ValAcc: 82.80%, TrainLoss: 0.0537, ValLoss: 0.8894, LR: 0.000125
[2025-07-31 05:43:52,102] [INFO] Epoch 18/50, ValAcc: 82.40%, TrainLoss: 0.0502, ValLoss: 0.9181, LR: 0.000125
[2025-07-31 05:44:10,549] [INFO] Epoch 19/50, ValAcc: 82.99%, TrainLoss: 0.0453, ValLoss: 0.9341, LR: 0.000125
[2025-07-31 05:44:28,986] [INFO] Epoch 20/50, ValAcc: 82.93%, TrainLoss: 0.0445, ValLoss: 0.9392, LR: 6.25e-05
[2025-07-31 05:44:28,987] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-31 05:44:31,825] [INFO] Namespace(path=['vrchat-worlds/meta-ip_len/meta-ip_len.csv'], pktcount=1000, kfold=5, model=['bigru'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753940254.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753940254.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8355,0.8318,0.8348,0.8336
[2025-07-31 05:44:31,826] [INFO] Training Fold 2/5
[2025-07-31 05:44:43,506] [INFO] Feature 0 normalized using token
[2025-07-31 05:44:43,506] [INFO] Train shape: (10659, 1000), Val shape: (1523, 1000), Test shape: (3046, 1000)
[2025-07-31 05:44:43,535] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-31 05:44:43,535] [INFO] Training...
[2025-07-31 05:45:02,005] [INFO] Epoch 1/50, ValAcc: 17.01%, TrainLoss: 3.5684, ValLoss: 2.9467, LR: 0.001
[2025-07-31 05:45:20,472] [INFO] Epoch 2/50, ValAcc: 49.90%, TrainLoss: 2.4206, ValLoss: 1.6801, LR: 0.001
[2025-07-31 05:45:38,941] [INFO] Epoch 3/50, ValAcc: 65.20%, TrainLoss: 1.4409, ValLoss: 1.0650, LR: 0.001
[2025-07-31 05:45:57,408] [INFO] Epoch 4/50, ValAcc: 74.33%, TrainLoss: 0.9421, ValLoss: 0.8283, LR: 0.001
[2025-07-31 05:46:15,869] [INFO] Epoch 5/50, ValAcc: 76.43%, TrainLoss: 0.6941, ValLoss: 0.7560, LR: 0.001
[2025-07-31 05:46:34,333] [INFO] Epoch 6/50, ValAcc: 78.14%, TrainLoss: 0.5709, ValLoss: 0.7268, LR: 0.001
[2025-07-31 05:46:52,795] [INFO] Epoch 7/50, ValAcc: 78.59%, TrainLoss: 0.4567, ValLoss: 0.6723, LR: 0.001
[2025-07-31 05:47:11,152] [INFO] Epoch 8/50, ValAcc: 79.84%, TrainLoss: 0.3926, ValLoss: 0.7029, LR: 0.001
[2025-07-31 05:47:29,457] [INFO] Epoch 9/50, ValAcc: 81.16%, TrainLoss: 0.3292, ValLoss: 0.6873, LR: 0.001
[2025-07-31 05:47:47,747] [INFO] Epoch 10/50, ValAcc: 79.19%, TrainLoss: 0.2705, ValLoss: 0.8314, LR: 0.001
[2025-07-31 05:48:06,044] [INFO] Epoch 11/50, ValAcc: 81.75%, TrainLoss: 0.1690, ValLoss: 0.7548, LR: 0.0005
[2025-07-31 05:48:24,357] [INFO] Epoch 12/50, ValAcc: 80.56%, TrainLoss: 0.1095, ValLoss: 0.8374, LR: 0.0005
[2025-07-31 05:48:42,681] [INFO] Epoch 13/50, ValAcc: 81.62%, TrainLoss: 0.0981, ValLoss: 0.8860, LR: 0.0005
[2025-07-31 05:49:00,992] [INFO] Epoch 14/50, ValAcc: 81.02%, TrainLoss: 0.0702, ValLoss: 0.8997, LR: 0.00025
[2025-07-31 05:49:19,306] [INFO] Epoch 15/50, ValAcc: 81.94%, TrainLoss: 0.0552, ValLoss: 0.9366, LR: 0.00025
[2025-07-31 05:49:37,623] [INFO] Epoch 16/50, ValAcc: 81.55%, TrainLoss: 0.0477, ValLoss: 0.9639, LR: 0.00025
[2025-07-31 05:49:55,951] [INFO] Epoch 17/50, ValAcc: 81.81%, TrainLoss: 0.0426, ValLoss: 0.9829, LR: 0.000125
[2025-07-31 05:50:14,274] [INFO] Epoch 18/50, ValAcc: 81.16%, TrainLoss: 0.0383, ValLoss: 1.0152, LR: 0.000125
[2025-07-31 05:50:32,576] [INFO] Epoch 19/50, ValAcc: 81.16%, TrainLoss: 0.0361, ValLoss: 1.0361, LR: 0.000125
[2025-07-31 05:50:50,889] [INFO] Epoch 20/50, ValAcc: 81.35%, TrainLoss: 0.0336, ValLoss: 1.0326, LR: 6.25e-05
[2025-07-31 05:50:50,889] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-31 05:50:53,665] [INFO] Namespace(path=['vrchat-worlds/meta-ip_len/meta-ip_len.csv'], pktcount=1000, kfold=5, model=['bigru'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753940254.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753940254.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8191,0.8201,0.8217,0.8226
[2025-07-31 05:50:53,665] [INFO] Training Fold 3/5
[2025-07-31 05:51:05,059] [INFO] Feature 0 normalized using token
[2025-07-31 05:51:05,059] [INFO] Train shape: (10659, 1000), Val shape: (1523, 1000), Test shape: (3046, 1000)
[2025-07-31 05:51:05,085] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-31 05:51:05,085] [INFO] Training...
[2025-07-31 05:51:23,416] [INFO] Epoch 1/50, ValAcc: 18.58%, TrainLoss: 3.5426, ValLoss: 2.8875, LR: 0.001
[2025-07-31 05:51:41,734] [INFO] Epoch 2/50, ValAcc: 49.51%, TrainLoss: 2.3566, ValLoss: 1.6568, LR: 0.001
[2025-07-31 05:52:00,042] [INFO] Epoch 3/50, ValAcc: 65.00%, TrainLoss: 1.3892, ValLoss: 1.0773, LR: 0.001
[2025-07-31 05:52:18,343] [INFO] Epoch 4/50, ValAcc: 70.78%, TrainLoss: 0.9369, ValLoss: 0.9023, LR: 0.001
[2025-07-31 05:52:36,665] [INFO] Epoch 5/50, ValAcc: 73.93%, TrainLoss: 0.7182, ValLoss: 0.7877, LR: 0.001
[2025-07-31 05:52:54,982] [INFO] Epoch 6/50, ValAcc: 76.89%, TrainLoss: 0.5877, ValLoss: 0.7168, LR: 0.001
[2025-07-31 05:53:13,305] [INFO] Epoch 7/50, ValAcc: 78.00%, TrainLoss: 0.4689, ValLoss: 0.6992, LR: 0.001
[2025-07-31 05:53:31,624] [INFO] Epoch 8/50, ValAcc: 78.92%, TrainLoss: 0.3895, ValLoss: 0.6626, LR: 0.001
[2025-07-31 05:53:49,945] [INFO] Epoch 9/50, ValAcc: 77.94%, TrainLoss: 0.3592, ValLoss: 0.7525, LR: 0.001
[2025-07-31 05:54:08,258] [INFO] Epoch 10/50, ValAcc: 78.00%, TrainLoss: 0.3009, ValLoss: 0.8090, LR: 0.001
[2025-07-31 05:54:26,570] [INFO] Epoch 11/50, ValAcc: 79.58%, TrainLoss: 0.2707, ValLoss: 0.7347, LR: 0.001
[2025-07-31 05:54:44,878] [INFO] Epoch 12/50, ValAcc: 81.62%, TrainLoss: 0.1650, ValLoss: 0.6920, LR: 0.0005
[2025-07-31 05:55:03,202] [INFO] Epoch 13/50, ValAcc: 82.99%, TrainLoss: 0.0995, ValLoss: 0.7984, LR: 0.0005
[2025-07-31 05:55:21,526] [INFO] Epoch 14/50, ValAcc: 82.99%, TrainLoss: 0.0859, ValLoss: 0.8357, LR: 0.0005
[2025-07-31 05:55:39,848] [INFO] Epoch 15/50, ValAcc: 83.19%, TrainLoss: 0.0661, ValLoss: 0.8178, LR: 0.00025
[2025-07-31 05:55:58,164] [INFO] Epoch 16/50, ValAcc: 83.45%, TrainLoss: 0.0531, ValLoss: 0.8265, LR: 0.00025
[2025-07-31 05:56:16,487] [INFO] Epoch 17/50, ValAcc: 83.26%, TrainLoss: 0.0492, ValLoss: 0.8430, LR: 0.00025
[2025-07-31 05:56:34,808] [INFO] Epoch 18/50, ValAcc: 83.45%, TrainLoss: 0.0430, ValLoss: 0.8719, LR: 0.000125
[2025-07-31 05:56:53,123] [INFO] Epoch 19/50, ValAcc: 83.72%, TrainLoss: 0.0403, ValLoss: 0.9020, LR: 0.000125
[2025-07-31 05:57:11,444] [INFO] Epoch 20/50, ValAcc: 83.45%, TrainLoss: 0.0384, ValLoss: 0.8825, LR: 0.000125
[2025-07-31 05:57:29,763] [INFO] Epoch 21/50, ValAcc: 83.72%, TrainLoss: 0.0377, ValLoss: 0.8946, LR: 6.25e-05
[2025-07-31 05:57:29,763] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-31 05:57:32,539] [INFO] Namespace(path=['vrchat-worlds/meta-ip_len/meta-ip_len.csv'], pktcount=1000, kfold=5, model=['bigru'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753940254.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753940254.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8175,0.8192,0.8243,0.8223
[2025-07-31 05:57:32,539] [INFO] Training Fold 4/5
[2025-07-31 05:57:43,548] [INFO] Feature 0 normalized using token
[2025-07-31 05:57:43,549] [INFO] Train shape: (10660, 1000), Val shape: (1523, 1000), Test shape: (3045, 1000)
[2025-07-31 05:57:43,573] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-31 05:57:43,574] [INFO] Training...
[2025-07-31 05:58:01,906] [INFO] Epoch 1/50, ValAcc: 19.04%, TrainLoss: 3.5929, ValLoss: 2.9540, LR: 0.001
[2025-07-31 05:58:20,230] [INFO] Epoch 2/50, ValAcc: 48.92%, TrainLoss: 2.4330, ValLoss: 1.7145, LR: 0.001
[2025-07-31 05:58:38,553] [INFO] Epoch 3/50, ValAcc: 66.19%, TrainLoss: 1.4239, ValLoss: 1.0468, LR: 0.001
[2025-07-31 05:58:56,873] [INFO] Epoch 4/50, ValAcc: 73.34%, TrainLoss: 0.9391, ValLoss: 0.8084, LR: 0.001
[2025-07-31 05:59:15,213] [INFO] Epoch 5/50, ValAcc: 74.66%, TrainLoss: 0.7112, ValLoss: 0.7256, LR: 0.001
[2025-07-31 05:59:33,547] [INFO] Epoch 6/50, ValAcc: 74.66%, TrainLoss: 0.5635, ValLoss: 0.7346, LR: 0.001
[2025-07-31 05:59:51,868] [INFO] Epoch 7/50, ValAcc: 76.69%, TrainLoss: 0.4801, ValLoss: 0.7378, LR: 0.001
[2025-07-31 06:00:10,187] [INFO] Epoch 8/50, ValAcc: 76.03%, TrainLoss: 0.3917, ValLoss: 0.8282, LR: 0.001
[2025-07-31 06:00:28,500] [INFO] Epoch 9/50, ValAcc: 78.99%, TrainLoss: 0.2419, ValLoss: 0.7118, LR: 0.0005
[2025-07-31 06:00:46,803] [INFO] Epoch 10/50, ValAcc: 78.20%, TrainLoss: 0.1796, ValLoss: 0.7745, LR: 0.0005
[2025-07-31 06:01:05,137] [INFO] Epoch 11/50, ValAcc: 78.27%, TrainLoss: 0.1458, ValLoss: 0.9091, LR: 0.0005
[2025-07-31 06:01:23,455] [INFO] Epoch 12/50, ValAcc: 78.59%, TrainLoss: 0.1264, ValLoss: 0.8769, LR: 0.0005
[2025-07-31 06:01:41,781] [INFO] Epoch 13/50, ValAcc: 79.84%, TrainLoss: 0.0940, ValLoss: 0.9076, LR: 0.00025
[2025-07-31 06:02:00,110] [INFO] Epoch 14/50, ValAcc: 80.04%, TrainLoss: 0.0757, ValLoss: 0.9150, LR: 0.00025
[2025-07-31 06:02:18,429] [INFO] Epoch 15/50, ValAcc: 80.37%, TrainLoss: 0.0657, ValLoss: 0.9490, LR: 0.00025
[2025-07-31 06:02:36,747] [INFO] Epoch 16/50, ValAcc: 80.24%, TrainLoss: 0.0567, ValLoss: 0.9586, LR: 0.000125
[2025-07-31 06:02:55,052] [INFO] Epoch 17/50, ValAcc: 79.51%, TrainLoss: 0.0530, ValLoss: 1.0001, LR: 0.000125
[2025-07-31 06:03:13,376] [INFO] Epoch 18/50, ValAcc: 80.63%, TrainLoss: 0.0502, ValLoss: 0.9890, LR: 0.000125
[2025-07-31 06:03:31,702] [INFO] Epoch 19/50, ValAcc: 80.30%, TrainLoss: 0.0451, ValLoss: 1.0133, LR: 6.25e-05
[2025-07-31 06:03:31,702] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-31 06:03:34,504] [INFO] Namespace(path=['vrchat-worlds/meta-ip_len/meta-ip_len.csv'], pktcount=1000, kfold=5, model=['bigru'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753940254.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753940254.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8240,0.8216,0.8233,0.8235
[2025-07-31 06:03:34,504] [INFO] Training Fold 5/5
[2025-07-31 06:03:45,701] [INFO] Feature 0 normalized using token
[2025-07-31 06:03:45,701] [INFO] Train shape: (10660, 1000), Val shape: (1523, 1000), Test shape: (3045, 1000)
[2025-07-31 06:03:45,730] [INFO] Classifier: VRScannerModel(
  (input_modules): ModuleList(
    (0): Embedding(1385, 512)
  )
  (rnn_layers): ModuleList(
    (0): GRU(512, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  )
  (attn_nets): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
  (projection_layers): ModuleList(
    (0): Linear(in_features=512, out_features=256, bias=True)
  )
  (feature_attn_net): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): Tanh()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=128, out_features=50, bias=True)
  )
)
[2025-07-31 06:03:45,730] [INFO] Training...
[2025-07-31 06:04:04,067] [INFO] Epoch 1/50, ValAcc: 20.81%, TrainLoss: 3.5766, ValLoss: 2.7956, LR: 0.001
[2025-07-31 06:04:22,488] [INFO] Epoch 2/50, ValAcc: 49.18%, TrainLoss: 2.3805, ValLoss: 1.7082, LR: 0.001
[2025-07-31 06:04:40,926] [INFO] Epoch 3/50, ValAcc: 68.02%, TrainLoss: 1.4691, ValLoss: 0.9926, LR: 0.001
[2025-07-31 06:04:59,366] [INFO] Epoch 4/50, ValAcc: 75.11%, TrainLoss: 0.9505, ValLoss: 0.7771, LR: 0.001
[2025-07-31 06:05:17,802] [INFO] Epoch 5/50, ValAcc: 77.94%, TrainLoss: 0.7010, ValLoss: 0.6604, LR: 0.001
[2025-07-31 06:05:36,230] [INFO] Epoch 6/50, ValAcc: 78.53%, TrainLoss: 0.5468, ValLoss: 0.6452, LR: 0.001
[2025-07-31 06:05:54,670] [INFO] Epoch 7/50, ValAcc: 76.76%, TrainLoss: 0.4558, ValLoss: 0.7201, LR: 0.001
[2025-07-31 06:06:13,113] [INFO] Epoch 8/50, ValAcc: 79.12%, TrainLoss: 0.3844, ValLoss: 0.6758, LR: 0.001
[2025-07-31 06:06:31,555] [INFO] Epoch 9/50, ValAcc: 80.11%, TrainLoss: 0.3444, ValLoss: 0.7049, LR: 0.001
[2025-07-31 06:06:49,993] [INFO] Epoch 10/50, ValAcc: 82.14%, TrainLoss: 0.2108, ValLoss: 0.6170, LR: 0.0005
[2025-07-31 06:07:08,430] [INFO] Epoch 11/50, ValAcc: 81.88%, TrainLoss: 0.1483, ValLoss: 0.6721, LR: 0.0005
[2025-07-31 06:07:26,874] [INFO] Epoch 12/50, ValAcc: 82.27%, TrainLoss: 0.1263, ValLoss: 0.7435, LR: 0.0005
[2025-07-31 06:07:45,322] [INFO] Epoch 13/50, ValAcc: 80.43%, TrainLoss: 0.1127, ValLoss: 0.7971, LR: 0.0005
[2025-07-31 06:08:03,751] [INFO] Epoch 14/50, ValAcc: 82.34%, TrainLoss: 0.0829, ValLoss: 0.7830, LR: 0.00025
[2025-07-31 06:08:22,167] [INFO] Epoch 15/50, ValAcc: 82.21%, TrainLoss: 0.0680, ValLoss: 0.7849, LR: 0.00025
[2025-07-31 06:08:40,589] [INFO] Epoch 16/50, ValAcc: 82.86%, TrainLoss: 0.0595, ValLoss: 0.8526, LR: 0.00025
[2025-07-31 06:08:59,016] [INFO] Epoch 17/50, ValAcc: 82.60%, TrainLoss: 0.0512, ValLoss: 0.8678, LR: 0.000125
[2025-07-31 06:09:17,431] [INFO] Epoch 18/50, ValAcc: 81.88%, TrainLoss: 0.0455, ValLoss: 0.8765, LR: 0.000125
[2025-07-31 06:09:35,854] [INFO] Epoch 19/50, ValAcc: 82.07%, TrainLoss: 0.0447, ValLoss: 0.8653, LR: 0.000125
[2025-07-31 06:09:54,288] [INFO] Epoch 20/50, ValAcc: 82.40%, TrainLoss: 0.0426, ValLoss: 0.8572, LR: 6.25e-05
[2025-07-31 06:09:54,288] [INFO] Learning rate 0.000063 is below threshold. Stopping early.
[2025-07-31 06:09:57,106] [INFO] Namespace(path=['vrchat-worlds/meta-ip_len/meta-ip_len.csv'], pktcount=1000, kfold=5, model=['bigru'], norm=['token'], input_dim=512, hidden_size=256, num_layers=3, dropout=0.3, fusion_dim=256, fc_hidden_size=128, batch_size=64, epoch=50, lr=0.001, feature_attention=False, timestep_attention=False, strict=True, window_size=300, step_size=100, debug_path='output/vrchat-worlds/train/train_meta_1753940254.debug', leaderboard_path='output/vrchat-worlds/train/train_meta_1753940254.csv', step1=False, step2=False, step3=False, train=True, sliding_window_evaluation=False, longitudinal_evaluation=False, openworld_evaluation=False, device=device(type='cuda'), name='iplen_token_bigru_1000_64_50_0.001_512_256_3_0.3_256_128'),0.8361,0.8350,0.8380,0.8346
[2025-07-31 06:09:57,256] [INFO] [(0.8355219960604071, 0.8318268223604541, 0.8347525337597979, 0.8335567283725541), (0.8191070256073539, 0.8201110650112973, 0.8217239585350626, 0.8226457760794825), (0.8174655285620486, 0.8191787887273211, 0.8242508247494321, 0.8223420613549629), (0.8239737274220033, 0.8216102058953222, 0.8233231029781396, 0.823525355863043), (0.8361247947454844, 0.8349602797549067, 0.8379667786140662, 0.8345616024095517)]
